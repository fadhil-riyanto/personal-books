<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<!-- Mirrored from 0x80.pl/notesen/2015-12-27-base64-encoding.html by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 16:43:46 GMT -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.21.2: https://docutils.sourceforge.io/" />
<meta name="author" content="Wojciech Muła" />
<title>Base64 encoding &mdash; implementation study</title>
<link rel="stylesheet" href="style.css" type="text/css" />
</head>
<body><script src="../switch_theme.js" type="text/javascript"></script>
<div class="document" id="base64-encoding-implementation-study">
<h1 class="title">Base64 encoding &mdash; implementation study</h1>
<table class="docinfo" frame="void" rules="none">
<col class="docinfo-name" />
<col class="docinfo-content" />
<tbody valign="top">
<tr><th class="docinfo-name">Author:</th>
<td>Wojciech Muła</td></tr>
<tr class="added-on field"><th class="docinfo-name">Added on:</th><td class="field-body">2015-12-27</td>
</tr>
<tr class="updates field"><th class="docinfo-name">Updates:</th><td class="field-body">2016-01-17 (results from Core i7, BMI2, 64-bit variant of version B (case #7))</td>
</tr>
</tbody>
</table>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="toc-entry-1">Introduction</a></li>
<li><a class="reference internal" href="#loading-data-and-extracting-indices-points-1-2" id="toc-entry-2">Loading data and extracting indices (points 1 &amp; 2)</a><ul>
<li><a class="reference internal" href="#base-version-a" id="toc-entry-3">Base (version A)</a></li>
<li><a class="reference internal" href="#improvement-version-b" id="toc-entry-4">Improvement (version B)</a><ul>
<li><a class="reference internal" href="#bit-code" id="toc-entry-5">64-bit code</a></li>
<li><a class="reference internal" href="#bmi2" id="toc-entry-6">BMI2</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#translating-and-saving-the-result-points-3-4" id="toc-entry-7">Translating and saving the result (points 3 &amp; 4)</a><ul>
<li><a class="reference internal" href="#base-version-x" id="toc-entry-8">Base (version X)</a></li>
<li><a class="reference internal" href="#improvement-version-y" id="toc-entry-9">Improvement (version Y)</a></li>
<li><a class="reference internal" href="#improvement-version-z" id="toc-entry-10">Improvement (version Z)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#comparison" id="toc-entry-11">Comparison</a><ul>
<li><a class="reference internal" href="#results-from-core-i5" id="toc-entry-12">Results from Core i5</a></li>
<li><a class="reference internal" href="#results-from-core-i6" id="toc-entry-13">Results from Core i6</a></li>
</ul>
</li>
<li><a class="reference internal" href="#see-also" id="toc-entry-14">See also</a></li>
</ul>
</div>
<div class="section" id="introduction">
<h1>Introduction</h1>
<p>In a basic step of the Base64 encoding three bytes are processed producing
four output bytes. The step consist following stages:</p>
<ol class="arabic simple">
<li>load 3 bytes;</li>
<li>split these 3 bytes to four 6-bit indices;</li>
<li>translate the indices using a lookup table;</li>
<li>save four bytes.</li>
</ol>
<p>In this text different <strong>implementations</strong> of the procedure are examined.</p>
</div>
<div class="section" id="loading-data-and-extracting-indices-points-1-2">
<h1>Loading data and extracting indices (points 1 &amp; 2)</h1>
<div class="section" id="base-version-a">
<h2>Base (version A)</h2>
<p>The naive way is to load 3 bytes and then combine them to 4 indices:</p>
<pre class="literal-block">
const uint8_t x0 = input[0];
const uint8_t x1 = input[1];
const uint8_t x2 = input[2];

const uint8_t a = x0 &amp; 0x3f;
const uint8_t b = ((x1 &amp; 0xf) &lt;&lt; 2) | (x0 &gt;&gt; 6);
const uint8_t c = ((x2 &amp; 0x3) &lt;&lt; 4) | (x1 &gt;&gt; 4);
const uint8_t d = x2 &gt;&gt; 2;
</pre>
<p>This solution requires following operations:</p>
<ul class="simple">
<li>3 memory loads;</li>
<li>3 bit-ands;</li>
<li>2 bit-ors</li>
<li>5 bit-shifts.</li>
</ul>
<p>Although some of the instructions could be executed in parallel (this depends
on a CPU) the overall speed of the code is not very big.</p>
</div>
<div class="section" id="improvement-version-b">
<h2>Improvement (version B)</h2>
<p>Possible modification is to load <strong>at once</strong> 4 bytes (we use just 3 of them)
and then easily extract 6-bit indices:</p>
<pre class="literal-block">
const uint32_t x = input[0..3];

const uint8_t a = x &amp; 0x3f;
const uint8_t b = (x &gt;&gt; 8) &amp; 0x3f;
const uint8_t c = (x &gt;&gt; 16) &amp; 0x3f;
const uint8_t d = (x &gt;&gt; 24) &amp; 0x3f;
</pre>
<p>The modified version requires:</p>
<ul class="simple">
<li><strong>1 memory load</strong>;</li>
<li>4 bit-ands;</li>
<li>3 bit-shifts;</li>
</ul>
<p>A drawback is that 4-bytes chunk is loaded, it's possible to read off the available
process space; this is the drawback of all SWAR/SIMD algorithms.</p>
<div class="section" id="bit-code">
<h3>64-bit code</h3>
<p>On 64-bit machines an input word could be 64-bit. Thanks to that the number of load
decreases two times. However, only 6 bytes is used per iteration.</p>
</div>
<div class="section" id="bmi2">
<h3>BMI2</h3>
<p>The common scheme in this optimization is right shift followed by masking 6 lowest
bits. <a class="reference external" href="http://en.wikipedia.org/wiki/Bit_Manipulation_Instruction_Sets">Bit Manipulation Instruction</a> comes with an instruction <tt class="docutils literal">bext</tt> doing
exactly the described operation.</p>
</div>
</div>
</div>
<div class="section" id="translating-and-saving-the-result-points-3-4">
<h1>Translating and saving the result (points 3 &amp; 4)</h1>
<div class="section" id="base-version-x">
<h2>Base (version X)</h2>
<p>Again, the basic solution is sequentially translate and save separate bytes:</p>
<pre class="literal-block">
*out++ = lookup8[a];
*out++ = lookup8[b];
*out++ = lookup8[c];
*out++ = lookup8[d];
</pre>
<p>Memory loads are needed to properly translate the indices, thus 4 memory loads
are <strong>unavoidable</strong>.</p>
<p>The only positive aspect is that size of <tt class="docutils literal">lookup8</tt> is only 64 bytes, it perfectly
fits in the cache line.</p>
<p>This solution requires:</p>
<ul class="simple">
<li>4 memory loads (4 lookups);</li>
<li><strong>4 memory stores</strong>.</li>
</ul>
</div>
<div class="section" id="improvement-version-y">
<h2>Improvement (version Y)</h2>
<p>Modification of this step is similar to loading, i.e. we build a 32-bit result
and store the dword using just one instruction:</p>
<pre class="literal-block">
const uint32_t w =  uint32_t(lookup8[a])
                 | (uint32_t(lookup8[b]) &lt;&lt; 8)
                 | (uint32_t(lookup8[c]) &lt;&lt; 16)
                 | (uint32_t(lookup8[d]) &lt;&lt; 24);

output[0..3] = w;
</pre>
<p>This modification requires:</p>
<ul class="simple">
<li>4 memory loads (4 lookups);</li>
<li>3 bit-ors;</li>
<li>3 bit-shifts;</li>
<li><strong>1 memory store</strong>.</li>
</ul>
</div>
<div class="section" id="improvement-version-z">
<h2>Improvement (version Z)</h2>
<p>Bit shifts introduced in the above version could be avoided if special lookups
were introduced. For each index separate, 32-bit lookup is required, and then
the code simplifies to:</p>
<pre class="literal-block">
const uint32_t w = uint32_t(lookup32_0[a])
                 | uint32_t(lookup32_1[b])
                 | uint32_t(lookup32_2[c])
                 | uint32_t(lookup32_3[d]);

outout[0..3] = w;
</pre>
<p>The obvious negative point of this solution is much larger lookups (1 kB)
than the lookup used in the base version (64 bytes).</p>
</div>
</div>
<div class="section" id="comparison">
<h1>Comparison</h1>
<p>Sample code is <a class="reference external" href="https://github.com/WojciechMula/toys/tree/master/base64/encode/scalar">available at github</a>.</p>
<p>The test program converts 32 MiB of data; this is repeated 10 times and the
minimum time is printed.</p>
<div class="section" id="results-from-core-i5">
<h2>Results from Core i5</h2>
<table border="1" class="docutils">
<colgroup>
<col width="2%" />
<col width="26%" />
<col width="35%" />
<col width="8%" />
<col width="7%" />
<col width="21%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">#</th>
<th class="head">extract indices from ...</th>
<th class="head">output</th>
<th class="head">tims [s]</th>
<th class="head">speedup</th>
<th class="head">&nbsp;</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>1</td>
<td rowspan="3">... bytes (A)</td>
<td>save bytes (X)</td>
<td>0.121</td>
<td>100.0%</td>
<td><tt class="docutils literal"><span class="pre">██████████</span></tt></td>
</tr>
<tr><td>2</td>
<td>construct dword from 1 lookup table (Y)</td>
<td>0.146</td>
<td>82.9%</td>
<td><tt class="docutils literal"><span class="pre">████████</span></tt></td>
</tr>
<tr><td>3</td>
<td>construct dword from 4 lookup tables (Z)</td>
<td>0.124</td>
<td>97.6%</td>
<td><tt class="docutils literal"><span class="pre">█████████</span></tt></td>
</tr>
<tr><td>4</td>
<td rowspan="3">... 32-bit word (B)</td>
<td>save bytes (X)</td>
<td>0.095</td>
<td>127.4%</td>
<td><tt class="docutils literal"><span class="pre">█████████████</span></tt></td>
</tr>
<tr><td>5</td>
<td>construct dword from 1 lookup table (Y)</td>
<td>0.122</td>
<td>99.2%</td>
<td><tt class="docutils literal"><span class="pre">██████████</span></tt></td>
</tr>
<tr><td>6</td>
<td>construct dword from 4 lookup tables (Z)</td>
<td>0.099</td>
<td>122.2%</td>
<td><tt class="docutils literal"><span class="pre">████████████</span></tt></td>
</tr>
<tr><td>7</td>
<td>... 64-bit word (B)</td>
<td>save bytes (X)</td>
<td>0.070</td>
<td>172.8%</td>
<td><tt class="docutils literal"><span class="pre">█████████████████</span></tt></td>
</tr>
</tbody>
</table>
<p>Summary:</p>
<ul class="simple">
<li>The only factor causing significant speedup is minimizing memory reads,
i.e. using variant B for input. Case 7 is the brightest example.</li>
<li>Fancy methods building a result at once do not help at all.</li>
<li>The biggest speedup is merely 25% and I guess the memory throughput limits
further improvement.</li>
</ul>
</div>
<div class="section" id="results-from-core-i6">
<h2>Results from Core i6</h2>
<table border="1" class="docutils">
<colgroup>
<col width="2%" />
<col width="26%" />
<col width="35%" />
<col width="8%" />
<col width="7%" />
<col width="21%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">#</th>
<th class="head">extract indices from ...</th>
<th class="head">output</th>
<th class="head">tims [s]</th>
<th class="head">speedup</th>
<th class="head">&nbsp;</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>1</td>
<td rowspan="3">... bytes (A)</td>
<td>save bytes (X)</td>
<td>0.082</td>
<td>100.0%</td>
<td><tt class="docutils literal"><span class="pre">██████████</span></tt></td>
</tr>
<tr><td>2</td>
<td>construct dword from 1 lookup table (Y)</td>
<td>0.082</td>
<td>100.0%</td>
<td><tt class="docutils literal"><span class="pre">██████████</span></tt></td>
</tr>
<tr><td>3</td>
<td>construct dword from 4 lookup tables (Z)</td>
<td>0.073</td>
<td>122.3%</td>
<td><tt class="docutils literal"><span class="pre">████████████</span></tt></td>
</tr>
<tr><td>4</td>
<td rowspan="3">... 32-bit word (B)</td>
<td>save bytes (X)</td>
<td>0.056</td>
<td>146.4%</td>
<td><tt class="docutils literal"><span class="pre">██████████████</span></tt></td>
</tr>
<tr><td>5</td>
<td>construct dword from 1 lookup table (Y)</td>
<td>0.064</td>
<td>128.1%</td>
<td><tt class="docutils literal"><span class="pre">█████████████</span></tt></td>
</tr>
<tr><td>6</td>
<td>construct dword from 4 lookup tables (Z)</td>
<td>0.050</td>
<td>164.0%</td>
<td><tt class="docutils literal"><span class="pre">████████████████</span></tt></td>
</tr>
<tr><td>7</td>
<td>... 64-bit word (B)</td>
<td>save bytes (X)</td>
<td>0.043</td>
<td>190.7%</td>
<td><tt class="docutils literal"><span class="pre">███████████████████</span></tt></td>
</tr>
<tr><td>8</td>
<td>... 64-bit word &amp; bextr (B)</td>
<td>save bytes (X)</td>
<td>0.060</td>
<td>136.7%</td>
<td><tt class="docutils literal"><span class="pre">█████████████</span></tt></td>
</tr>
<tr><td>9</td>
<td>... 64-bit word &amp; bextr (B)</td>
<td>construct 64-bit words from 1 lookup</td>
<td>0.072</td>
<td>113.9%</td>
<td><tt class="docutils literal"><span class="pre">███████████</span></tt></td>
</tr>
</tbody>
</table>
<p>Summary:</p>
<ul class="simple">
<li>Unlike i5, on Core i7 there is speedup caused by minimizing number of 32-bit writes
(clearly visible in cases 3 and 6).</li>
<li>However coalescing writes to 64-bit word (case 10) has negative impact.</li>
<li>The biggest boost is due to use 64-bit loads (case 7).</li>
<li>Using the new shiny instruction <tt class="docutils literal">bextr</tt> doesn't help. I'm underwhelmed and shocked.
According to the manuals, throughput and latency are very small, the instruction
replaces two other (including quite expansive shift). What a shame.</li>
</ul>
</div>
</div>
<div class="section" id="see-also">
<h1>See also</h1>
<ul class="simple">
<li><a class="reference external" href="2016-01-12-sse-base64-encoding.html">Base64 encoding with SIMD instructions</a> &mdash; SSE boosts base64 encoding 2 times</li>
</ul>
</div>
</div>
</body>

<!-- Mirrored from 0x80.pl/notesen/2015-12-27-base64-encoding.html by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 16:43:46 GMT -->
</html>
