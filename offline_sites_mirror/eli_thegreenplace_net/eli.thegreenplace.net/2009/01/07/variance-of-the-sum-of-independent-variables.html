<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2009/01/07/variance-of-the-sum-of-independent-variables by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 17 Feb 2025 00:09:42 GMT -->
<head>
    <title>Variance of the sum of independent random variables - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../theme/css/style.css" type="text/css"/>

        <link href="../../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../../index.html" class="navbar-brand">
                <img src="../../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="variance-of-the-sum-of-independent-variables.html"
                       rel="bookmark"
                       title="Permalink to Variance of the sum of independent random variables">
                        Variance of the sum of independent random variables
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> January 07, 2009 at 22:06</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../../tag/math.html">Math</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                
    <p>
Yesterday I was trying to brush up my skills in probability and came upon this formula on the Wikipedia page <a href="http://en.wikipedia.org/wiki/Variance">about variance</a>:
</p>


<p><img src="../../../images/math/5a8707440af01e8319f02e80f8ea33d4600a4a4b.gif" class="align-center" /></p>


<p>
The article calls this the <em>Bienaymé formula</em> and gives neither proof nor
a link to one. Googling this formula proved equally fruitless in terms of
proofs. 
</p>

<p>
So, I set out to find why this works. It took me a few hours of digging through
books and removing dust from my University-learned probability skills of 8 years
ago, but finally I've made it. Here's how.
</p>

<p>
<em>Note: the Wikipedia article states the Bienaymé formula for uncorrelated
  variables. Here I'll prove the case of independent variables, which is a more
useful and frequently used application of the formula. I'm also proving it for
discrete random variables - the continuous case is equivalent.</em>
</p>

<h2>Expected value and variance</h2>

<p>
We'll start with a few definitions. 
Formally, the expected value of a (discrete) random variable X is defined by:
</p>
<p><img src="../../../images/math/6e3bd6378c646ec0f285a69df7db72194f308f5b.gif" class="align-center" /></p>


Where <img src="../../../images/math/810bdf91cc65f953d130a2f239cee691fa024330.gif" /> is
the <a href="http://en.wikipedia.org/wiki/Probability_mass_function">PMF</a> of
X, <img src="../../../images/math/30eaa4945a586b21346e14bd193b9914db6c2166.gif" />.
For a function <img src="../../../images/math/65405422ff71ebf2db437dbd89a41355f4f19183.gif" />:


<p><img src="../../../images/math/28e5d4f4c0d4dc5023e96687ce05e8851a8f8329.gif" class="align-center" /></p>

<p>
The variance of X is defined in terms of the expected value as:
</p>

<p><img src="../../../images/math/fe590d0bcb58c4be73d18e751200721bbc402dc0.gif" class="align-center" /></p>

<p>
From this we can also obtain:
</p>

<p><img src="../../../images/math/dfcad71ca591b4179d442332299b6a5d5963e628.gif" class="align-center" /></p>


<p><img src="../../../images/math/03ccc00bf5863f84fa1c081dc26c4b450ee7afcc.gif" class="align-center" /></p>


<p><img src="../../../images/math/c7e98eaab83b497e5716ffa78dcd80baff9d8c59.gif" class="align-center" /></p>


<p><img src="../../../images/math/a10dd400a0c745f0f369d8919994ae0652b21024.gif" class="align-center" /></p>


<p><img src="../../../images/math/8a43a908e597eeb8f279a521c4d833f3ac88f7f9.gif" class="align-center" /></p>


Which is more convenient to use in some calculations.

<h2>Linear function of a random variable</h2>
<p>
From the definitions given above it can be easily shown that given a linear function of a random variable: <img src="../../../images/math/5abba821a5142e83482cea2117ec22b289d4a3d6.gif" />, the expected value and variance of Y are:
</p>

<p><img src="../../../images/math/d8e69b52ca0d6dcb06ba7b8975266518278145e3.gif" class="align-center" /></p>


<p><img src="../../../images/math/c420f86130e26c754241fd1f56cbe14cd86d1358.gif" class="align-center" /></p>

<p>
For the expected value, we can make a stronger claim for any g(x):
</p>

<p><img src="../../../images/math/e4cdac3ba0c020100a9b98b82f3e0ac6b74b2b78.gif" class="align-center" /></p>


<h2>Multiple random variables</h2>
<p>
When multiple random variables are involved, things start getting a bit more complicated. I'll focus on two random variables here, but this is easily extensible to N variables. Given two random variables that participate in an experiment, their joint PMF is:
</p>

<p><img src="../../../images/math/955c39ca88717c6449629b633a7589a910c9555f.gif" class="align-center" /></p>

<p>
The joint PMF determines the probability of any event that can be specified in terms of the random variables X and Y. For example if A is the set of all pairs <img src="../../../images/math/f09b5d4028feab230a8f9a4499e21a0b4db3ccce.gif" /> that have a certain property, then:
</p>

<p><img src="../../../images/math/6b1ea5b82b9f9b2609973a2c0959ea2a44c80fd8.gif" class="align-center" /></p>

<p>
Note that from this PMF we can infer the PMF for a single variable, like this:
</p>

<p><img src="../../../images/math/30eaa4945a586b21346e14bd193b9914db6c2166.gif" class="align-center" /></p>


<p><img src="../../../images/math/60de9cadbeef9f0d1931ba799bc7790960ca3c61.gif" class="align-center" /></p>


<p><img src="../../../images/math/fec2fcd06d4ac717101f02690278a8daa3f2e068.gif" class="align-center" /></p>

<p>
The expected value for functions of two variables naturally extends and takes the form:
</p>

<p><img src="../../../images/math/0d00c9b76a37f91f0a2a1efe986e545ac7c90639.gif" class="align-center" /></p>


<h2>Sum of random variables</h2>
<p>
Let's see how the sum of random variables behaves. From the previous formula:
</p>

<p><img src="../../../images/math/a00cd8185a75e2256e4ec8a11d9c6fe7fd776d06.gif" class="align-center" /></p>


<p><img src="../../../images/math/30e7d900e6261f0d926e1e0b34a7f8bf1d6bd9e8.gif" class="align-center" /></p>

<p>
But recall equation (1). The above simply equals to:
</p>

<p><img src="../../../images/math/ecece91067e8b6d6ae4250b7b59f92fc367cf433.gif" class="align-center" /></p>


<p><img src="../../../images/math/3b2b53ab4299f6a63867ed4b5645018abee982e5.gif" class="align-center" /></p>

<p>
We'll also want to prove that <img src="../../../images/math/08167b14267064495a086afab04d97c83751184c.gif" />. This is only true for independent X and Y, so we'll have to make this assumption (assuming that they're independent means that <img src="../../../images/math/9a31b8e023593b070b7bfdcfca07f2707d47265a.gif" />).
</p>

<p><img src="../../../images/math/f7fec121582e25c06c1119e689ba6828d20883b2.gif" class="align-center" /></p>

<p>
By independence:
</p>

<p><img src="../../../images/math/762a9e3e2ff39253bec8efb60e0eb17f8110af99.gif" class="align-center" /></p>


<p><img src="../../../images/math/ee6ebbb2633e33514285d312a999a17354e6521c.gif" class="align-center" /></p>


<p><img src="../../../images/math/0b6f7ad819416477a9e8a948c7a8c9d84be25c12.gif" class="align-center" /></p>

<p>
A very similar proof can show that for independent X and Y:
</p>

<p><img src="../../../images/math/951d25c3af4d29c3cb9c1e69c4d546f2e7ac7521.gif" class="align-center" /></p>

<p>
For any functions g and h (because if X and Y are independent, so are g(X) and
h(y)).  Now, at last, we're ready to tackle the variance of X + Y. We start by
expanding the definition of variance:
</p>

<p><img src="../../../images/math/10b9e3c7ad7f5621b21440e0113ec31f11b4884d.gif" class="align-center" /></p>


By (2):


<p><img src="../../../images/math/b9fa5c9ce518390c4e2fb4d672c29bcbfb2e91b7.gif" class="align-center" /></p>


<p><img src="../../../images/math/74f08dfb9aa6322473e84f1afc42e1d9e53fac1f.gif" class="align-center" /></p>


<p><img src="../../../images/math/45c1416ce8b6179109d77f80ac353251e525de2c.gif" class="align-center" /></p>


<p><img src="../../../images/math/65ddaa4af89d23925dae7e3cbe4d7526df04ad25.gif" class="align-center" /></p>

<p>
Now, note that the random variables <img src="../../../images/math/c5f6fd358d4c1cbb485216af24b715050ff8a121.gif" /> and <img src="../../../images/math/3da3e48f75d2e03d9795c7617016701d1cd28b2c.gif" /> are independent, so:
</p>

<p><img src="../../../images/math/29c181dd186163c688dcfec42ce8fb08e366cbfa.gif" class="align-center" /></p>


But using (2) again:


<p><img src="../../../images/math/918d718f3654bfe32037276651f2f9e1c2472585.gif" class="align-center" /></p>


<img src="../../../images/math/ec36fba3bda13db973cd60f1910c7955f10757c2.gif" /> is obviously just <img src="../../../images/math/769c095daa5985533efb5176c86007611e6f4eb5.gif" />, therefore the above reduces to 0.
<p>
So, coming back to the long expression for the variance of sums, the last term is 0, and we have:
</p>

<p><img src="../../../images/math/97b8d284ae899aa4f6568479f30fb6c40cf7f13d.gif" class="align-center" /></p>


<p><img src="../../../images/math/2172289164dba259d1e5c274e2611e15862edecb.gif" class="align-center" /></p>

<p>
As I've mentioned before, proving this for the sum of two variables suffices,
because the proof for N variables is a simple mathematical extension, and can be
intuitively understood by means of a "mental induction". Therefore:
</p>

<p><img src="../../../images/math/5a8707440af01e8319f02e80f8ea33d4600a4a4b.gif" class="align-center" /></p>

<p>
For N independent variables <img src="../../../images/math/97fd495350d680b99411eaf425194e5b295465a6.gif" />. <img src="../../../images/math/7b47d4175993a732aa2287de666a82273110f26e.gif" />
</p>
    
            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2009/01/07/variance-of-the-sum-of-independent-variables by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 17 Feb 2025 00:09:42 GMT -->
</html>