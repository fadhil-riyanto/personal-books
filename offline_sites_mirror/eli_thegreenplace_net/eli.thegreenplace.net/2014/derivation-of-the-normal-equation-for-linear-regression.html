<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 17 Feb 2025 01:01:11 GMT -->
<head>
    <title>Derivation of the Normal Equation for linear regression - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../theme/css/style.css" type="text/css"/>

        <link href="../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../index.html" class="navbar-brand">
                <img src="../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="derivation-of-the-normal-equation-for-linear-regression/index.html"
                       rel="bookmark"
                       title="Permalink to Derivation of the Normal Equation for linear regression">
                        Derivation of the Normal Equation for linear regression
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> December 22, 2014 at 20:50</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../tag/math.html">Math</a>
        ,
    <a href="../tag/machine-learning.html">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>I was going through the Coursera &quot;Machine Learning&quot; course, and in the section
on multivariate linear regression something caught my eye. Andrew Ng presented
the <a class="reference external" href="http://en.wikipedia.org/w/index.php?title=Normal_equation&amp;redirect=no">Normal Equation</a> as an
analytical solution to the linear regression problem with a least-squares cost
function. He mentioned that in some cases (such as for small feature sets) using
it is more effective than applying gradient descent; unfortunately, he left its
derivation out.</p>
<p>Here I want to show how the normal equation is derived.</p>
<p>First, some terminology. The following symbols are compatible with the machine
learning course, not with the exposition of the normal equation on Wikipedia and
other sites - semantically it's all the same, just the symbols are different.</p>
<p>Given the hypothesis function:</p>
<img alt="\[h_{\theta}(x)=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n\]" class="align-center" src="../images/math/dd8fad9bf111e83d47252d51dd037a6c6c3136aa.png" style="height: 18px;" />
<p>We'd like to minimize the least-squares cost:</p>
<img alt="\[J(\theta_{0...n})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2\]" class="align-center" src="../images/math/c1abe0768f4deb31ed97f37d760236c94439a780.png" style="height: 50px;" />
<p>Where <img alt="x^{(i)}" class="valign-0" src="../images/math/233014006c0adbee71ec71ba3a70f22ad1b906a1.png" style="height: 17px;" /> is the <tt class="docutils literal">i</tt>-th sample (from a set of <tt class="docutils literal">m</tt> samples) and
<img alt="y^{(i)}" class="valign-m4" src="../images/math/d34414117d493106f731939df6bb7f1762365d3f.png" style="height: 21px;" /> is the <tt class="docutils literal">i</tt>-th expected result.</p>
<p>To proceed, we'll represent the problem in matrix notation; this is natural,
since we essentially have a system of linear equations here. The regression
coefficients <img alt="\theta" class="valign-0" src="../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> we're looking for are the vector:</p>
<img alt="\[\begin{pmatrix} \theta_0\\ \theta_1\\ ...\\ \theta_n \end{pmatrix}\in\mathbb{R}^{n+1}\]" class="align-center" src="../images/math/b16fd3d2b3041f13cb70199837a7c02c756078c7.png" style="height: 86px;" />
<p>Each of the <tt class="docutils literal">m</tt> input samples is similarly a column vector with <tt class="docutils literal">n+1</tt> rows,
<img alt="x_0" class="valign-m3" src="../images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /> being 1 for convenience. So we can now rewrite the hypothesis
function as:</p>
<img alt="\[h_{\theta}(x)=\theta^Tx\]" class="align-center" src="../images/math/be661047c89f6a48c7bc563b81207949c251de6a.png" style="height: 21px;" />
<p>When this is summed over all samples, we can dip further into matrix notation.
We'll define the &quot;design matrix&quot; <tt class="docutils literal">X</tt> (uppercase X) as a matrix of <tt class="docutils literal">m</tt> rows,
in which each row is the <tt class="docutils literal">i</tt>-th sample (the vector <img alt="x^{(i)}" class="valign-0" src="../images/math/233014006c0adbee71ec71ba3a70f22ad1b906a1.png" style="height: 17px;" />). With
this, we can rewrite the least-squares cost as following, replacing the explicit
sum by matrix multiplication:</p>
<img alt="\[J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)\]" class="align-center" src="../images/math/db5e3da78e25c18c8fc88f1291c1ac13a2645388.png" style="height: 36px;" />
<p>Now, using some matrix transpose identities, we can simplify this a bit. I'll
throw the <img alt="\frac{1}{2m}" class="valign-m6" src="../images/math/7a2a3f6dba54b64f0e88e18c40e0f68c523713ea.png" style="height: 22px;" /> part away since we're going to compare a
derivative to zero anyway:</p>
<img alt="\[J(\theta)=((X\theta)^T-y^T)(X\theta-y)\]" class="align-center" src="../images/math/c1368de1a0634c3fbeb92d67f368f253943d089f.png" style="height: 21px;" />
<img alt="\[J(\theta)=(X\theta)^TX\theta-(X\theta)^Ty-y^T(X\theta)+y^Ty\]" class="align-center" src="../images/math/e41fc822adccf1f865b02100f5671e265e7b30bc.png" style="height: 21px;" />
<p>Note that <img alt="X\theta" class="valign-0" src="../images/math/52f2de6065bdc187b876c5696041f3c716c446f5.png" style="height: 12px;" /> is a vector, and so is <tt class="docutils literal">y</tt>. So when we multiply
one by another, it doesn't matter what the order is (as long as the dimensions
work out). So we can further simplify:</p>
<img alt="\[J(\theta)=\theta^TX^TX\theta-2(X\theta)^Ty+y^Ty\]" class="align-center" src="../images/math/2864b88546c007a79dc92271f5e01487ba608e43.png" style="height: 21px;" />
<p>Recall that here <img alt="\theta" class="valign-0" src="../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> is our unknown. To find where the above
function has a minimum, we will derive by <img alt="\theta" class="valign-0" src="../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> and compare to 0.
Deriving by a vector may feel uncomfortable, but there's nothing to worry about.
Recall that here we only use matrix notation to conveniently represent a system
of linear formulae. So we derive by each component of the vector, and then
combine the resulting derivatives into a vector again. The result is:</p>
<img alt="\[\frac{\partial J}{\partial \theta}=2X^TX\theta-2X^{T}y=0\]" class="align-center" src="../images/math/9b142c00e031c9db7f575b0542e86261732a4689.png" style="height: 38px;" />
<p>Or:</p>
<img alt="\[X^TX\theta=X^{T}y\]" class="align-center" src="../images/math/ab453f9f1f7bd4b1d646b9712fbe0b2fbe01740f.png" style="height: 21px;" />
<p>Now, assuming that the matrix <img alt="X^TX" class="valign-0" src="../images/math/5c817c84ec1f83b23494df6125edd091a7c413dd.png" style="height: 15px;" /> is invertible, we can multiply both
sides by <img alt="(X^TX)^{-1}" class="valign-m4" src="../images/math/57f592cee6ceac659262d97e61c64f9ca405d7f1.png" style="height: 19px;" /> and get:</p>
<img alt="\[\theta=(X^TX)^{-1}X^Ty\]" class="align-center" src="../images/math/20baabd9d33dcd26003bc44c7d81ba39e1ad4caa.png" style="height: 21px;" />
<p>Which is the normal equation.</p>
<p>[<strong>Update 27-May-2015</strong>: I've written <a class="reference external" href="../2015/the-normal-equation-and-matrix-calculus/index.html">another post</a>
that explains in more detail how these derivatives are computed.]</p>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 17 Feb 2025 01:01:11 GMT -->
</html>