<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Eli Bendersky's website - Math</title><link href="https://eli.thegreenplace.net/" rel="alternate"></link><link href="https://eli.thegreenplace.net/feeds/math.atom.xml" rel="self"></link><id>https://eli.thegreenplace.net/</id><updated>2025-01-17T21:51:01-08:00</updated><entry><title>Reverse mode Automatic Differentiation</title><link href="https://eli.thegreenplace.net/2025/reverse-mode-automatic-differentiation/" rel="alternate"></link><published>2025-01-13T19:02:00-08:00</published><updated>2025-01-17T21:51:01-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2025-01-13:/2025/reverse-mode-automatic-differentiation/</id><summary type="html">&lt;p&gt;Automatic Differentiation (AD) is an important algorithm for calculating the
derivatives of arbitrary functions that can be expressed by a computer program.
One of my favorite CS papers is
&lt;a class="reference external" href="https://arxiv.org/abs/1502.05767"&gt;&amp;quot;Automatic differentiation in machine learning: a survey&amp;quot;&lt;/a&gt; by
Baydin, Perlmutter, Radul and Siskind (ADIMLAS from here on).
While this post attempts â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Automatic Differentiation (AD) is an important algorithm for calculating the
derivatives of arbitrary functions that can be expressed by a computer program.
One of my favorite CS papers is
&lt;a class="reference external" href="https://arxiv.org/abs/1502.05767"&gt;&amp;quot;Automatic differentiation in machine learning: a survey&amp;quot;&lt;/a&gt; by
Baydin, Perlmutter, Radul and Siskind (ADIMLAS from here on).
While this post attempts to be useful on its own, it serves best as a followup
to the ADIMLAS paper - so I strongly encourage you to read that first.&lt;/p&gt;
&lt;p&gt;The main idea of AD is to treat a computation as a nested sequence of function
compositions, and then calculate the derivative of the outputs w.r.t. the inputs
using repeated applications of the chain rule. There are two methods of AD:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Forward mode: where derivatives are computed starting at the inputs&lt;/li&gt;
&lt;li&gt;Reverse mode: where derivatives are computed starting at the outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reverse mode AD is a generalization of the &lt;em&gt;backpropagation&lt;/em&gt; technique used
in training neural networks. While backpropagation starts from a single scalar
output, reverse mode AD works for any number of function outputs. In this post
I'm going to be describing how reverse mode AD works in detail.&lt;/p&gt;
&lt;p&gt;While reading the ADIMLAS paper is strongly recommended but not required,
there &lt;em&gt;is&lt;/em&gt; one mandatory pre-requisite for this post: a good understanding of
the chain rule of calculus, including its multivariate formulation. Please
read &lt;a class="reference external" href="https://eli.thegreenplace.net/2016/the-chain-rule-of-calculus"&gt;my earlier post on the subject&lt;/a&gt;
first if you're not familiar with it.&lt;/p&gt;
&lt;div class="section" id="linear-chain-graphs"&gt;
&lt;h2&gt;Linear chain graphs&lt;/h2&gt;
&lt;p&gt;Let's start with a simple example where the computation is a linear chain of
primitive operations: the Sigmoid function.&lt;/p&gt;
&lt;img alt="\[S(x)=\frac{1}{1+e^{-x}}\]" class="align-center" src="https://eli.thegreenplace.net/images/math/9a39d0495ce32da5840b76adaf508a0349394c49.png" style="height: 38px;" /&gt;
&lt;p&gt;This is a basic Python implementation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To apply the chain rule, we'll break down the calculation of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/fa94a6b584da9e970e40fbcbe4d615031ac59bc2.svg" style="height: 19px;" type="image/svg+xml"&gt;S(x)&lt;/object&gt; to
a sequence of function compositions, as follows:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/33494603270cd9090af303e2e99cf367173f588b.svg" style="height: 119px;" type="image/svg+xml"&gt;\[\begin{align*}
  f(x)&amp;amp;=-x\\
  g(f)&amp;amp;=e^f\\
  w(g)&amp;amp;=1+g\\
  v(w)&amp;amp;=\frac{1}{w}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Take a moment to convince yourself that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/fa94a6b584da9e970e40fbcbe4d615031ac59bc2.svg" style="height: 19px;" type="image/svg+xml"&gt;S(x)&lt;/object&gt; is equivalent to
the composition &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/2496463ea05f38acb598c84a70bfcc4692591511.svg" style="height: 19px;" type="image/svg+xml"&gt;v\circ(w\circ(g\circ f))(x)&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;The same decomposition of &lt;tt class="docutils literal"&gt;sigmoid&lt;/tt&gt; into primitives in Python would look as
follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;
    &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Yet another representation is this computational graph:&lt;/p&gt;
&lt;img alt="Computational graph showing sigmoid" class="align-center" src="https://eli.thegreenplace.net/images/2025/sigmoid-graph.png" /&gt;
&lt;p&gt;Each box (graph node) represents a primitive operation, and the name assigned
to it (the green rectangle on the right of each box). An arrows (graph edge)
represent the flow of values between operations.&lt;/p&gt;
&lt;p&gt;Our goal is to find the derivative of &lt;em&gt;S&lt;/em&gt; w.r.t. &lt;em&gt;x&lt;/em&gt; at some point &lt;img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /&gt;,
denoted as &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e4ba6fb484fe0c504010c93b6144b99a885a0b97.svg" style="height: 19px;" type="image/svg+xml"&gt;S&amp;#x27;(x_0)&lt;/object&gt;. The process starts by running the computational
graph forward with our value of &lt;img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /&gt;. As an example, we'll use
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/064a529ef0eed72cc7839b45e015ec432a83c9b2.svg" style="height: 16px;" type="image/svg+xml"&gt;x_0=0.5&lt;/object&gt;:&lt;/p&gt;
&lt;img alt="Computational graph with forward calculation at 0.5" class="align-center" src="https://eli.thegreenplace.net/images/2025/sigmoid-graph-forward-calc.png" /&gt;
&lt;p&gt;Since all the functions in this graph have a single input and a single output,
it's sufficient to use the single-variable formulation of the chain rule.&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/28362ed858958e6d1e4754338cb3e6bc6aca6026.svg" style="height: 21px;" type="image/svg+xml"&gt;\[(g \circ f)&amp;#x27;(x_0)={g}&amp;#x27;(f(x_0)){f}&amp;#x27;(x_0)\]&lt;/object&gt;
&lt;p&gt;To avoid confusion, let's switch notation so we can explicitly see which
derivatives are involved. For &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/5ad8d30dbab83209f45a10850ffbcacf6662321e.svg" style="height: 19px;" type="image/svg+xml"&gt;g(f)&lt;/object&gt; as before, we can
write the derivatives like this:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2a365d03b211d0f53930958bcf200b18409b32b6.svg" style="height: 40px;" type="image/svg+xml"&gt;\[f&amp;#x27;(x)=\frac{df}{dx}\quad g&amp;#x27;(f)=\frac{dg}{df}\]&lt;/object&gt;
&lt;p&gt;Each of these is a function we can evaluate at some point; for example, we
denote the evaluation of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/9567f23affb8a7bd391269e4572b0200f9747cb8.svg" style="height: 19px;" type="image/svg+xml"&gt;f&amp;#x27;(x)&lt;/object&gt; at &lt;img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /&gt; as &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/5bf185ec15f166ab5130fbff525b5dc8a2d91546.svg" style="height: 23px;" type="image/svg+xml"&gt;\frac{df}{dx}(x_0)&lt;/object&gt;.
So we can rewrite the chain rule like this:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9ec7442f9adf5913013decd3f4959574375fec58.svg" style="height: 42px;" type="image/svg+xml"&gt;\[\frac{d(g \circ f)}{dx}(x_0)=\frac{dg}{df}(f(x_0))\frac{df}{dx}(x_0)\]&lt;/object&gt;
&lt;p&gt;Reverse mode AD means applying the chain rule to our computation graph, starting
with the last operation and ending at the first.
Remember that our final goal is to calculate:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e2774a76c051cd50d2dbf1ae13593fa7f1e773db.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{dS}{dx}(x_0)\]&lt;/object&gt;
&lt;p&gt;Where &lt;em&gt;S&lt;/em&gt; is a composition of multiple functions. The first composition we
unravel is the last node in the graph, where &lt;em&gt;v&lt;/em&gt; is calculated from &lt;em&gt;w&lt;/em&gt;. This is
the chain rule for it:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/234154080611fa2d641c60b299b89513aed5dad6.svg" style="height: 38px;" type="image/svg+xml"&gt;\[\frac{dS}{dw}=\frac{d(S \circ v)}{dw}(x_0)=\frac{dS}{dv}(v(x_0))\frac{dv}{dw}(x_0)\]&lt;/object&gt;
&lt;p&gt;The formula for &lt;em&gt;S&lt;/em&gt; is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/f5dec59579550096a611d2ed38b64a0935219947.svg" style="height: 19px;" type="image/svg+xml"&gt;S(v)=v&lt;/object&gt;, so its derivative is 1. The formula for
&lt;em&gt;v&lt;/em&gt; is &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/15935505468369a4c3d1371b6092dd47fb843936.svg" style="height: 22px;" type="image/svg+xml"&gt;v(w)=\frac{1}{w}&lt;/object&gt;, so its derivative is &lt;object class="valign-m7" data="https://eli.thegreenplace.net/images/math/26a341f4d7d5266620d3ad953a2b3b6cf45d1b42.svg" style="height: 23px;" type="image/svg+xml"&gt;-\frac{1}{w^2}&lt;/object&gt;.
Substituting the value of &lt;em&gt;w&lt;/em&gt; computed in the forward pass, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f4fac2ae8c9004597295e076cb4bbabac5a364f1.svg" style="height: 45px;" type="image/svg+xml"&gt;\[\frac{dS}{dw}(x_0)=1\cdot\frac{-1}{w^2}\bigg\rvert_{w=1.61}=-0.39\]&lt;/object&gt;
&lt;p&gt;Continuing backwards from &lt;em&gt;v&lt;/em&gt; to &lt;em&gt;w&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b16fa07dd10b039349e4addc1948ffe7c1207bed.svg" style="height: 40px;" type="image/svg+xml"&gt;\[\frac{dS}{dg}(x_0)=\frac{dS}{dw}(x_0)\frac{dw}{dg}(x_0)\]&lt;/object&gt;
&lt;p&gt;We've already calculated &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/7f843eaca6c53d1341ffd2285f643d0f233d18cb.svg" style="height: 22px;" type="image/svg+xml"&gt;\frac{dS}{dw}(x_0)&lt;/object&gt; in the previous step. Since
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/f16c67e0ba9a5df1ffa4ebe6e9d13dcf9a9df0e1.svg" style="height: 16px;" type="image/svg+xml"&gt;w=1+g&lt;/object&gt;, we know that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/93fc061d1af1e6125431be32a6d07be34ae82976.svg" style="height: 19px;" type="image/svg+xml"&gt;w&amp;#x27;(g)=1&lt;/object&gt;, so:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/bead5038f022559d95b89314911e713bc88d0a05.svg" style="height: 40px;" type="image/svg+xml"&gt;\[\frac{dS}{dg}(x_0)=-0.39\cdot1=-0.39\]&lt;/object&gt;
&lt;p&gt;Continuing similarly down the chain, until we get to the input &lt;em&gt;x&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e9b20f826ab6a143e8dc08f0e1d4bffdef1739f1.svg" style="height: 94px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{dS}{df}(x_0)&amp;amp;=\frac{dS}{dg}(x_0)\frac{dg}{df}(x_0)=-0.39\cdot e^f\bigg\rvert_{f=-0.5}=-0.24\\
  \frac{dS}{dx}(x_0)&amp;amp;=\frac{dS}{df}(x_0)\frac{df}{dx}(x_0)=-0.24\cdot -1=0.24
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We're done; the value of the derivative of the sigmoid function at &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b252e76d8a58888c218a5a4c2d463bdd8f0c0b20.svg" style="height: 13px;" type="image/svg+xml"&gt;x=0.5&lt;/object&gt;
is 0.24; this can be easily verified with a calculator using the analytical
derivative of this function.&lt;/p&gt;
&lt;p&gt;As you can see, this procedure is rather mechanical and it's not surprising that
it can be automated. Before we get to automation, however, let's review the
more common scenario where the computational graph is a DAG rather than a linear
chain.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="general-dags"&gt;
&lt;h2&gt;General DAGs&lt;/h2&gt;
&lt;p&gt;The sigmoid sample we worked though above has a very simple, linear
computational graph. Each node has a single predecessor and a single successor;
moreover, the function itself has a single input and single output. Therefore,
the single-variable chain rule is sufficient here.&lt;/p&gt;
&lt;p&gt;In the more general case, we'll encounter functions that have multiple inputs,
may also have multiple outputs &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;, and the internal nodes are connected in
non-linear patterns. To compute their derivatives, we have to use the
multivariate chain rule.&lt;/p&gt;
&lt;p&gt;As a reminder, in the most general case we're dealing with a function that has
&lt;em&gt;n&lt;/em&gt; inputs, denoted &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/ea64bb914bc3a768b4557a09b6e79badac50efb6.svg" style="height: 12px;" type="image/svg+xml"&gt;a=a_1,a_2\cdots a_n&lt;/object&gt;, and &lt;em&gt;m&lt;/em&gt;
outputs, denoted &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/97ca4dff67272bbafc9631d485c7687d07dffe90.svg" style="height: 16px;" type="image/svg+xml"&gt;f_1,f_2\cdots f_m&lt;/object&gt;. In other words, the function is
mapping &lt;img alt="f:\mathbb{R}^{n} \to \mathbb{R}^{m}" class="valign-m4" src="https://eli.thegreenplace.net/images/math/13f219789047343729036279bb11630db317d98d.png" style="height: 16px;" /&gt;.&lt;/p&gt;
&lt;p&gt;The partial derivative of output &lt;em&gt;i&lt;/em&gt; w.r.t. input &lt;em&gt;j&lt;/em&gt; at some point &lt;em&gt;a&lt;/em&gt; is:&lt;/p&gt;
&lt;img alt="\[D_j f_i(a)=\frac{\partial f_i}{\partial a_j}(a)\]" class="align-center" src="https://eli.thegreenplace.net/images/math/30881b5a92e45259714ba01c7a12fbf8f6c56109.png" style="height: 42px;" /&gt;
&lt;p&gt;Assuming &lt;em&gt;f&lt;/em&gt; is differentiable at &lt;em&gt;a&lt;/em&gt;, then the complete derivative of &lt;em&gt;f&lt;/em&gt;
w.r.t. its inputs can be represented by the &lt;em&gt;Jacobian matrix&lt;/em&gt;:&lt;/p&gt;
&lt;img alt="\[Df(a)=\begin{bmatrix} D_1 f_1(a) &amp;amp;amp; \cdots &amp;amp;amp; D_n f_1(a) \\ \vdots &amp;amp;amp;  &amp;amp;amp; \vdots \\ D_1 f_m(a) &amp;amp;amp; \cdots &amp;amp;amp; D_n f_m(a) \\ \end{bmatrix}\]" class="align-center" src="https://eli.thegreenplace.net/images/math/ab09367d48e9ef4d8bc2314a60313dec700193af.png" style="height: 76px;" /&gt;
&lt;p&gt;The multivariate chain rule then states that if we compose &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/fc2de39fe9cb2349e17b06356b230494023e2663.svg" style="height: 16px;" type="image/svg+xml"&gt;f\circ g&lt;/object&gt;
(and assuming all the dimensions are correct), the derivative is:&lt;/p&gt;
&lt;img alt="\[D(f \circ g)(a)=Df(g(a)) \cdot Dg(a)\]" class="align-center" src="https://eli.thegreenplace.net/images/math/00bdefa904bd34df2dfb50cc385e6497c4e5096e.png" style="height: 18px;" /&gt;
&lt;p&gt;This is the matrix multiplication of &lt;img alt="Df(g(a))" class="valign-m4" src="https://eli.thegreenplace.net/images/math/e567730c48bb2f95c258b630b4d6e997043e09ab.png" style="height: 18px;" /&gt; and &lt;img alt="Dg(a)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/2575fc98e794a733a7aa6237fe67246a41e6c8c5.png" style="height: 18px;" /&gt;.&lt;/p&gt;
&lt;div class="section" id="linear-nodes"&gt;
&lt;h3&gt;Linear nodes&lt;/h3&gt;
&lt;p&gt;As a warmup, let's start with a linear node that has a single input and a single
output:&lt;/p&gt;
&lt;img alt="A single node f(x) with one input and one output" class="align-center" src="https://eli.thegreenplace.net/images/2025/linear-node.png" /&gt;
&lt;p&gt;In all these examples, we assume the full graph output is &lt;em&gt;S&lt;/em&gt;, and its
derivative by the node's outputs is
&lt;object class="valign-m9" data="https://eli.thegreenplace.net/images/math/f57c6269b575f0c225f18e0bef2f974a5a048802.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial f}&lt;/object&gt;.
We're then interested in finding &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/0367612a0ebda078dea5f14a1fe973802a982d52.svg" style="height: 23px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial x}&lt;/object&gt;.
Since since &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/77d5e13c951f5a369089f05a0505728896773ed3.svg" style="height: 16px;" type="image/svg+xml"&gt;f:\mathbb{R}\to\mathbb{R}&lt;/object&gt;, the Jacobian is just a scalar:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/507412aef50458ce55fd10110906db71979a548b.svg" style="height: 37px;" type="image/svg+xml"&gt;\[Df=\frac{\partial f}{\partial x}\]&lt;/object&gt;
&lt;p&gt;And the chain rule is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9d9fc987a32c5c653bccfce4a89ee860b14286ff.svg" style="height: 41px;" type="image/svg+xml"&gt;\[D(S\circ f)=DS(f)\cdot Df=\frac{\partial S}{\partial f}\frac{\partial f}{\partial x}\]&lt;/object&gt;
&lt;p&gt;No surprises so far - this is just the single variable chain rule!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fan-in"&gt;
&lt;h3&gt;Fan-in&lt;/h3&gt;
&lt;p&gt;Let's move on to the next scenario, where &lt;em&gt;f&lt;/em&gt; has two inputs:&lt;/p&gt;
&lt;img alt="A single node f(x1,x2) with two inputs and one output" class="align-center" src="https://eli.thegreenplace.net/images/2025/fan-in-node.png" /&gt;
&lt;p&gt;Once again, we already have the derivative &lt;object class="valign-m9" data="https://eli.thegreenplace.net/images/math/f57c6269b575f0c225f18e0bef2f974a5a048802.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial f}&lt;/object&gt;
available, and we're interested in finding the derivative of &lt;em&gt;S&lt;/em&gt; w.r.t. the
inputs.&lt;/p&gt;
&lt;p&gt;In this case, &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/20b92036dc896227ef062a8bbe58ba8c022805ab.svg" style="height: 19px;" type="image/svg+xml"&gt;f:\mathbb{R}^2\to\mathbb{R}&lt;/object&gt;, so the Jacobian is a 1x2
matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0d81f11639f8d8506ca7bdc461d5e5d3a9cc2e5b.svg" style="height: 42px;" type="image/svg+xml"&gt;\[Df=\left [
  \frac{\partial f}{\partial x_1} \quad \frac{\partial f}{\partial x_2}
\right ]\]&lt;/object&gt;
&lt;p&gt;And the chain rule here means multiplying a 1x1 matrix by a 1x2 matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/057bba77db0d7abde54de3542704c4f5e0c32ff9.svg" style="height: 42px;" type="image/svg+xml"&gt;\[D(S\circ f)=DS(f)\cdot Df=
  \left [ \frac{\partial S}{\partial f} \right ]
  \left [ \frac{\partial f}{\partial x_1} \quad \frac{\partial f}{\partial x_2} \right ]
= \left [ \frac{\partial S}{\partial f} \frac{\partial f}{\partial x_1} \quad \frac{\partial S}{\partial f} \frac{\partial f}{\partial x_2} \right ]\]&lt;/object&gt;
&lt;p&gt;Therefore, we see that the output derivative propagates to each input
separately:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/781df9a4c6233932820bc19c4a6b155b86522aa5.svg" style="height: 87px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial S}{\partial x_1}&amp;amp;=\frac{\partial S}{\partial f} \frac{\partial f}{\partial x_1}\\
  \frac{\partial S}{\partial x_2}&amp;amp;=\frac{\partial S}{\partial f} \frac{\partial f}{\partial x_2}
\end{align*}\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="fan-out"&gt;
&lt;h3&gt;Fan-out&lt;/h3&gt;
&lt;p&gt;In the most general case, &lt;em&gt;f&lt;/em&gt; may have multiple inputs but its output may also
be used by more than one other node. As a concrete example, here's a node with
three inputs and an output that's used in two places:&lt;/p&gt;
&lt;img alt="A single node f(x1,x2,x3) with three inputs and two outputs" class="align-center" src="https://eli.thegreenplace.net/images/2025/fan-out-node.png" /&gt;
&lt;p&gt;While we denote each output edge from &lt;em&gt;f&lt;/em&gt; with a different name, &lt;em&gt;f&lt;/em&gt;
has a single output! This point is a bit subtle and important to dwell on:
yes, &lt;em&gt;f&lt;/em&gt; has a single output, so in the forward calculation both &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/0b35cc5e94a0d7682217d901f757b70990808891.svg" style="height: 16px;" type="image/svg+xml"&gt;f_1&lt;/object&gt;
and &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/9f68396d20ac17fe4705c1bf347774699bc27a3e.svg" style="height: 16px;" type="image/svg+xml"&gt;f_2&lt;/object&gt; will have the same value. However, we have to treat them
differently for the derivative calculation, because it's very possible that
&lt;object class="valign-m9" data="https://eli.thegreenplace.net/images/math/18c0d7d012453644b8448317709f2e54ebf0599c.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial f_1}&lt;/object&gt; and &lt;object class="valign-m9" data="https://eli.thegreenplace.net/images/math/ceaa592c34f5079817c6abcfa6fabf5b2a3ed061.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial f_2}&lt;/object&gt;
are different!&lt;/p&gt;
&lt;p&gt;In other words, we're reusing the machinery of multi-output functions here.
If &lt;em&gt;f&lt;/em&gt; had multiple outputs (e.g. a vector function), everything would work
exactly the same.&lt;/p&gt;
&lt;p&gt;In this case, since we treat &lt;em&gt;f&lt;/em&gt; as &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/c0ac304ca884452f91df6662e7f6919db48d69f9.svg" style="height: 19px;" type="image/svg+xml"&gt;f:\mathbb{R}^3\to\mathbb{R}^2&lt;/object&gt;,
its Jacobian is a 2x3 matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1a29aa7661d6293f39df5cf65181e04848ed6b82.svg" style="height: 75px;" type="image/svg+xml"&gt;\[Df=
\begin{bmatrix}
  \frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \frac{\partial f_1}{\partial x_3} \\ \\
  \frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \frac{\partial f_2}{\partial x_3} \\
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;The Jacobian &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a6de1a158410623c078f4274fe7c35e95fa36d98.svg" style="height: 19px;" type="image/svg+xml"&gt;DS(f)&lt;/object&gt; is a 1x2 matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/7aae6e867bb005f41aed0591aee6d837dff93795.svg" style="height: 42px;" type="image/svg+xml"&gt;\[DS(f)=\left [ \frac{\partial S}{\partial f_1} \quad \frac{\partial S}{\partial f_2} \right ]\]&lt;/object&gt;
&lt;p&gt;Applying the chain rule:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/885af1372b513038aa1d6f52fc9e2d69e4e8f9a1.svg" style="height: 123px;" type="image/svg+xml"&gt;\[\begin{align*}
D(S\circ f)=DS(f)\cdot Df&amp;amp;=
\left [ \frac{\partial S}{\partial f_1} \quad \frac{\partial S}{\partial f_2} \right ]
  \begin{bmatrix}
  \frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \frac{\partial f_1}{\partial x_3} \\ \\
  \frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \frac{\partial f_2}{\partial x_3} \\
\end{bmatrix}\\
&amp;amp;=
\left [
\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_1}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_1}\qquad
\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_2}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_2}\qquad
\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_3}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_3}
\right ]
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Therefore, we have:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/5148bfc79c2c82a178c9a0818931405d8067befd.svg" style="height: 134px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial S}{\partial x_1}&amp;amp;=\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_1}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_1}\\
  \frac{\partial S}{\partial x_2}&amp;amp;=\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_2}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_2}\\
  \frac{\partial S}{\partial x_3}&amp;amp;=\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_3}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_3}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;The key point here - which we haven't encountered before - is that the derivatives
through &lt;em&gt;f&lt;/em&gt; add up for each of its outputs (or for each copy of its output).
Qualitatively, it means that the sensitivity of &lt;em&gt;f&lt;/em&gt;'s input to the output is
the sum of its sensitivities across each output separately. This makes logical
sense, and mathematically it's just the consequence of the dot product inherent
in matrix multiplication.&lt;/p&gt;
&lt;p&gt;Now that we understand how reverse mode AD works for the more general case
of DAG nodes, let's work through a complete example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="general-dags-full-example"&gt;
&lt;h2&gt;General DAGs - full example&lt;/h2&gt;
&lt;p&gt;Consider this function (a sample used in the ADIMLAS paper):&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1568c8ae7ffdb24afc990d937585b7a779e185b2.svg" style="height: 19px;" type="image/svg+xml"&gt;\[f(x_1, x_2)=ln(x_1)+x_1 x_2-sin(x_2)\]&lt;/object&gt;
&lt;p&gt;It has two inputs and a single output; once we decompose it to primitive
operations, we can represent it with the following computational graph &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="Computational graph of f as function of x_1 and x_2" class="align-center" src="https://eli.thegreenplace.net/images/2025/fpaper-graph.png" /&gt;
&lt;p&gt;As before, we begin by running the computation forward for the values of
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/c5ea3aabcca7242c957a64cb671c96944ec70bcc.svg" style="height: 12px;" type="image/svg+xml"&gt;x_1,x_2&lt;/object&gt; at which we're interested to find the derivative. Let's take
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a1e99f7b67c1a129f77df5026f3701a5cfbdcc3f.svg" style="height: 15px;" type="image/svg+xml"&gt;x_1=2&lt;/object&gt; and &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/c2025f310759a37b724da44e26ea744680a2743e.svg" style="height: 16px;" type="image/svg+xml"&gt;x_2=5&lt;/object&gt;:&lt;/p&gt;
&lt;img alt="Computational graph with forward calculation at 2, 5" class="align-center" src="https://eli.thegreenplace.net/images/2025/fpaper-graph-forward-calc.png" /&gt;
&lt;p&gt;Recall that our goal is to calculate &lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/92a7ef7129edaf480d777987e2b69306fff75f87.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial f}{\partial x_1}&lt;/object&gt;
and &lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/0af90c749150f76175dd2af7dc930774ab7579c0.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial f}{\partial x_2}&lt;/object&gt;. Initially we know that
&lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/dc5d153e1b607ee33620d8c97cc46edf61fa7696.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial f}{\partial v_5}=1&lt;/object&gt; &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Starting with the &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/ca513d085e9f35047a4c85c1759e63b9f3d23e5b.svg" style="height: 11px;" type="image/svg+xml"&gt;v_5&lt;/object&gt; node, let's use the fan-in formulas developed
earlier:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b45f5a7e39c1f64899b95d2ea3ef7562401b3eeb.svg" style="height: 85px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial f}{\partial v_4}&amp;amp;=\frac{\partial f}{\partial v_5} \frac{\partial v_5}{\partial v_4}=1\cdot 1=1\\
  \frac{\partial f}{\partial v_3}&amp;amp;=\frac{\partial f}{\partial v_5} \frac{\partial v_5}{\partial v_3}=1\cdot -1=-1
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Next, let's tackle &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/9a69ccb973264d203d460f9a7e0df867faf7e918.svg" style="height: 11px;" type="image/svg+xml"&gt;v_4&lt;/object&gt;. It also has a fan-in configuration, so we'll
use similar formulas, plugging in the value of &lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/39e8a55c9a3ae7936e835846a6ed4c631922e039.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial f}{\partial v_4}&lt;/object&gt; we've just
calculated:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e735c4d3d5ef555607e8acde0f84310291277101.svg" style="height: 85px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial f}{\partial v_1}&amp;amp;=\frac{\partial f}{\partial v_4} \frac{\partial v_4}{\partial v_1}=1\cdot 1=1\\
  \frac{\partial f}{\partial v_2}&amp;amp;=\frac{\partial f}{\partial v_4} \frac{\partial v_4}{\partial v_2}=1\cdot 1=1
\end{align*}\]&lt;/object&gt;
&lt;p&gt;On to &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/9b12bbf79036cb3e904f971fd86838db1dade1aa.svg" style="height: 12px;" type="image/svg+xml"&gt;v_1&lt;/object&gt;. It's a simple linear node, so:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/55bb2c892485048d8ccb9796017d2002cd205231.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial x_1}^{(1)}=\frac{\partial f}{\partial v_1} \frac{\partial v_1}{\partial x_1}=1\cdot \frac{1}{x_1}=0.5\]&lt;/object&gt;
&lt;p&gt;Note the (1) superscript though! Since &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/593f4cff5d4210d46e140db57bafc4f692493f76.svg" style="height: 11px;" type="image/svg+xml"&gt;x_1&lt;/object&gt; is a fan-out node, it will have
more than one contribution to its derivative; we've just computed the one from
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/9b12bbf79036cb3e904f971fd86838db1dade1aa.svg" style="height: 12px;" type="image/svg+xml"&gt;v_1&lt;/object&gt;. Next, let's compute the one from &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/2e84f52c0f54659a1f533b25591adb924f2a4131.svg" style="height: 11px;" type="image/svg+xml"&gt;v_2&lt;/object&gt;. That's another
fan-in node:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/abb4615332fa4d17f90efc39ef6574223a66c901.svg" style="height: 94px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial f}{\partial x_1}^{(2)}&amp;amp;=\frac{\partial f}{\partial v_2} \frac{\partial v_2}{\partial x_1}=1\cdot x_2=5\\
  \frac{\partial f}{\partial x_2}^{(1)}&amp;amp;=\frac{\partial f}{\partial v_2} \frac{\partial v_2}{\partial x_2}=1\cdot x_1=2
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We've calculated the other contribution to the &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/593f4cff5d4210d46e140db57bafc4f692493f76.svg" style="height: 11px;" type="image/svg+xml"&gt;x_1&lt;/object&gt; derivative, and the
first out of two contributions for the &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a8728ff397f08f1999170f64ff5838333f755380.svg" style="height: 11px;" type="image/svg+xml"&gt;x_2&lt;/object&gt; derivative. Next, let's
handle &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/73db09ec63501c91f822ea29cefadf3bb9837084.svg" style="height: 11px;" type="image/svg+xml"&gt;v_3&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6a9dce7fcdeee31cdf6dead194b8d3898b04db61.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial x_2}^{(2)}=\frac{\partial f}{\partial v_3} \frac{\partial v_3}{\partial x_2}=-1\cdot cos(x_2)=-0.28\]&lt;/object&gt;
&lt;p&gt;Finally, we're ready to add up the derivative contributions for the input arguments.
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/593f4cff5d4210d46e140db57bafc4f692493f76.svg" style="height: 11px;" type="image/svg+xml"&gt;x_1&lt;/object&gt; is a &amp;quot;fan-out&amp;quot; node, with two outputs. Recall from the section above
that we just sum their contributions:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2905b796b72079d5677b85735f0e8715a5bfd0da.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial x_1}=\frac{\partial f}{\partial x_1}^{(1)}+\frac{\partial f}{\partial x_1}^{(2)}=0.5+5=5.5\]&lt;/object&gt;
&lt;p&gt;And:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/98bfd4fb2ad61c35ffc3d5e45ff26c0886829a6e.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial x_2}=\frac{\partial f}{\partial x_2}^{(1)}+\frac{\partial f}{\partial x_2}^{(2)}=2-0.28=1.72\]&lt;/object&gt;
&lt;p&gt;And we're done! Once again, it's easy to verify - using a calculator and the
analytical derivatives of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/c644ca35529294cbdae3f76b2dab55120f5cbdbf.svg" style="height: 19px;" type="image/svg+xml"&gt;f(x_1,x_2)&lt;/object&gt; - that these are the right
derivatives at the given points.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="backpropagation-in-ml-reverse-mode-ad-and-vjps"&gt;
&lt;h2&gt;Backpropagation in ML, reverse mode AD and VJPs&lt;/h2&gt;
&lt;p&gt;A quick note on reverse mode AD vs forward mode (please read the ADIMLAS paper
for much more details):&lt;/p&gt;
&lt;p&gt;Reverse mode AD is the approach commonly used for machine learning and neural
networks, because these tend to have a scalar &lt;em&gt;loss&lt;/em&gt; (or &lt;em&gt;error&lt;/em&gt;) output that we
want to minimize. In reverse mode, we have to run AD once per output, while in
forward mode we'd have to run it once per &lt;em&gt;input&lt;/em&gt;. Therefore, when the input
size is much larger than the output size (as is the case in NNs), reverse mode
is preferable.&lt;/p&gt;
&lt;p&gt;There's another advantage, and it relates to the term &lt;em&gt;vector-jacobian product&lt;/em&gt;
(VJP) that you will definitely run into once you start digging deeper in this
domain.&lt;/p&gt;
&lt;p&gt;The VJP is basically a fancy way of saying &amp;quot;using the chain rule in reverse mode
AD&amp;quot;. Recall that in the most general case, the multivariate chain rule is:&lt;/p&gt;
&lt;img alt="\[D(f \circ g)(a)=Df(g(a)) \cdot Dg(a)\]" class="align-center" src="https://eli.thegreenplace.net/images/math/00bdefa904bd34df2dfb50cc385e6497c4e5096e.png" style="height: 18px;" /&gt;
&lt;p&gt;However, in the case of reverse mode AD, we typically have a single output
from the full graph, so &lt;img alt="Df(g(a))" class="valign-m4" src="https://eli.thegreenplace.net/images/math/e567730c48bb2f95c258b630b4d6e997043e09ab.png" style="height: 18px;" /&gt; is a row vector. The chain rule
then means multiplying this row vector by a matrix representing the node's
jacobian. This is the &lt;em&gt;vector-jacobian product&lt;/em&gt;, and its output is another
row vector. Scroll back to the &lt;em&gt;Fan-out&lt;/em&gt; sample to see an example of this.&lt;/p&gt;
&lt;p&gt;This may not seem very profound so far, but it carries an important meaning in
terms of computational efficiency. For each node in the graph, we don't have
to store its complete jacobian; all we need is a function that takes a row
vector and produces the VJP. This is important because jacobians can be very
large and very sparse &lt;a class="footnote-reference" href="#footnote-4" id="footnote-reference-4"&gt;[4]&lt;/a&gt;. In practice, this means that when AD libraries
define the derivative of a computation node, they don't ask you to register
a complete jacobian for each operation, but rather a VJP.&lt;/p&gt;
&lt;p&gt;This also provides an additional way to think about the relative efficiency of
reverse mode AD for ML
applications; since a graph typically has many inputs (all the weights), and a
single output (scalar loss), accumulating from the end going backwards means the
intermediate products are VJPs that are row vectors; accumulating from the front
would mean multiplying full jacobians together, and the intermediate results
would be matrices &lt;a class="footnote-reference" href="#footnote-5" id="footnote-reference-5"&gt;[5]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-simple-python-implementation-of-reverse-mode-ad"&gt;
&lt;h2&gt;A simple Python implementation of reverse mode AD&lt;/h2&gt;
&lt;p&gt;Enough equations, let's see some code! The whole point of AD is that it's
&lt;em&gt;automatic&lt;/em&gt;, meaning that it's simple to implement in a program. What follows
is the simplest implementation I could think of; it requires one to build
expressions out of a special type, which can then calculate gradients
automatically.&lt;/p&gt;
&lt;p&gt;Let's start with some usage samples; here's the Sigmoid calculation presented
earlier:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sigmoid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;xx = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, sigmoid = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;dsigmoid/dxx = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We begin by building the Sigmoid expression using &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; values (more on this
later). We can then run the &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; method on a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;, with an output
gradient of 1.0 and see that the gradient for &lt;tt class="docutils literal"&gt;xx&lt;/tt&gt; is 0.24, as calculated
before.&lt;/p&gt;
&lt;p&gt;Here's the expression we used for the DAG section:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;x1 = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, x2 = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, f = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;df/dx1 = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, df/dx2 = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once again, we build up the expression, then call &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; on the final value.
It will populate the &lt;tt class="docutils literal"&gt;gv&lt;/tt&gt; attributes of input &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;s with the derivatives
calculated w.r.t. these inputs.&lt;/p&gt;
&lt;p&gt;Let's see how &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; works. The high-level overview is:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; represents a node in the computational graph we've been discussing
in this post.&lt;/li&gt;
&lt;li&gt;Using operator overloading and custom math functions (like the &lt;tt class="docutils literal"&gt;exp&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;sin&lt;/tt&gt;
and &lt;tt class="docutils literal"&gt;log&lt;/tt&gt; seen in the samples above), when an expression is constructed
out of &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; values, we also build the computational graph in the
background. Each &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; has links to its &lt;em&gt;predecessors&lt;/em&gt; in the graph (the
other &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;s that feed into it).&lt;/li&gt;
&lt;li&gt;When the &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; method is called, it runs reverse mode AD through the
computational graph, using the chain rule.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's the &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; class:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;v&lt;/tt&gt; is the value (forward calculation) of this &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;. &lt;tt class="docutils literal"&gt;predecessors&lt;/tt&gt; is
the list of predecessors, each of this type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@dataclass&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;multiplier&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;
    &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Var&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Consider the &lt;tt class="docutils literal"&gt;v5&lt;/tt&gt; node in DAG sample, for example. It represents the
calculation &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;v4-v3&lt;/span&gt;&lt;/tt&gt;. The &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; representing &lt;tt class="docutils literal"&gt;v5&lt;/tt&gt; will have a list of
two predecessors, one for &lt;tt class="docutils literal"&gt;v4&lt;/tt&gt; and one for &lt;tt class="docutils literal"&gt;v3&lt;/tt&gt;. Each of these will
have a &amp;quot;multiplier&amp;quot; associated with it:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;For &lt;tt class="docutils literal"&gt;v3&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;Predecessor.var&lt;/tt&gt; points to the &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; representing &lt;tt class="docutils literal"&gt;v3&lt;/tt&gt;
and &lt;tt class="docutils literal"&gt;Predecessor.multiplier&lt;/tt&gt; is -1, since this is the derivative
of &lt;tt class="docutils literal"&gt;v5&lt;/tt&gt; w.r.t. &lt;tt class="docutils literal"&gt;v3&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Similarly, for &lt;tt class="docutils literal"&gt;v4&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;Predecessor.var&lt;/tt&gt; points to the &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; representing
&lt;tt class="docutils literal"&gt;v4&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;Predecessor.multiplier&lt;/tt&gt; is 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's see some overloaded operators of &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; &lt;a class="footnote-reference" href="#footnote-6" id="footnote-reference-6"&gt;[6]&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__add__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensure_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;

&lt;span class="c1"&gt;# ...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__mul__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensure_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And some of the custom math functions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;log(x) - natural logarithm of x&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensure_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;sin(x)&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensure_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note how the multipliers for each node are exactly the derivatives of its
output w.r.t. corresponding input. Notice also that in some cases we use the
forward calculated value of a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;'s inputs to calculate this derivative
(e.g. in the case of &lt;tt class="docutils literal"&gt;sin(x)&lt;/tt&gt;, the derivative is &lt;tt class="docutils literal"&gt;cos(x)&lt;/tt&gt;, so we need the
actual value of &lt;tt class="docutils literal"&gt;x&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;Finally, this is the &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;gv&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiplier&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Some notes about this method:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;It has to be invoked on a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; node that represents the entire computation.&lt;/li&gt;
&lt;li&gt;Since this function walks the graph backwards (from the outputs to the inputs),
this is the direction our graph edges are pointing (we keep track of the
predecessors of each node, not the successors).&lt;/li&gt;
&lt;li&gt;Since we typically want the derivative of some output &amp;quot;loss&amp;quot; w.r.t. each
&lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;, the computation will usually start with &lt;tt class="docutils literal"&gt;grad(1.0)&lt;/tt&gt;, because the
output of the entire computation &lt;em&gt;is&lt;/em&gt; the loss.&lt;/li&gt;
&lt;li&gt;For each node, &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; adds the incoming gradient to its own, and propagates
the incoming gradient to each of its predecessors, using the relevant
multiplier.&lt;/li&gt;
&lt;li&gt;The addition &lt;tt class="docutils literal"&gt;self.gv += gv&lt;/tt&gt; is key to managing nodes with fan-out. Recall
our discussion from the DAG section - according to the multivariate chain rule,
fan-out nodes' derivatives add up for each of their outputs.&lt;/li&gt;
&lt;li&gt;This implementation of &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; is very simplistic and inefficient because it
will process the same &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; multiple times in complex graphs. A more
efficient implementation would sort the graph topologically first and then
would only have to visit each &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; once.&lt;/li&gt;
&lt;li&gt;Since the gradient of each &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; adds up, one shouldn't be reusing &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;s
between different computations. Once &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; was run, the &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; should
not be used for other &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; calculations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The full code for this sample is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2025/rad/rad.py"&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The goal of this post is to serve as a supplement for the ADIMLAS paper; once
again, if the topic of AD is interesting to you, I strongly encourage you to
read the paper! I hope this post added something on top - please let me know
if you have any questions.&lt;/p&gt;
&lt;p&gt;Industrial strength implementations of AD, like &lt;a class="reference external" href="https://github.com/HIPS/autograd/"&gt;autograd&lt;/a&gt;
and &lt;a class="reference external" href="https://github.com/jax-ml/jax"&gt;JAX&lt;/a&gt;, have much better ergonomics and
performance than the toy implementation shown above. That said, the underlying
principles are similar - reverse mode AD on computational graphs. To explore
how such a system works, see my
&lt;a class="reference external" href="https://github.com/eliben/radgrad"&gt;radgrad&lt;/a&gt; project.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;In this post we're only looking at single-output graphs, however, since
these are typically sufficient in machine learning (the output is some
scalar &amp;quot;loss&amp;quot; or &amp;quot;error&amp;quot; that we're trying to minimize). That said,
for functions with multiple outputs the process is very similar - we just
have to run the reverse mode AD process for each output variable
separately.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Note that the notation here is a bit different from the one used for
the sigmoid function. This notation is adopted from the ADIMLAS paper,
which uses &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/f0cd317158e9b3b19134b2c5db4e0861fcd95222.svg" style="height: 11px;" type="image/svg+xml"&gt;v_i&lt;/object&gt; for all temporary values within the graph.
I'm keeping the notations different to emphasize they have absolutely
no bearing on the math and the AD algorithm. They're just a naming
convention.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;For consistency, I'll be using the partial derivative notation
throughout this example, even for nodes that have a single input and
output.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;For an example of gigantic, sparse jacobians see my older post
on &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/backpropagation-through-a-fully-connected-layer/"&gt;backpropagation through a fully connected layer&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;There are a lot of additional nuances here to explain; I strongly
recommend this &lt;a class="reference external" href="https://videolectures.net/videos/deeplearning2017_johnson_automatic_differentiation"&gt;excellent lecture&lt;/a&gt;
by &lt;a class="reference external" href="https://github.com/mattjj"&gt;Matthew Johnson&lt;/a&gt; (of JAX and autograd
fame) for a deeper overview.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-6"&gt;[6]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;These use the utility function &lt;tt class="docutils literal"&gt;ensure_var&lt;/tt&gt;; all it does is wrap the
its argument in a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; if it's not already a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;. This is needed
to wrap constants in the expression, to ensure that the computational
graph includes everything.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="Machine Learning"></category><category term="Python"></category></entry><entry><title>Bloch sphere</title><link href="https://eli.thegreenplace.net/2024/bloch-sphere/" rel="alternate"></link><published>2024-10-29T16:10:00-07:00</published><updated>2024-11-04T14:08:15-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-10-29:/2024/bloch-sphere/</id><summary type="html">&lt;p&gt;When learning the basics of quantum computing, the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Bloch_sphere"&gt;Bloch sphere&lt;/a&gt;
comes early on as a visualization technique of quantum states. It shows the
state of a single qubit as a point on this sphere:&lt;/p&gt;
&lt;img alt="Bloch sphere showing kets 0 and 1 in the usual places" class="align-center" src="https://eli.thegreenplace.net/images/2024/bloch-with-kets-01.png" /&gt;
&lt;p&gt;This post explains how the Bloch sphere works and also &lt;em&gt;why&lt;/em&gt; it works.&lt;/p&gt;
&lt;div class="section" id="mapping-4-dimensions-onto-a-sphere"&gt;
&lt;h2&gt;Mapping 4 dimensions â€¦&lt;/h2&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;When learning the basics of quantum computing, the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Bloch_sphere"&gt;Bloch sphere&lt;/a&gt;
comes early on as a visualization technique of quantum states. It shows the
state of a single qubit as a point on this sphere:&lt;/p&gt;
&lt;img alt="Bloch sphere showing kets 0 and 1 in the usual places" class="align-center" src="https://eli.thegreenplace.net/images/2024/bloch-with-kets-01.png" /&gt;
&lt;p&gt;This post explains how the Bloch sphere works and also &lt;em&gt;why&lt;/em&gt; it works.&lt;/p&gt;
&lt;div class="section" id="mapping-4-dimensions-onto-a-sphere"&gt;
&lt;h2&gt;Mapping 4 dimensions onto a sphere&lt;/h2&gt;
&lt;p&gt;We have a qubit in an arbitrary state:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a63b3b4185f94421fe82d6b27e83db4c871fcfb6.svg" style="height: 19px;" type="image/svg+xml"&gt;\[\ket{\psi}=a\ket{0}+b\ket{1}\]&lt;/object&gt;
&lt;p&gt;How can we visualize this state?&lt;/p&gt;
&lt;p&gt;Had &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; been real numbers, it would have been easy since there would
only be two dimensions (degrees of freedom). However, in reality
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/a40c5e2c5b3fee3c060de7e3623cfde67a0a54f2.svg" style="height: 16px;" type="image/svg+xml"&gt;a,b\in\mathbb{C}&lt;/object&gt;, making our visualization task much more challenging
because there are now 4 dimensions (two for each complex number). The Bloch
sphere is a clever mapping from this 4D reality into something we can visualize.&lt;/p&gt;
&lt;p&gt;We start by representing each of the complex coefficients using their
&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/notes-on-the-euler-formula/"&gt;polar representation&lt;/a&gt;,
where the magnitudes and angles are real numbers:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0be9ded0fff08d6e751472c127b3e40b279ddc0a.svg" style="height: 49px;" type="image/svg+xml"&gt;\[\begin{align*}
a&amp;amp;=r_a e^{i\phi_a}\\
b&amp;amp;=r_b e^{i\phi_b}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/45bd78f16aaf510347915f3d0ba14ca0f8676e28.svg" style="height: 22px;" type="image/svg+xml"&gt;\[\ket{\psi}=r_a e^{i\phi_a}\ket{0} + r_b e^{i\phi_b}\ket{1}\]&lt;/object&gt;
&lt;p&gt;Since a global phase doesn't affect the observable properties of a qubit (see
the appendix for more on this), we can multiply this state by the global
state &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/8d2d62a6083c60a9df49922270ce494c1b9fc567.svg" style="height: 15px;" type="image/svg+xml"&gt;e^{-i\phi_a}&lt;/object&gt; to get &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/02be39bdbc3a331d4bf7a7ea0a1ed596e26ae8bf.svg" style="height: 61px;" type="image/svg+xml"&gt;\[\begin{align*}
\ket{\psi}&amp;amp;=e^{-i\phi_a}\left (r_a e^{i\phi_a}\ket{0} + r_b e^{i\phi_b}\ket{1} \right ) \\
 &amp;amp;=r_a\ket{0}+r_b e^{i(\phi_b-\phi_a)}\ket{1}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;There's only a single angle in this equation: &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/a062edb59d046220003d17f70cc81374153ac27f.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi_b-\phi_a&lt;/object&gt;; this
is the &lt;em&gt;relative&lt;/em&gt; phase between the two components of the state. Let's
call it just &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt;, and then:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/239c993ea0542ee402bb3d77eed2400f7a2c51b6.svg" style="height: 22px;" type="image/svg+xml"&gt;\[\ket{\psi}=r_a\ket{0}+r_b e^{i\phi}\ket{1}\]&lt;/object&gt;
&lt;p&gt;We're down from 4 real parameters to 3. But there's still a constraint on our
state equation that we didn't use; all quantum states must be normalized - the
amplitudes must satisfy:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6a408bfe9b9e2ac643a9f2563112c11e74024ffd.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|a|^2+|b|^2=1\]&lt;/object&gt;
&lt;p&gt;In our case:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1c78aafcb7004b3f4bfe9aa9f1845b90caf2d392.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|r_a|^2+|r_b e^{i\phi}|^2=1\]&lt;/object&gt;
&lt;p&gt;To proceed from here, we'll rewrite the second magnitude using the cartesian
representation of complex numbers (instead of parameters &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/cdfe7c131e161adc750f10a87ccd098fbf8a276c.svg" style="height: 11px;" type="image/svg+xml"&gt;r_b&lt;/object&gt; and
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt;, we'll use &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;) and do some algebra:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/735fd5adac7e0e47a77befd1509ce4e21d6183c6.svg" style="height: 77px;" type="image/svg+xml"&gt;\[\begin{align*}
|r_a|^2+|x+iy|^2&amp;amp;=1 \\
{r_a}^2+(x+iy)(x-iy)&amp;amp;=1 \\
{r_a}^2+x^2+y^2&amp;amp;=1 \\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;If we rename &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/cb3719acabf42ce9628ea5488e484e3ff3958fb3.svg" style="height: 11px;" type="image/svg+xml"&gt;r_a&lt;/object&gt; to &lt;em&gt;z&lt;/em&gt;, this equation should look familiar:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/580c4e5828c85ff64ba780c9857f89283b5b6bba.svg" style="height: 21px;" type="image/svg+xml"&gt;\[x^2+y^2+z^2=1\]&lt;/object&gt;
&lt;p&gt;This is the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sphere#Equations"&gt;equation of a unit sphere&lt;/a&gt;!
We can now use &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Spherical_coordinate_system"&gt;spherical coordinates&lt;/a&gt;
to express:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3566cb7d40bd8e2b4991672b7200c15b9a83ef1a.svg" style="height: 72px;" type="image/svg+xml"&gt;\[\begin{align*}
x&amp;amp;=sin(\theta) cos(\phi) \\
y&amp;amp;=sin(\theta) sin(\phi) \\
z&amp;amp;=cos(\theta) \\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Where &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt; is the angle from the positive &lt;em&gt;z&lt;/em&gt; axis (&amp;quot;polar angle&amp;quot;) and
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt; is the angle from the positive &lt;em&gt;x&lt;/em&gt; axis (&amp;quot;azimuthal angle&amp;quot;):&lt;/p&gt;
&lt;img alt="Unit sphere with spherical coordinates" class="align-center" src="https://eli.thegreenplace.net/images/2024/unit-sphere-polar.png" /&gt;
&lt;p&gt;If you recall, we expressed &lt;em&gt;a&lt;/em&gt; in our quantum state as &lt;em&gt;z&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; as
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/dc5648d309ad9fa02d53124cd7d954697f66d39c.svg" style="height: 16px;" type="image/svg+xml"&gt;x+iy&lt;/object&gt;. Substituting back into the state equation:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/adea20be1037a6345308eb2c42c05caea1619503.svg" style="height: 102px;" type="image/svg+xml"&gt;\[\begin{align*}
\ket{\psi}&amp;amp;=z\ket{0}+(x+iy)\ket{1}\\
&amp;amp;=cos(\theta)\ket{0}+(sin(\theta)cos(\phi) + i sin(\theta)sin(\phi))\ket{1}\\
&amp;amp;=cos(\theta)\ket{0}+sin(\theta)(cos(\phi) + i sin(\phi))\ket{1}\\
&amp;amp;=cos(\theta)\ket{0}+sin(\theta)e^{i \phi}\ket{1}\\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We're now down to just 2 real parameters: the angles &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt; and
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt;. These two angles uniquely map a quantum state onto a unit sphere.&lt;/p&gt;
&lt;p&gt;Now comes the trickiest part in understanding the Bloch sphere, IMHO. Notice
that something is wrong with the mapping we've just found. In spherical
coordinates, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6ac3c9b7281d43fe8cec91958a20e2750285cad9.svg" style="height: 15px;" type="image/svg+xml"&gt;$$0\leq\theta\leq \pi$$&lt;/object&gt;. However, if we set &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/35d63cd9f27cd61eae24d22e3869f0ce0a2acceb.svg" style="height: 12px;" type="image/svg+xml"&gt;\theta=\pi&lt;/object&gt;
in the state equation we've just derived, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/384c07724f4cdc2a000d76ac2303521000cea56b.svg" style="height: 19px;" type="image/svg+xml"&gt;\[\ket{\psi}=-\ket{0}\]&lt;/object&gt;
&lt;p&gt;Which is &lt;em&gt;not&lt;/em&gt; what we see on the Bloch sphere; indeed, since the states
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6b461a7c4ec702afa72360726f18f43fc7d785a1.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{0}&lt;/object&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt; are orthogonal by definition, we should
not be seeing &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt; as the polar opposite of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6b461a7c4ec702afa72360726f18f43fc7d785a1.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{0}&lt;/object&gt; on the
Bloch sphere, and yet we do. What's going on?&lt;/p&gt;
&lt;p&gt;The simple (but partial) explanation is that we'd need only half a sphere
to represent states, but it wouldn't have the nice visual properties of the
full Bloch sphere. There are deeper mathematical reasons for this - see
discussions &lt;a class="reference external" href="https://physics.stackexchange.com/questions/174562/why-is-theta-over-2-used-for-a-bloch-sphere-instead-of-theta"&gt;#1&lt;/a&gt;
and &lt;a class="reference external" href="https://quantumcomputing.stackexchange.com/questions/4118/why-are-half-angles-used-in-the-bloch-sphere-representation-of-qubits"&gt;#2&lt;/a&gt;
for more details.&lt;/p&gt;
&lt;p&gt;The usual mathematical trick is to stretch the state space over an entire
sphere by using &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/fb50c3fe8ab16c3bc51af81f89862dd768effcd8.svg" style="height: 22px;" type="image/svg+xml"&gt;\frac{\theta}{2}&lt;/object&gt; instead of &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt;. Therefore,
our state would be:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8418b36e6c0230cfb3867aa4ba3bdb3dea774b94.svg" style="height: 43px;" type="image/svg+xml"&gt;\[\ket{\psi}=cos\left (\frac{\theta}{2} \right )\ket{0}+sin\left (\frac{\theta}{2} \right )e^{i \phi}\ket{1}\]&lt;/object&gt;
&lt;p&gt;Now if we set &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/35d63cd9f27cd61eae24d22e3869f0ce0a2acceb.svg" style="height: 12px;" type="image/svg+xml"&gt;\theta=\pi&lt;/object&gt;, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c45966410875507bf789ea509bd8554fd8e5a3c0.svg" style="height: 22px;" type="image/svg+xml"&gt;\[\ket{\psi}=0\cdot\ket{0}+e^{i\phi}\ket{1}\equiv\ket{1}\]&lt;/object&gt;
&lt;p&gt;Once again, the equivalence to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt; is because now that the phase
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt; applies only to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt;, it makes no difference.
Indeed, if we have a point on the &lt;em&gt;z&lt;/em&gt; axis, it doesn't really matter what its
azimuthal angle is.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="using-the-bloch-sphere-to-visualize-quantum-states"&gt;
&lt;h2&gt;Using the Bloch sphere to visualize quantum states&lt;/h2&gt;
&lt;p&gt;Now that we understand the math behind the mapping, let's use it to visualize
some quantum state superpositions on the Bloch sphere.&lt;/p&gt;
&lt;p&gt;We'll start with the important state &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/f1c98d68542f8a94e29c79e5e65768469360e707.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{+}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6af9c454692362b2fce37aa6406e66f376883cda.svg" style="height: 40px;" type="image/svg+xml"&gt;\[\ket{+}=\frac{1}{\sqrt{2}}\left (\ket{0}+\ket{1} \right )\]&lt;/object&gt;
&lt;p&gt;To find the angles for the spherical representation:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8418b36e6c0230cfb3867aa4ba3bdb3dea774b94.svg" style="height: 43px;" type="image/svg+xml"&gt;\[\ket{\psi}=cos\left (\frac{\theta}{2} \right )\ket{0}+sin\left (\frac{\theta}{2} \right )e^{i \phi}\ket{1}\]&lt;/object&gt;
&lt;p&gt;We notice that &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/863965f01f19330eab0895c6fe94ce1b3263d91a.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi=0&lt;/object&gt; because the coefficient of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt; is
real. Also:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/aa76639e3630fc599b822ac8ccb3f00f9a4eb5b1.svg" style="height: 43px;" type="image/svg+xml"&gt;\[\frac{\theta}{2}=cos^{-1}\left (\frac{1}{\sqrt{2}}\right ) \Longrightarrow \theta=\frac{\pi}{2}\]&lt;/object&gt;
&lt;p&gt;Recalling the meaning of &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt; and &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt; in spherical
coordinates, this means that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/f1c98d68542f8a94e29c79e5e65768469360e707.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{+}&lt;/object&gt; is on the positive &lt;em&gt;x&lt;/em&gt; axis. Here
is this state on the Bloch sphere, along with other important states
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e15b266d80354b55bd364547e650588797cc10f6.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{-}&lt;/object&gt;, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/484eb17230e803270870cd1a6cbc42008dfd42a0.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{i}&lt;/object&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/638a583ff638ffde63113246f47f0e1fd1abb0e3.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{-i}&lt;/object&gt;, which are similarly
simple to compute:&lt;/p&gt;
&lt;img alt="Bloch sphere showing kets 0, 1, +, -, i, -i" class="align-center" src="https://eli.thegreenplace.net/images/2024/bloch-with-basic-states.png" /&gt;
&lt;p&gt;The measurement probabilities of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/f1c98d68542f8a94e29c79e5e65768469360e707.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{+}&lt;/object&gt; in the standard basis are
&lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/eb946338365d9781f7d2e9ec692c26702d0ae3a7.svg" style="height: 22px;" type="image/svg+xml"&gt;\frac{1}{2}&lt;/object&gt; for 0 and &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/eb946338365d9781f7d2e9ec692c26702d0ae3a7.svg" style="height: 22px;" type="image/svg+xml"&gt;\frac{1}{2}&lt;/object&gt; for 1 &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;. The Bloch sphere makes
it geometrically intuitive, because &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/f1c98d68542f8a94e29c79e5e65768469360e707.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{+}&lt;/object&gt; lies on the &amp;quot;equator&amp;quot;,
half-way between &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6b461a7c4ec702afa72360726f18f43fc7d785a1.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{0}&lt;/object&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt;. Note that all the
states depicted above have these measurement probabilities - the only difference
between them is the relative phase, which is expressed with the &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt;
angle on the sphere.&lt;/p&gt;
&lt;p&gt;As the final example, let's map the following state onto the Bloch sphere:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/346da634519f91d3e34dca04cf2cfe525cd64f49.svg" style="height: 40px;" type="image/svg+xml"&gt;\[\ket{\psi}=\frac{3+i\sqrt{3}}{4}\ket{0}-\frac{1}{2}\ket{1}\]&lt;/object&gt;
&lt;p&gt;Rearranging a bit:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/57b0f70624be5a1a4b25ae190f6f2cc9b5ec24f7.svg" style="height: 54px;" type="image/svg+xml"&gt;\[\ket{\psi}=\frac{\sqrt{3}}{2}\left (\frac{\sqrt{3}}{2}+i\frac{1}{2}\right )\ket{0}-\frac{1}{2}\ket{1}\]&lt;/object&gt;
&lt;p&gt;Since &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/3a2a06328699cf55bff36f68bca03d4867809d2e.svg" style="height: 22px;" type="image/svg+xml"&gt;sin(\pi/6)=\frac{1}{2}&lt;/object&gt; and &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/47c6055c65e5687c104d3023f120baad97a03d0d.svg" style="height: 25px;" type="image/svg+xml"&gt;cos(\pi/6)=\frac{\sqrt{3}}{2}&lt;/object&gt;,
we can rewrite it as:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2370970cdcec491e0a78395ac09a6c1cefefb40b.svg" style="height: 40px;" type="image/svg+xml"&gt;\[\ket{\psi}=\frac{\sqrt{3}}{2}e^{i\pi/6}\ket{0}-\frac{1}{2}\ket{1}\]&lt;/object&gt;
&lt;p&gt;To get into the spherical coordinate representation, we have to move the
relative phase to the &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt;. To do that, we'll factor out
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/c491b6741e7886066364f39c0c06d5d99bebad88.svg" style="height: 16px;" type="image/svg+xml"&gt;e^{i\pi/6}&lt;/object&gt; and then ignore the global phase. We'll then use the
fact that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/445f8fe14bdc777990faaf6a92e624c9eb1446e1.svg" style="height: 16px;" type="image/svg+xml"&gt;e^{-i\pi/6}=e^{i5\pi/6}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6041ae8707c629a2a07d186a8853d96c3df32d60.svg" style="height: 146px;" type="image/svg+xml"&gt;\[\begin{align*}
\ket{\psi}&amp;amp;=\frac{\sqrt{3}}{2}e^{i\pi/6}\ket{0}-\frac{1}{2}\ket{1}\\
&amp;amp;=e^{i\pi/6}\left(\frac{\sqrt{3}}{2}\ket{0}-e^{-i\pi/6}\frac{1}{2}\ket{1}\right )\\
&amp;amp;\equiv\frac{\sqrt{3}}{2}\ket{0}+e^{i5\pi/6}\frac{1}{2}\ket{1}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Now we're ready to extract the angles:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/cfe93f7d3af5e3652da02da879fe4e5a40b9bd72.svg" style="height: 96px;" type="image/svg+xml"&gt;\[\begin{align*}
\frac{\theta}{2}&amp;amp;=cos^{-1}\left (\frac{\sqrt{3}}{2}\right ) \Longrightarrow \theta = \frac{\pi}{3}\\
\phi&amp;amp;=\frac{5\pi}{6}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Here's how this looks on the sphere:&lt;/p&gt;
&lt;img alt="Bloch sphere showing psi" class="align-center" src="https://eli.thegreenplace.net/images/2024/bloch-phi-pi-6.png" /&gt;
&lt;p&gt;Visually, we get the intuitive sense that the probability of measuring
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6b461a7c4ec702afa72360726f18f43fc7d785a1.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{0}&lt;/object&gt; is
higher than the probability of measuring &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt;, for example. Also, if
we measure using another base like &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/f1c98d68542f8a94e29c79e5e65768469360e707.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{+}&lt;/object&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e15b266d80354b55bd364547e650588797cc10f6.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{-}&lt;/object&gt;, it's
apparent that the probability of measuring &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e15b266d80354b55bd364547e650588797cc10f6.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{-}&lt;/object&gt; will be higher, and
so on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="appendix-equivalence-of-global-phases"&gt;
&lt;h2&gt;Appendix: Equivalence of global phases&lt;/h2&gt;
&lt;p&gt;Let's take a qubit in this superposition:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f862d0020e1490cadeeeeec1c7c7b0dfe4fb9600.svg" style="height: 40px;" type="image/svg+xml"&gt;\[\psi=\frac{\sqrt{3}}{2}\ket{0}+\frac{1}{2}\ket{1}\]&lt;/object&gt;
&lt;p&gt;The probabilities of measurements in the standard base are:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/59bc2b214542e2bf031ff2a60346a96d17dd4881.svg" style="height: 109px;" type="image/svg+xml"&gt;\[\begin{align*}
  0\longrightarrow\left|\frac{\sqrt{3}}{2}\right|^2&amp;amp;=\frac{3}{4}\\
  1\longrightarrow\left|\frac{1}{2}\right|^2&amp;amp;=\frac{1}{4}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Now let's apply a global phase &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt; to our qubit:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/4e2cce421bc6291a7bcb7daa8387b17b8ef36c92.svg" style="height: 54px;" type="image/svg+xml"&gt;\[\hat{\psi}=e^{i\phi}\left(\frac{\sqrt{3}}{2}\ket{0}+\frac{1}{2}\ket{1}\right)\]&lt;/object&gt;
&lt;p&gt;And calculate the measurement probabilities again:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/fda63c6aa7775f911388c1dbf45c87251d901edd.svg" style="height: 109px;" type="image/svg+xml"&gt;\[\begin{align*}
  0\longrightarrow\left|e^{i\phi}\frac{\sqrt{3}}{2}\right|^2=\frac{\sqrt{3}}{2}e^{i\phi}\cdot\frac{\sqrt{3}}{2}e^{-i\phi}&amp;amp;=\frac{3}{4}\\
  1\longrightarrow\left|\frac{1}{2}\right|^2=\frac{1}{2}e^{i\phi}\cdot\frac{1}{2}e^{-i\phi}&amp;amp;=\frac{1}{4}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;The global phase didn't change our measurement probabilities in any way;
since measurement is the only way to observe the quantum state, we say that
the global phase is irrelevant and can be ignored.&lt;/p&gt;
&lt;p&gt;As an exercise, it's easy to show that this applies for any
quantum state &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/b573b4df40ba8628e08f542cce5ff9fdf024818d.svg" style="height: 19px;" type="image/svg+xml"&gt;a\ket{0}+b\ket{1}&lt;/object&gt;. It also applies for any computational
basis, not just the standard basis &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6b461a7c4ec702afa72360726f18f43fc7d785a1.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{0}&lt;/object&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4f12cd6e6422f93a0993205418cfb42a13b4ec39.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{1}&lt;/object&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Strictly speaking, we shouldn't be using the equals sign here because
this new state isn't equal to the old one; it's equivalent to it,
so we can use the &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/265a73f0318d82099471b873fb7854c7f06ef9d4.svg" style="height: 8px;" type="image/svg+xml"&gt;\equiv&lt;/object&gt; sign. I'll just keep using
&lt;object class="valign-2" data="https://eli.thegreenplace.net/images/math/21606782c65e44cac7afbb90977d8b6f82140e76.svg" style="height: 5px;" type="image/svg+xml"&gt;=&lt;/object&gt; for simplicity, though.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;For a generalized quantum superposition &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/1f4e773c84381f25644aa1407e4a7cfe845b2033.svg" style="height: 19px;" type="image/svg+xml"&gt;\ket{\psi}=a\ket{0}+b\ket{1}&lt;/object&gt;,
the probability of measuring 0 is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/16af801e554a81bc5fcb604ec6948d46829d5683.svg" style="height: 20px;" type="image/svg+xml"&gt;|a|^2&lt;/object&gt;, and the probability
of measuring 1 is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/afd90d000ca1b52787b43dc4ad5db6a9848cb930.svg" style="height: 20px;" type="image/svg+xml"&gt;|b|^2&lt;/object&gt;. Recall that in the general case
&lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; are complex, so we have to calculate the
&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/calculating-the-norm-of-a-complex-number/"&gt;norm-squared&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Calculating the norm of a complex number</title><link href="https://eli.thegreenplace.net/2024/calculating-the-norm-of-a-complex-number/" rel="alternate"></link><published>2024-10-17T19:45:00-07:00</published><updated>2024-11-04T14:08:15-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-10-17:/2024/calculating-the-norm-of-a-complex-number/</id><summary type="html">&lt;p&gt;In this quick post I'll dispel a common confusion in the basic math
of complex numbers. It's often useful to calculate the norm-square (also
known as &lt;a class="reference external" href="https://mathworld.wolfram.com/AbsoluteSquare.html"&gt;absolute square&lt;/a&gt;)
of a complex number &lt;em&gt;z&lt;/em&gt;. This norm-square is denoted &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/360ebdabd39e1822650a9e06af52bafa42c612d1.svg" style="height: 20px;" type="image/svg+xml"&gt;|z|^2&lt;/object&gt;. One
could naively expect that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0ba9970d4320810ab52d4760cc3710f8b1aa4d2f.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|z|^2=zz\]&lt;/object&gt;
&lt;p&gt;However, that's false â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this quick post I'll dispel a common confusion in the basic math
of complex numbers. It's often useful to calculate the norm-square (also
known as &lt;a class="reference external" href="https://mathworld.wolfram.com/AbsoluteSquare.html"&gt;absolute square&lt;/a&gt;)
of a complex number &lt;em&gt;z&lt;/em&gt;. This norm-square is denoted &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/360ebdabd39e1822650a9e06af52bafa42c612d1.svg" style="height: 20px;" type="image/svg+xml"&gt;|z|^2&lt;/object&gt;. One
could naively expect that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0ba9970d4320810ab52d4760cc3710f8b1aa4d2f.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|z|^2=zz\]&lt;/object&gt;
&lt;p&gt;However, that's false! The way to calculate norm-square is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/93a6ea8b04b9e10b4ac45e52aee614cda725e38f.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|z|^2=zz^*\]&lt;/object&gt;
&lt;p&gt;Where &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/3c28f425f15b96a78602a12bc367a48e1708c344.svg" style="height: 12px;" type="image/svg+xml"&gt;z^*&lt;/object&gt; is the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Complex_conjugate"&gt;complex conjugate&lt;/a&gt; of &lt;em&gt;z&lt;/em&gt;.
But why? To understand the mechanics of this calculation, let's start by
defining what a &lt;em&gt;norm&lt;/em&gt; is.&lt;/p&gt;
&lt;div class="section" id="the-norm-of-a-complex-number"&gt;
&lt;h2&gt;The norm of a complex number&lt;/h2&gt;
&lt;p&gt;Informally, a &lt;em&gt;norm&lt;/em&gt; is a generalization of the
concept of &amp;quot;length&amp;quot; or &amp;quot;size&amp;quot;. For a real number &lt;em&gt;r&lt;/em&gt;, the norm is its absolute
value &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/5a43d0019ad191c183202219f9300ad0d99ecdf3.svg" style="height: 19px;" type="image/svg+xml"&gt;|r|&lt;/object&gt;. No matter if the number is positive or negative, the norm is
its &amp;quot;length&amp;quot; - the distance from the origin.&lt;/p&gt;
&lt;p&gt;A norm is defined similarly for complex numbers. Here's a standard geometrical
interpretation of a complex number &lt;em&gt;z&lt;/em&gt;, showing both cartesian and polar
coordinates:&lt;/p&gt;
&lt;img alt="Cartesian and polar representation of a complex number" class="align-center" src="https://eli.thegreenplace.net/images/2024/complex-cartesian-polar.png" /&gt;
&lt;p&gt;The &amp;quot;norm&amp;quot; of &lt;em&gt;z&lt;/em&gt; is the length of the blue line, or the distance of its
endpoint from the origin (&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/notes-on-the-euler-formula/"&gt;in the polar representation&lt;/a&gt;&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/0d8a226a76f59bb6b9b4b23b11fbb18118336c40.svg" style="height: 15px;" type="image/svg+xml"&gt;z=re^{i\theta}&lt;/object&gt;,
it's exactly &lt;em&gt;r&lt;/em&gt;). The norm of a complex number uses the same notation
as the absolute value: &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/0d4ebef37595cd27057748de36eab66c6d8e1e04.svg" style="height: 19px;" type="image/svg+xml"&gt;|z|&lt;/object&gt;, because it means the same thing. Another
common name for the norm of complex numbers is &lt;a class="reference external" href="https://mathworld.wolfram.com/ComplexModulus.html"&gt;modulus&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="norm-a-formal-definition"&gt;
&lt;h2&gt;Norm: a formal definition&lt;/h2&gt;
&lt;p&gt;The formal definition of a norm (&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Norm_(mathematics)"&gt;from Wikipedia&lt;/a&gt;)
talks about generalized vector spaces. Since complex numbers are also a vector
space (of dimension 1), we can simplify the definition just for
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/706e4005d6760527e1bd99f36d2e91c4f60ad91f.svg" style="height: 12px;" type="image/svg+xml"&gt;\mathbb{C}&lt;/object&gt; as follows:&lt;/p&gt;
&lt;p&gt;A norm on &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/706e4005d6760527e1bd99f36d2e91c4f60ad91f.svg" style="height: 12px;" type="image/svg+xml"&gt;\mathbb{C}&lt;/object&gt; is a real-valued function &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/633a6c4d4c130b45fb1b8ef67cd28851444288bf.svg" style="height: 16px;" type="image/svg+xml"&gt;p:\mathbb{C}\rightarrow\mathbb{R}&lt;/object&gt;
with the following properties:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Triangle inequality: &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/209e13884ab8dafefe1963d183fdda62ba1864fa.svg" style="height: 19px;" type="image/svg+xml"&gt;p(z+w)\leq p(z)+p(w)&lt;/object&gt; for all &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/3eb34c306327bdc359651378e76a616fb3b0f8cb.svg" style="height: 16px;" type="image/svg+xml"&gt;z,w\in \mathbb{C}&lt;/object&gt;&lt;/li&gt;
&lt;li&gt;Absolute homogeneity: &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/bac398d257afc0a11114fbcd2cf4f79ad9aa2b57.svg" style="height: 19px;" type="image/svg+xml"&gt;p(sz)=|s|p(z)&lt;/object&gt; for all &lt;object class="valign-m1" data="https://eli.thegreenplace.net/images/math/b1bc0cc92c24cfcad20c59b6d677cb462cdfe104.svg" style="height: 13px;" type="image/svg+xml"&gt;z\in \mathbb{C}&lt;/object&gt; and
all &lt;object class="valign-m1" data="https://eli.thegreenplace.net/images/math/3062e83c8f48712f2e4e6d14b50bf3c6a2695b2c.svg" style="height: 13px;" type="image/svg+xml"&gt;s\in \mathbb{R}&lt;/object&gt;&lt;/li&gt;
&lt;li&gt;Positive definiteness: for all &lt;object class="valign-m1" data="https://eli.thegreenplace.net/images/math/b1bc0cc92c24cfcad20c59b6d677cb462cdfe104.svg" style="height: 13px;" type="image/svg+xml"&gt;z\in \mathbb{C}&lt;/object&gt; if &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/fcd0d0402f66de0afbe427cfb03a753e7acce3ec.svg" style="height: 19px;" type="image/svg+xml"&gt;p(z)=0&lt;/object&gt;
then &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/f44367dd295fad0767e7cdf7174fec85da66458c.svg" style="height: 12px;" type="image/svg+xml"&gt;z=0&lt;/object&gt;.&lt;/li&gt;
&lt;li&gt;Non-negativity: &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/749c1daf2a7faa84adf6091934867be7a53d5cc3.svg" style="height: 19px;" type="image/svg+xml"&gt;p(z)\geq 0&lt;/object&gt; for all &lt;object class="valign-m1" data="https://eli.thegreenplace.net/images/math/b1bc0cc92c24cfcad20c59b6d677cb462cdfe104.svg" style="height: 13px;" type="image/svg+xml"&gt;z\in \mathbb{C}&lt;/object&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a complex number &lt;em&gt;z&lt;/em&gt;, setting &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4d1a7f50f65484a5c2db197377fffed5044490a7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(z)=|z|&lt;/object&gt; makes all these properties
work out intuitively:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Graphically, complex numbers add like vectors; therefore, given &lt;em&gt;z&lt;/em&gt; and &lt;em&gt;w&lt;/em&gt;,
the length &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6fb540afab729a6a220cbf521596adbce4e36e30.svg" style="height: 19px;" type="image/svg+xml"&gt;|z+w|&lt;/object&gt; is always &lt;em&gt;at most&lt;/em&gt; the combined lengths
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a88991833b27e12c9974180b9db252ee7c7d225d.svg" style="height: 19px;" type="image/svg+xml"&gt;|z|+|w|&lt;/object&gt;.&lt;/li&gt;
&lt;li&gt;When we scale &lt;em&gt;z&lt;/em&gt; by a scalar &lt;em&gt;s&lt;/em&gt;, its length is scaled similarly. Think
about the polar representation of a complex number: &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/0d8a226a76f59bb6b9b4b23b11fbb18118336c40.svg" style="height: 15px;" type="image/svg+xml"&gt;z=re^{i\theta}&lt;/object&gt;.
Multiplying that by &lt;em&gt;s&lt;/em&gt;, we get &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d8b19ba3f76e67da5a34bc2e85deea9a9eda4905.svg" style="height: 15px;" type="image/svg+xml"&gt;sz=sre^{i\theta}&lt;/object&gt;, which has length
&lt;em&gt;sr&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;In the polar representation: if &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/c6087094ab24bf158bda02a759a6f464d1878133.svg" style="height: 19px;" type="image/svg+xml"&gt;|z|=0&lt;/object&gt;, then &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/fa05edcebc0b56a6ac848cede267b8ff43b01a41.svg" style="height: 12px;" type="image/svg+xml"&gt;r=0&lt;/object&gt;,
meaning that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/f44367dd295fad0767e7cdf7174fec85da66458c.svg" style="height: 12px;" type="image/svg+xml"&gt;z=0&lt;/object&gt;.&lt;/li&gt;
&lt;li&gt;By definition, a length is non-negative; it's the distance from the origin.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="why-z-squared-is-not-a-norm-square"&gt;
&lt;h2&gt;Why &lt;em&gt;z&lt;/em&gt; squared is not a norm-square&lt;/h2&gt;
&lt;p&gt;Now it's time to go back to the question we started the post with. Why isn't
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d7dacae2c968388960bf8970080a980ed5c5dcb7.svg" style="height: 8px;" type="image/svg+xml"&gt;zz&lt;/object&gt; (or &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/0409c5da5b5540f8df097997a3201c82e674414e.svg" style="height: 15px;" type="image/svg+xml"&gt;z^2&lt;/object&gt;) the norm-square?&lt;/p&gt;
&lt;p&gt;For a general &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/a8630c2c602beeac9c7d604f7a5c4c72b6235adf.svg" style="height: 16px;" type="image/svg+xml"&gt;z=x+iy&lt;/object&gt;, we can calculate &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d7dacae2c968388960bf8970080a980ed5c5dcb7.svg" style="height: 8px;" type="image/svg+xml"&gt;zz&lt;/object&gt; as follows:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/24bdce64d3c2d2516903588c116cb27f42269503.svg" style="height: 75px;" type="image/svg+xml"&gt;\[\begin{align*}
  zz&amp;amp;=(x+iy)(x+iy)\\
    &amp;amp;=x^2+iyx+ixy-y^2\\
    &amp;amp;=x^2-y^2+2ixy
\end{align*}\]&lt;/object&gt;
&lt;p&gt;This is clearly a complex number, with a real and an imaginary component. It
doesn't seem to fit the requirement of being a &amp;quot;length&amp;quot; or distance from the
origin, which we'd expect to be a real quantity.&lt;/p&gt;
&lt;p&gt;Multiplying &lt;em&gt;z&lt;/em&gt; by itself in polar coordinates can be insightful:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2a9ebafa3b0b745bb3d84494376be884791a5747.svg" style="height: 75px;" type="image/svg+xml"&gt;\[\begin{align*}
  zz&amp;amp;=re^{i\theta}re^{i\theta}\\
  &amp;amp;=rre^{i\theta+i\theta}\\
  &amp;amp;=r^2 e^{i\cdot 2\theta}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;It seems like we &lt;em&gt;almost&lt;/em&gt; get what we want, because the magnitude of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d7dacae2c968388960bf8970080a980ed5c5dcb7.svg" style="height: 8px;" type="image/svg+xml"&gt;zz&lt;/object&gt;
is &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/23b008c61c26b6b07c237ebcd14cc1bd5590d5b7.svg" style="height: 15px;" type="image/svg+xml"&gt;r^2&lt;/object&gt;, which seems like the right magnitude for the square of the
distance from origin; but there's still an angle... Here's how it looks
geometrically:&lt;/p&gt;
&lt;img alt="Polar representation of zz" class="align-center" src="https://eli.thegreenplace.net/images/2024/complex-squared.png" /&gt;
&lt;p&gt;Recall that when multiplying two complex numbers - their magnitudes multiply,
but their angles add. In this case, we got the &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/23b008c61c26b6b07c237ebcd14cc1bd5590d5b7.svg" style="height: 15px;" type="image/svg+xml"&gt;r^2&lt;/object&gt; we needed, but with
an angle of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/bcafa268aec732fa30f1ca94c5a71c2c51a835c2.svg" style="height: 12px;" type="image/svg+xml"&gt;2\theta&lt;/object&gt;, which is not what we need. If only we could get
rid of the angle somehow... keep this thought in your head for the next
section.&lt;/p&gt;
&lt;p&gt;Looking at the formal definition of the norm, it's clear right away that
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d7dacae2c968388960bf8970080a980ed5c5dcb7.svg" style="height: 8px;" type="image/svg+xml"&gt;zz&lt;/object&gt; won't do. The norm is defined as a &lt;em&gt;real-valued&lt;/em&gt; function, whereas
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d7dacae2c968388960bf8970080a980ed5c5dcb7.svg" style="height: 8px;" type="image/svg+xml"&gt;zz&lt;/object&gt; is not real-valued.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="zz-is-a-norm-square"&gt;
&lt;h2&gt;&lt;em&gt;zz*&lt;/em&gt; is a norm-square&lt;/h2&gt;
&lt;p&gt;The conjugate &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/3c28f425f15b96a78602a12bc367a48e1708c344.svg" style="height: 12px;" type="image/svg+xml"&gt;z^*&lt;/object&gt; of &lt;em&gt;z&lt;/em&gt; is defined as:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f97bf5145b4f109dd873f026dd19b6261bee8e7a.svg" style="height: 18px;" type="image/svg+xml"&gt;\[z^*=x-iy\]&lt;/object&gt;
&lt;p&gt;Or in polar form:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/cb428cca8c7c2334e82204dcc6ec8500cc6abd3e.svg" style="height: 17px;" type="image/svg+xml"&gt;\[z^*=re^{-i\theta}\]&lt;/object&gt;
&lt;p&gt;Recall how we had that pesky angle remaining when calculating &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d7dacae2c968388960bf8970080a980ed5c5dcb7.svg" style="height: 8px;" type="image/svg+xml"&gt;zz&lt;/object&gt;?
Let's find a way to get rid of it; since angles add when we multiply complex
numbers, to get rid of &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt; we should multiply &lt;em&gt;z&lt;/em&gt; by something with
an angle of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1d89455d52a663f9bd876993dab3244eca9825bf.svg" style="height: 12px;" type="image/svg+xml"&gt;-\theta&lt;/object&gt;. Do you see where this is going?&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/38969c078a5e7218ff67a0757edd77f69ef33926.svg" style="height: 18px;" type="image/svg+xml"&gt;\[zz^*=re^{i\theta}re^{-i\theta}=r^2e^{i(\theta-\theta)}=r^2\]&lt;/object&gt;
&lt;p&gt;Voila! Multiplying &lt;em&gt;z&lt;/em&gt; by its complex conjugate gives us a real number. Moreover,
it gives us exactly the number we want - &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/23b008c61c26b6b07c237ebcd14cc1bd5590d5b7.svg" style="height: 15px;" type="image/svg+xml"&gt;r^2&lt;/object&gt;. This is the norm-square,
or &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/360ebdabd39e1822650a9e06af52bafa42c612d1.svg" style="height: 20px;" type="image/svg+xml"&gt;|z|^2&lt;/object&gt;. The norm of &lt;em&gt;z&lt;/em&gt; is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/0d4ebef37595cd27057748de36eab66c6d8e1e04.svg" style="height: 19px;" type="image/svg+xml"&gt;|z|&lt;/object&gt; and is precisely &lt;em&gt;r&lt;/em&gt;, the
&amp;quot;length&amp;quot; of the complex number.&lt;/p&gt;
&lt;p&gt;Let's verify this works in cartesian coordinates:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e9ad74b17f3c7c150032032ba187665192af9c3c.svg" style="height: 75px;" type="image/svg+xml"&gt;\[\begin{align*}
  zz^*&amp;amp;=(x+iy)(x-iy)\\
    &amp;amp;=x^2+iyx-ixy+y^2\\
    &amp;amp;=x^2+y^2
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Indeed, this makes intuitive sense because:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1fc08c47d9bde9c56ce120223f02d6e4e4aa119c.svg" style="height: 23px;" type="image/svg+xml"&gt;\[|z|=\sqrt{zz*}=\sqrt{x^2+y^2}\]&lt;/object&gt;
&lt;p&gt;And this is exactly what we expect when calculating the length of &lt;em&gt;z&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The norm square of a complex number &lt;em&gt;z&lt;/em&gt; is denoted &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/360ebdabd39e1822650a9e06af52bafa42c612d1.svg" style="height: 20px;" type="image/svg+xml"&gt;|z|^2&lt;/object&gt;. In this post
we've seen why&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/be43c9bbc0688fbd5bd2a5ccbbcbfc490e8157ec.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|z|^2\neq z^2\]&lt;/object&gt;
&lt;p&gt;(even though the mathematical notation makes it seem like this should be true).
Instead, this is how the norm-square is actually calculated:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/93a6ea8b04b9e10b4ac45e52aee614cda725e38f.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|z|^2=zz^*\]&lt;/object&gt;
&lt;p&gt;With the norm itself being&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/17853097cca6756851e0ae03c50c118bbc0ba3d9.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|z|=\sqrt{zz^*}\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="appendix-1-zz-and-the-formal-definition-of-norm"&gt;
&lt;h2&gt;Appendix 1: &lt;em&gt;zz*&lt;/em&gt; and the formal definition of norm&lt;/h2&gt;
&lt;p&gt;Let's get back to the formal definition of norm and show that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4e7ff76404faee9370867212b4d0db77fbef2153.svg" style="height: 9px;" type="image/svg+xml"&gt;zz*&lt;/object&gt;
satisfies it.&lt;/p&gt;
&lt;p&gt;First of all, we've already seen that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4e7ff76404faee9370867212b4d0db77fbef2153.svg" style="height: 9px;" type="image/svg+xml"&gt;zz*&lt;/object&gt; is a &lt;em&gt;real&lt;/em&gt; number, no matter
what &lt;em&gt;z&lt;/em&gt; is. Therefore, it can indeed serve as a real-valued function
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/633a6c4d4c130b45fb1b8ef67cd28851444288bf.svg" style="height: 16px;" type="image/svg+xml"&gt;p:\mathbb{C}\rightarrow\mathbb{R}&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;The proof of property 1 is a bit tedious,
&lt;a class="reference external" href="https://proofwiki.org/wiki/Triangle_Inequality/Complex_Numbers"&gt;but straightforward using the Cauchy-Schwartz inequality&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For property 2, since &lt;em&gt;s&lt;/em&gt; is a real number, the square root of its square is
just its absolute value:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/cda157a5549a049536ef4b3a9f9ae2405702a85c.svg" style="height: 23px;" type="image/svg+xml"&gt;\[|sz|=\sqrt{(sx)^2+(sy)^2}=\sqrt{s^2(x^2+y^2)}=|s|\sqrt{x^2+y^2}=|s||z|\]&lt;/object&gt;
&lt;p&gt;For property 3, consider a &lt;em&gt;z&lt;/em&gt; such that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8f9c2e89345d32212e440f1784f96b4b38537cc8.svg" style="height: 23px;" type="image/svg+xml"&gt;\[|z|=\sqrt{x^2+y^2}=0\]&lt;/object&gt;
&lt;p&gt;Since neither addend inside the square root can be negative,
clearly both &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; must be zero, meaning that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/f44367dd295fad0767e7cdf7174fec85da66458c.svg" style="height: 12px;" type="image/svg+xml"&gt;z=0&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Property 4 is similarly straightforward: given real components &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;,
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/20fbc79261e6506fa206545e608a128dfe7f96fb.svg" style="height: 22px;" type="image/svg+xml"&gt;|z|=\sqrt{x^2+y^2}&lt;/object&gt; is non-negative.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="appendix-2-norms-of-expressions"&gt;
&lt;h2&gt;Appendix 2: norms of expressions&lt;/h2&gt;
&lt;p&gt;Say we want to calculate the norm squared of some expression involving complex
numbers; the simplest example would be a sum of &lt;em&gt;z&lt;/em&gt; and &lt;em&gt;w&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ee8259923714f60717a9d19affab1c2468c15961.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|z+w|^2\]&lt;/object&gt;
&lt;p&gt;We go about it pretty much the same way:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3b8d01185756bc07e6d5e7a85ea5eca1e0e49d81.svg" style="height: 103px;" type="image/svg+xml"&gt;\[\begin{align*}
|z+w|^2&amp;amp;=(z+w)(z+w)^*\\
&amp;amp;=(z+w)(z^*+w^*)\\
&amp;amp;=zz^*+zw^*+wz^*+ww^*\\
&amp;amp;=|z|^2+|w|^2+wz^*+zw^*
\end{align*}\]&lt;/object&gt;
&lt;p&gt;If we want to push a little further, the imaginary components of
&lt;object class="valign-m2" data="https://eli.thegreenplace.net/images/math/39c0a484bf8140d0769ee249ddb031251bd9c65d.svg" style="height: 14px;" type="image/svg+xml"&gt;wz^*+zw^*&lt;/object&gt; cancel out, while their real components are duplicates;
so we end up with:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0458b649a4c34e3aa1c88de2d04fc06c67c810f1.svg" style="height: 22px;" type="image/svg+xml"&gt;\[|z+w|^2=|z|^2+|w|^2+2Re(zw^*)\]&lt;/object&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Notes on the Euler formula</title><link href="https://eli.thegreenplace.net/2024/notes-on-the-euler-formula/" rel="alternate"></link><published>2024-09-07T05:44:00-07:00</published><updated>2024-09-08T13:47:45-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-09-07:/2024/notes-on-the-euler-formula/</id><summary type="html">&lt;p&gt;The Euler formula states that for any real &lt;em&gt;x&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e41e42437950b87591a40b19d5bd68d01bd7786b.svg" style="height: 22px;" type="image/svg+xml"&gt;\[e^{ix}=cos(x)+i sin(x)\]&lt;/object&gt;
&lt;p&gt;Where &lt;em&gt;i&lt;/em&gt; is the imaginary unit. This formula is extremely important in many
branches of mathematics and engineering, but at first glance it's puzzling. What
does a complex exponent even mean, and how can â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;The Euler formula states that for any real &lt;em&gt;x&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e41e42437950b87591a40b19d5bd68d01bd7786b.svg" style="height: 22px;" type="image/svg+xml"&gt;\[e^{ix}=cos(x)+i sin(x)\]&lt;/object&gt;
&lt;p&gt;Where &lt;em&gt;i&lt;/em&gt; is the imaginary unit. This formula is extremely important in many
branches of mathematics and engineering, but at first glance it's puzzling. What
does a complex exponent even mean, and how can it be related to the
trigonometric functions?&lt;/p&gt;
&lt;div class="section" id="complex-number-representations"&gt;
&lt;h2&gt;Complex number representations&lt;/h2&gt;
&lt;p&gt;Complex numbers have two canonical representations:&lt;/p&gt;
&lt;img alt="Cartesian and polar representation of a complex number" class="align-center" src="https://eli.thegreenplace.net/images/2024/complex-cartesian-polar.png" /&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Cartesian: &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/dc5648d309ad9fa02d53124cd7d954697f66d39c.svg" style="height: 16px;" type="image/svg+xml"&gt;x+iy&lt;/object&gt;&lt;/li&gt;
&lt;li&gt;Polar: &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/498a8152700d478a53607b91f0940c6b8fa0a1fb.svg" style="height: 12px;" type="image/svg+xml"&gt;r\angle \theta&lt;/object&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Trigonometric formulae can be used to convert between the two in a
straightforward way:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Given &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/dc5648d309ad9fa02d53124cd7d954697f66d39c.svg" style="height: 16px;" type="image/svg+xml"&gt;x+iy&lt;/object&gt;, we can compute &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/2728488e970286c68c04be64d662d549e241ba5c.svg" style="height: 22px;" type="image/svg+xml"&gt;r=\sqrt{x^2+y^2}&lt;/object&gt; and
&lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/4db3dc38b58aa628d82fadf84b3d6eb76572fcd5.svg" style="height: 21px;" type="image/svg+xml"&gt;\theta=tan^{-1}(\frac{y}{x})&lt;/object&gt;&lt;/li&gt;
&lt;li&gt;Given &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/498a8152700d478a53607b91f0940c6b8fa0a1fb.svg" style="height: 12px;" type="image/svg+xml"&gt;r\angle \theta&lt;/object&gt; we can compute &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1315df0eea0f941a4323e489788223ea5ac54f15.svg" style="height: 12px;" type="image/svg+xml"&gt;x=r cos\theta&lt;/object&gt;
and &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/16bc635450100d6285d3cedee1bac9513e31a7fe.svg" style="height: 16px;" type="image/svg+xml"&gt;y=r sin\theta&lt;/object&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="some-intuition-for-euler-s-formula"&gt;
&lt;h2&gt;Some intuition for Euler's formula&lt;/h2&gt;
&lt;p&gt;Representing &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; as above, we have:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9536f1bf02955baf02fab954b441b1efcd06768c.svg" style="height: 19px;" type="image/svg+xml"&gt;\[z=r(cos\theta+i sin\theta)\]&lt;/object&gt;
&lt;p&gt;Now, let's take two complex numbers, multiply them together and use some
basic trigonometric identities:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/671dbe789a44e7c82d528021ee225cd94095852e.svg" style="height: 153px;" type="image/svg+xml"&gt;\[\begin{align*}
  z_1&amp;amp;=r_1 (cos\theta+i sin\theta) \\
  z_2&amp;amp;=r_2 (cos\phi+i sin\phi)\\
  z_1 z_2 &amp;amp;= r_1 r_2 (cos\theta+i sin\theta)(cos\phi+i sin\phi)\\
  &amp;amp;=r_1 r_2 ((cos\theta cos\phi -sin\theta sin\phi) +i(cos\theta sin\phi + cos\phi sin\theta))\\
  &amp;amp;=r_1 r_2 (cos(\theta+\phi) + i sin(\theta+\phi))\\
  &amp;amp;= r_1 r_2 \angle (\theta+\phi)
\end{align*}\]&lt;/object&gt;
&lt;p&gt;When multiplying two complex numbers in polar form, their magnitudes multiply
but their angles &lt;em&gt;add&lt;/em&gt; &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, suppose we have a hypothetical function &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/cdbd3b5daa63e81b18753fcadf7ce6842362d7ce.svg" style="height: 19px;" type="image/svg+xml"&gt;f(\theta)&lt;/object&gt; such that
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7e89ab16a335a7db4b5f9665ad6bf3f5507ff6b2.svg" style="height: 19px;" type="image/svg+xml"&gt;r f(\theta)&lt;/object&gt; represents a complex number. We've just shown that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ebabe28755489ef975d05f0b8eca2b7fa9bdba58.svg" style="height: 19px;" type="image/svg+xml"&gt;\[(r_1 f(\theta))\cdot(r_2 f(\phi))=r_1 r_2 f(\theta+\phi)\]&lt;/object&gt;
&lt;p&gt;What real-life function do you know that behaves like this? An exponential
function! &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/9094dd6718488a8c2fd484f3a163473682f5c51d.svg" style="height: 14px;" type="image/svg+xml"&gt;a^x a^y=a^{x+y}&lt;/object&gt; for some &lt;em&gt;a&lt;/em&gt;. This isn't a proof of anything,
of course, and the base of the exponent can be arbitrary - but it does show
that there's something about complex numbers that behaves like exponentials.&lt;/p&gt;
&lt;p&gt;Let's look at it from another direction. Once again starting with:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9536f1bf02955baf02fab954b441b1efcd06768c.svg" style="height: 19px;" type="image/svg+xml"&gt;\[z=r(cos\theta+i sin\theta)\]&lt;/object&gt;
&lt;p&gt;We'll treat &lt;em&gt;z&lt;/em&gt; as a function of &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt;, and find its derivative:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f0c166c16c8953fb3d7413c38a98c3a7fe2d611a.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{dz}{d\theta}=-rsin\theta +i cos\theta\]&lt;/object&gt;
&lt;p&gt;If we factor &lt;em&gt;i&lt;/em&gt; out of the parenthesis, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3aad161fa2b1aabee4ca8e8b62a98b4896c25ab0.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{dz}{d\theta}=ri(cos\theta+i sin\theta)=iz\]&lt;/object&gt;
&lt;p&gt;Note that this is exactly the derivative if &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/3da5144c6fa0c57644b8f22d37ae190c1163a2f0.svg" style="height: 20px;" type="image/svg+xml"&gt;z(\theta)=e^{i\theta}&lt;/object&gt;,
another clue that complex numbers behave like exponential functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="proof-using-power-series"&gt;
&lt;h2&gt;Proof using power series&lt;/h2&gt;
&lt;p&gt;The canonical proof of Euler's formula uses &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/notes-on-taylor-and-maclaurin-series/"&gt;Maclaurin series expansions&lt;/a&gt;
for &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt;, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/1f2ea8ffab8deb0e5b854a260a68b42b7eb7b048.svg" style="height: 19px;" type="image/svg+xml"&gt;sin(x)&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;As a reminder, the Maclaurin series approximation for a function &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/766aa8a11f2b92dd363c0dab88fff5eb333165bd.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p(x) = f(0)+\frac{f&amp;#x27;(0)}{1!}x+\frac{f&amp;#x27;&amp;#x27;(0)}{2!}x^2+\frac{f&amp;#x27;&amp;#x27;&amp;#x27;(0)}{3!}x^3+\cdots=\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!}x^n\]&lt;/object&gt;
&lt;p&gt;For &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1624dce91de495347430ec2518baf6c6a5328d2e.svg" style="height: 12px;" type="image/svg+xml"&gt;e^x&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9484e0e6f1dad5e262b0637ea89651e71fce7df8.svg" style="height: 39px;" type="image/svg+xml"&gt;\[e^{x}=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\frac{x^5}{5!}+\cdots\]&lt;/object&gt;
&lt;p&gt;Substituting &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d4f506d11f53beffb8d67eda0740af3a887a992e.svg" style="height: 12px;" type="image/svg+xml"&gt;ix&lt;/object&gt; for &lt;em&gt;x&lt;/em&gt; and applying powers of &lt;em&gt;i&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/db09e62c7bdd82da448598d005baee788f2a3841.svg" style="height: 85px;" type="image/svg+xml"&gt;\[\begin{align*}
  e^{ix}&amp;amp;=1+ix+\frac{(ix)^2}{2!}+\frac{(ix)^3}{3!}+\frac{(ix)^4}{4!}+\frac{(ix)^5}{5!}+\cdots\\
        &amp;amp;=1+ix-\frac{x^2}{2!}-\frac{ix^3}{3!}+\frac{x^4}{4!}+\frac{ix^5}{5!}+\cdots
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Now let's regroup the real and imaginary parts of the series:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e336528c61ea33ff9928fef362e358ea0d3e52ce.svg" style="height: 44px;" type="image/svg+xml"&gt;\[e^{ix}=\left(1-\frac{x^2}{2!}+\frac{x^4}{4!}+\cdots\right)
+i\left(x-\frac{x^3}{3!}+\frac{x^5}{5!}+\cdots\right)\]&lt;/object&gt;
&lt;p&gt;The contents of the first parenthesis is precisely the Maclaurin series expansion
of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt;, and the contents of the second parenthesis is the expansion
of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/1f2ea8ffab8deb0e5b854a260a68b42b7eb7b048.svg" style="height: 19px;" type="image/svg+xml"&gt;sin(x)&lt;/object&gt;; therefore, we've just proven the Euler formula &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="proof-using-derivatives"&gt;
&lt;h2&gt;Proof using derivatives&lt;/h2&gt;
&lt;p&gt;Let's define &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/10196ffe9eae4b142f48f3a6a30852c5e39f8630.svg" style="height: 19px;" type="image/svg+xml"&gt;\xi(x)&lt;/object&gt; as follows:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/5b22bb50bf5f6ceec5d6f3dcd41e1b0a4ad237eb.svg" style="height: 38px;" type="image/svg+xml"&gt;\[\xi(x)=\frac{cos(x)+isin(x)}{e^{ix}}=e^{-ix}(cos(x)+isin(x))\]&lt;/object&gt;
&lt;p&gt;And compute its derivative:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/116f265e64f726d0eccc32d1c28c3d996eb76304.svg" style="height: 66px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{d\xi(x)}{dx}&amp;amp;=e^{-ix}(-sin(x)+icos(x))-ie^{-ix}(cos(x)+isin(x))\\
  &amp;amp;=e^{-ix}(-sin(x)+icos(x))-e^{-ix}(icos(x)-sin(x))=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Thus, it's a constant function; what is its value? We can find &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ce91859cf82a14e92ab96aaed0abbd7152f9f5fb.svg" style="height: 19px;" type="image/svg+xml"&gt;\xi(0)&lt;/object&gt;
easily - it's 1. Therefore, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7e2b6fb6c68ae16b535063e3a227c94746080406.svg" style="height: 19px;" type="image/svg+xml"&gt;\xi(x)=1&lt;/object&gt; everywhere and thus its numerator
and denominator are always equal &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="visualizing-euler-s-formula"&gt;
&lt;h2&gt;Visualizing Euler's formula&lt;/h2&gt;
&lt;p&gt;It's interesting to plot &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/835c7c5ff57a0abe89062ebc5e94c95acef69e33.svg" style="height: 15px;" type="image/svg+xml"&gt;e^{i\phi}&lt;/object&gt; to observe its behavior. In the general
case, it's very difficult to visualize functions in the complex domain because
both the input and output are two-dimensional; so we'd need a 4D plot. Luckily
for us, we're usually interested in &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/835c7c5ff57a0abe89062ebc5e94c95acef69e33.svg" style="height: 15px;" type="image/svg+xml"&gt;e^{i\phi}&lt;/object&gt; only for
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/8896a3aed7cce98a05251da9e29f829fc0a60dae.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi\in\mathbb{R}&lt;/object&gt;, so we have 3 dimensions to deal with:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Input dimension: &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt;&lt;/li&gt;
&lt;li&gt;Output dimensions: &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/3e7d36763f50c2d5acde9f87bd8478fbe2c79bbe.svg" style="height: 20px;" type="image/svg+xml"&gt;Re(e^{i\phi})&lt;/object&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/0ea36534dd6402c443e4b003f8218887a0422189.svg" style="height: 20px;" type="image/svg+xml"&gt;Im(e^{i\phi})&lt;/object&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;img alt="3D Plot of e^{i phi}" class="align-center" src="https://eli.thegreenplace.net/images/2024/eix-spiral.png" /&gt;
&lt;p&gt;If we isolate the two 2D plots of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4560241354db0347c87b3a51129d06d902a90ed5.svg" style="height: 20px;" type="image/svg+xml"&gt;f(\phi)=Re(e^{i\phi})&lt;/object&gt; as a function of
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6893b686e59874da28d1829cacac3a51f155e4b6.svg" style="height: 20px;" type="image/svg+xml"&gt;g(\phi)=Im(e^{i\phi})&lt;/object&gt;, we get:&lt;/p&gt;
&lt;img alt="Two 2D plots separately of Re and Im" class="align-center" src="https://eli.thegreenplace.net/images/2024/phi-re-im.png" /&gt;
&lt;p&gt;This is the expected result from Euler's formula! The real part of the complex
exponent is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e16afa9144b2205a80c1bde56036c8cf096fdf75.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(\phi)&lt;/object&gt;, while the imaginary part is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/9cac796934ea0be2805c5f4105ff7dae42e65421.svg" style="height: 19px;" type="image/svg+xml"&gt;sin(\phi)&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Finally, let's plot the projection of the 3D plot onto the real+imaginary axes:&lt;/p&gt;
&lt;img alt="Plotting e^{i phi} as projection on the re+im axes, unit circle" class="align-center" src="https://eli.thegreenplace.net/images/2024/unit-circle.png" /&gt;
&lt;p&gt;It hopefully comes as no surprise that we get the unit circle! This is another
way to demonstrate the beautiful connection between the trigonometric functions
and circles. If you imagine a point moving along the unit circle
counter-clockwise, this point's &lt;em&gt;Re&lt;/em&gt; value will be &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e16afa9144b2205a80c1bde56036c8cf096fdf75.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(\phi)&lt;/object&gt; where
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/411e715f9ab9075b0a30b4117d209921f0bc2389.svg" style="height: 16px;" type="image/svg+xml"&gt;\phi&lt;/object&gt; is its angle from the &lt;em&gt;Re&lt;/em&gt; axis, and its &lt;em&gt;Im&lt;/em&gt; value will be
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/9cac796934ea0be2805c5f4105ff7dae42e65421.svg" style="height: 19px;" type="image/svg+xml"&gt;sin(\phi)&lt;/object&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="euler-s-identity"&gt;
&lt;h2&gt;Euler's identity&lt;/h2&gt;
&lt;p&gt;Euler's famous identity ties the &amp;quot;five fundamental constants of mathematics&amp;quot;
together:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/26cc6b1c790a7db2144e4f5948158479292c55e6.svg" style="height: 19px;" type="image/svg+xml"&gt;\[e^{i\pi}+1=0\]&lt;/object&gt;
&lt;p&gt;This identity is trivial to derive from the Euler formula, because:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/168aa576bd1a872a6fc10c796b647144237931ce.svg" style="height: 22px;" type="image/svg+xml"&gt;\[e^{i\pi}=cos(\pi)+i sin(\pi)=-1\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="de-moivre-formula"&gt;
&lt;h2&gt;De Moivre formula&lt;/h2&gt;
&lt;p&gt;Let's take the complex exponent &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/0e85a3c21a84f33aef16ee284c5a954b3c90c2ef.svg" style="height: 15px;" type="image/svg+xml"&gt;e^{ix}&lt;/object&gt; and raise it to the n-th power,
where &lt;em&gt;n&lt;/em&gt; is an integer:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e159517d90bc55d4ca93f3e149ec573a0d609c76.svg" style="height: 22px;" type="image/svg+xml"&gt;\[(e^{ix})^n=e^{inx}\]&lt;/object&gt;
&lt;p&gt;We can replace the complex exponent by its trigonometric equivalent using
Euler's formula on both sides:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/96194ade4d0676d1b88fb7a030ebb4204615a1bb.svg" style="height: 19px;" type="image/svg+xml"&gt;\[(cos(x) +isin(x))^n=cos(nx)+isin(nx)\]&lt;/object&gt;
&lt;p&gt;This is &lt;a class="reference external" href="https://en.wikipedia.org/wiki/De_Moivre%27s_formula"&gt;De Moivre's Formula&lt;/a&gt;,
which is extremely useful in calculations involving complex numbers, and is a
treasure trove of trigonometric identities.&lt;/p&gt;
&lt;p&gt;An alternative formulation of the De Moivre formula uses fractional powers
and is useful for finding the roots of complex numbers. However, we have to
be careful here, because the complex root function (just like its real
counterpart!) is &lt;em&gt;multi-valued&lt;/em&gt;; it maps a single value in its domain to
potentially multiple values in its range &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This formulation says:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/fd11e3c26f7da5cc3427a955581d301125e9454b.svg" style="height: 37px;" type="image/svg+xml"&gt;\[e^{\frac{ix}{n}}=(cos(x) +isin(x))^{\frac{1}{n}}=cos(\frac{x+2\pi k}{n})+isin(\frac{x+2\pi k}{n})\]&lt;/object&gt;
&lt;p&gt;For integer &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/ad950d9a503bb8985bf9b60d02f867703eee04a6.svg" style="height: 15px;" type="image/svg+xml"&gt;$$0\leq k&amp;lt; n$$&lt;/object&gt;. This is because if we raise this number back
to the power of &lt;em&gt;n&lt;/em&gt;, we'll get back the original &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/0e85a3c21a84f33aef16ee284c5a954b3c90c2ef.svg" style="height: 15px;" type="image/svg+xml"&gt;e^{ix}&lt;/object&gt; for any of
these &lt;em&gt;k&lt;/em&gt; (both sine and cosine are periodic with a period of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/0833718ca4569f36e84dbdc7742eaec65e49b150.svg" style="height: 12px;" type="image/svg+xml"&gt;2\pi&lt;/object&gt;).&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;In other words, multiplying by a complex number combines a
&lt;em&gt;scaling&lt;/em&gt; and &lt;em&gt;rotation&lt;/em&gt; operations. Multiplying any &lt;em&gt;z&lt;/em&gt; by
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/498a8152700d478a53607b91f0940c6b8fa0a1fb.svg" style="height: 12px;" type="image/svg+xml"&gt;r\angle \theta&lt;/object&gt; scales (multiplies) z's magnitude by
&lt;em&gt;r&lt;/em&gt; and rotates it (counter-clockwise) by angle &lt;img alt="\theta" class="valign-0" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I've seen places that treat this as
the &lt;em&gt;definition&lt;/em&gt; of what a complex exponential means, rather than
a proof.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;And thus isn't strictly a function at all, if we want to put our
abstract algebra hat on.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Notes on Taylor and Maclaurin series</title><link href="https://eli.thegreenplace.net/2024/notes-on-taylor-and-maclaurin-series/" rel="alternate"></link><published>2024-07-23T18:55:00-07:00</published><updated>2024-07-24T01:55:31-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-07-23:/2024/notes-on-taylor-and-maclaurin-series/</id><summary type="html">&lt;p&gt;A Maclaurin series is a power series - a polynomial with carefully selected
coefficients and an infinite number of terms - used to approximate arbitrary
functions with some conditions (e.g. differentiability). The Maclaurin series
does this for input values close to 0, and is a special case of the Taylor
series â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;A Maclaurin series is a power series - a polynomial with carefully selected
coefficients and an infinite number of terms - used to approximate arbitrary
functions with some conditions (e.g. differentiability). The Maclaurin series
does this for input values close to 0, and is a special case of the Taylor
series which can be used to find a polynomial approximation around any value.&lt;/p&gt;
&lt;div class="section" id="intuition"&gt;
&lt;h2&gt;Intuition&lt;/h2&gt;
&lt;p&gt;Let's say we have a function &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; and we want to approximate it with
some other - polynomial - function &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt;. To make sure that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt;
is as close as possible to &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt;, we'll create a function that has
similar derivatives to &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt;.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;We start with a constant polynomial, such that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/012caf9ca6f5f20d23916c2628ebef524cefeed7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(0)=f(0)&lt;/object&gt;. This
approximation is perfect at 0 itself, but not as much elsewhere.&lt;/li&gt;
&lt;li&gt;We want &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt; to behave similarly to &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; around 0, so we'll
set the derivative of our approximation to be the same as the derivative
of &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at 0; in other words &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/210f8d24ca46f8afef927a302d65117bea405af9.svg" style="height: 19px;" type="image/svg+xml"&gt;p&amp;#x27;(0)=f&amp;#x27;(0)&lt;/object&gt;. This approximation
will be decent &lt;em&gt;very&lt;/em&gt; close to 0 (at least in the direction of the slope),
but will become progressively worse as we get farther away from 0.&lt;/li&gt;
&lt;li&gt;We continue this process, by setting the second derivative to be
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/cf52e0957b8105e96f4a7d58c94212757571f4cf.svg" style="height: 19px;" type="image/svg+xml"&gt;p&amp;#x27;&amp;#x27;(0)=f&amp;#x27;&amp;#x27;(0)&lt;/object&gt;, the third derivative to be &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/54583c50b4c7db87eb9546ee7bf42dbfddac2777.svg" style="height: 19px;" type="image/svg+xml"&gt;p&amp;#x27;&amp;#x27;&amp;#x27;(0)=f&amp;#x27;&amp;#x27;&amp;#x27;(0)&lt;/object&gt;
and so on, for as many terms as we need to achieve a good approximation in
our desired range. Intuitively, if many derivatives of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt; are
identical to the corresponding derivatives of &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at some point,
the two functions will have very similar behaviors around that point &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The full Maclaurin series that accomplishes this approximation is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/766aa8a11f2b92dd363c0dab88fff5eb333165bd.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p(x) = f(0)+\frac{f&amp;#x27;(0)}{1!}x+\frac{f&amp;#x27;&amp;#x27;(0)}{2!}x^2+\frac{f&amp;#x27;&amp;#x27;&amp;#x27;(0)}{3!}x^3+\cdots=\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!}x^n\]&lt;/object&gt;
&lt;p&gt;We'll get to how this equation is found in a moment, but first an example that
demonstrates its approximation capabilities. Suppose we want to find a polynomial
approximation for &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a70fbea75540fef14e8ab2c910d8f9616c5e9f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(x)=cos(x)&lt;/object&gt;. Following the definition of the Maclaurin
series, it's easy to calculate:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/359e30713a5e6accabbfb43c37c2798cd451df02.svg" style="height: 39px;" type="image/svg+xml"&gt;\[p_{cos}(x)=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\frac{x^8}{8!}-\cdots\]&lt;/object&gt;
&lt;p&gt;(try it as an exercise).&lt;/p&gt;
&lt;img alt="Successive approximation of cos(x) with Maclaurin series" class="align-center" src="https://eli.thegreenplace.net/images/2024/maclaurin-cos.png" /&gt;
&lt;p&gt;The dark blue line is the cosine function &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a70fbea75540fef14e8ab2c910d8f9616c5e9f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(x)=cos(x)&lt;/object&gt;. The light blue
lines are successive approximations, with &lt;em&gt;k&lt;/em&gt; terms of the power series
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/c739a406062fd6fb971e3f322a30a0f603757e25.svg" style="height: 19px;" type="image/svg+xml"&gt;p_{cos}(x)&lt;/object&gt; included:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;With &lt;em&gt;k=1&lt;/em&gt;, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ec45f9d0f46db99de6b7864c09d9f22b462d11e6.svg" style="height: 19px;" type="image/svg+xml"&gt;p_{cos}(x)=1&lt;/object&gt; since that's just the value of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; at 0.&lt;/li&gt;
&lt;li&gt;With &lt;em&gt;k=2&lt;/em&gt;, &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/dee962f0d6f639c30a4c02d2063fa6679fff3120.svg" style="height: 25px;" type="image/svg+xml"&gt;p_{cos}(x)=1-\frac{x^2}{2}&lt;/object&gt;, and indeed the line looks parabolic&lt;/li&gt;
&lt;li&gt;With &lt;em&gt;k=3&lt;/em&gt; we get a 4th degree polynomial which tracks the function better,
and so on&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With more terms in the power series, the approximation resembles
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; more and more, at least close to 0. The farther away we get from
0, the more terms we need for a good approximation &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-the-maclaurin-series-works"&gt;
&lt;h2&gt;How the Maclaurin series works&lt;/h2&gt;
&lt;p&gt;This section shows how one arrives at the formula for the Maclaurin series,
and connects it to the intuition of equating derivatives.&lt;/p&gt;
&lt;p&gt;We'll start by observing that the Maclaurin series is developed around 0 for
a good reason. The generalized form of a power series is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/17143ce86a9aa99908cc5ef88840f3a46a6e6216.svg" style="height: 22px;" type="image/svg+xml"&gt;\[p(x)=a_0+a_1 x+a_2 x^2 + a_3 x^3 + a_4 x^4 + \cdots\]&lt;/object&gt;
&lt;p&gt;To properly approximate a function, we need this series to &lt;em&gt;converge&lt;/em&gt;; therefore,
it would be desirable for its terms to decrease. An &lt;em&gt;x&lt;/em&gt; value close to zero
guarantees that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/a297bb91c9703af1975402dded3ab9b7e6431dde.svg" style="height: 12px;" type="image/svg+xml"&gt;x^n&lt;/object&gt; becomes smaller and smaller with each successive
term. There's a whole section on convergence further down with more details.&lt;/p&gt;
&lt;p&gt;Recall from the &lt;em&gt;Intuition&lt;/em&gt; section that we're looking for a polynomial that
passes through the same point as &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at 0, and that has derivatives
equal to those of &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at that point.&lt;/p&gt;
&lt;p&gt;Let's calculate a few of the first derivatives of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f86e6c6bb632c1ca2518f269fc1cc1b6737d4f7.svg" style="height: 19px;" type="image/svg+xml"&gt;p(x)&lt;/object&gt;; the function
itself can be considered as the 0-th derivative:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/305bdd6918c39e9b7fa95191f323ee811e744907.svg" style="height: 124px;" type="image/svg+xml"&gt;\[\begin{align*}
 p(x)&amp;amp;=a_0+a_1 x+a_2 x^2 + a_3 x^3+ a_4 x^4+\cdots\\
 p&amp;#x27;(x)&amp;amp;= a_1 +2 a_2 x + 3 a_3 x^2+4 a_4 x^3+\cdots\\
 p&amp;#x27;&amp;#x27;(x)&amp;amp;= 2 a_2 + 3 \cdot 2 a_3 x+ 4 \cdot 3 x^2+\cdots\\
 p&amp;#x27;&amp;#x27;&amp;#x27;(x)&amp;amp;= 3\cdot 2 a_3 + 4\cdot 3 \cdot 2 x+\cdots \\
 \cdots
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Now, equate these to corresponding derivatives of &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; at &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/8bdf21367eee06097384c37b0448375f07f950f0.svg" style="height: 12px;" type="image/svg+xml"&gt;x=0&lt;/object&gt;.
All the non-constant terms drop out, and we're left with:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/73ca5017e4342b3dc099f9895f78b58cb7eb486d.svg" style="height: 175px;" type="image/svg+xml"&gt;\[\begin{align*}
 f(0)&amp;amp;=p(0)=a_0\\
 f&amp;#x27;(0)&amp;amp;=p&amp;#x27;(0)= a_1 \\
 f&amp;#x27;&amp;#x27;(0)&amp;amp;=p&amp;#x27;&amp;#x27;(0)= 2 a_2 \\
 f&amp;#x27;&amp;#x27;&amp;#x27;(0)&amp;amp;=p&amp;#x27;&amp;#x27;&amp;#x27;(0)= 3\cdot 2 a_3 \\
 \cdots\\
 f^{(n)}(0)&amp;amp;=p^{(0)}(0)=n!a_n\\
 \cdots\\
\end{align*}\]&lt;/object&gt;
&lt;p&gt;So we can set the coefficients of the power series, generalizing the
denominators using factorials:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/37d903463af52a2e233564a101a6ee25cf5dd85c.svg" style="height: 228px;" type="image/svg+xml"&gt;\[\begin{align*}
 a_0 &amp;amp;= f(0)\\
 a_1 &amp;amp;= \frac{f&amp;#x27;(0)}{1!}\\
 a_2 &amp;amp;= \frac{f&amp;#x27;&amp;#x27;(0)}{2!}\\
 a_3 &amp;amp;= \frac{f&amp;#x27;&amp;#x27;&amp;#x27;(0)}{3!}\\
 \cdots \\
 a_n &amp;amp;= \frac{f^{(n)}(0)}{n!}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Which gives us the definition of the Maclaurin series:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/766aa8a11f2b92dd363c0dab88fff5eb333165bd.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p(x) = f(0)+\frac{f&amp;#x27;(0)}{1!}x+\frac{f&amp;#x27;&amp;#x27;(0)}{2!}x^2+\frac{f&amp;#x27;&amp;#x27;&amp;#x27;(0)}{3!}x^3+\cdots=\sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!}x^n\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="taylor-series"&gt;
&lt;h2&gt;Taylor series&lt;/h2&gt;
&lt;p&gt;The Maclaurin series is suitable for finding approximations for functions
around 0; what if we want to approximate a function around a different value?
First, let's see why we would even want that. A couple of major reasons come
to mind:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;We have a non-cyclic function and we're really interested in approximating
it around some specific value of &lt;em&gt;x&lt;/em&gt;; if we use Maclaurin series, we
get a good approximation around 0, but its quality is diminishing the
farther away we get. We may be able to use much fewer terms for a good
approximation if we start it around our target value.&lt;/li&gt;
&lt;li&gt;The function we're approximating is not well behaved around 0.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It's the second reason which is most common, at least in calculus. By &amp;quot;not well
behaved&amp;quot; I mean a function that's not finite at 0 (or close to it), or that
isn't differentiable at that point, or whose derivatives aren't finite.&lt;/p&gt;
&lt;p&gt;There's a very simple and common example of such a function - the natural
logarithm &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/86d082e11ec9c0d5b2a4df154c6b4a0755b4b512.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(x)&lt;/object&gt;. This function is undefined at 0 (it approaches
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/18787d835dea1ca698e365c252f82b506cecfce7.svg" style="height: 8px;" type="image/svg+xml"&gt;-\infty&lt;/object&gt;). Moreover, its derivatives are:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1220eae393ca586b7f501bdd2f4887701cdcf9e9.svg" style="height: 223px;" type="image/svg+xml"&gt;\[\begin{align*}
 ln&amp;#x27;(x)&amp;amp;= \frac{1}{x}\\
 ln&amp;#x27;&amp;#x27;(x)&amp;amp;= -\frac{1}{x^2}\\
 ln&amp;#x27;&amp;#x27;&amp;#x27;(x)&amp;amp;= \frac{2}{x^3}\\
 ln^{(4)}(x)&amp;amp;= -\frac{6}{x^4}\\
 ln^{(5)}(x)&amp;amp;= \frac{24}{x^5}\\
 \cdots
\end{align*}\]&lt;/object&gt;
&lt;p&gt;&lt;em&gt;None&lt;/em&gt; of these is defined at 0 either! The Maclaurin series won't work here,
and we'll have to turn to its generalization - the Taylor series:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d901c7af02dad6f8fefe97711dd287ccf8bb7bf4.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p(x) = f(a)+\frac{f&amp;#x27;(a)}{1!}(x-a)+\frac{f&amp;#x27;&amp;#x27;(a)}{2!}(x-a)^2+\frac{f&amp;#x27;&amp;#x27;&amp;#x27;(a)}{3!}(x-a)^3+\cdots=\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n\]&lt;/object&gt;
&lt;p&gt;This is a power series that provides an approximation for &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; around
any point &lt;em&gt;a&lt;/em&gt; where &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; is &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Smoothness"&gt;finite and differentiable&lt;/a&gt;. It's easy to use exactly the
same technique to develop this series as we did for Maclaurin.&lt;/p&gt;
&lt;p&gt;Let's use this to approximate &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/86d082e11ec9c0d5b2a4df154c6b4a0755b4b512.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(x)&lt;/object&gt; around &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7caf6056913504f0508c65faf2dc3f94ff65bcfd.svg" style="height: 12px;" type="image/svg+xml"&gt;x=1&lt;/object&gt;, where this
function is well behaved. &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/597f6fdf6538b7be50426035591ea5ca5b157af3.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(1)=0&lt;/object&gt; and substituting &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7caf6056913504f0508c65faf2dc3f94ff65bcfd.svg" style="height: 12px;" type="image/svg+xml"&gt;x=1&lt;/object&gt; into its
derivatives (as listed above) at this point, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a9abb6e999792d3ac77444b4c6e951881f1f016e.svg" style="height: 23px;" type="image/svg+xml"&gt;\[f&amp;#x27;(1)=1\quad f&amp;#x27;&amp;#x27;(1)=-1\quad f&amp;#x27;&amp;#x27;&amp;#x27;(1)=2\quad f^{(4)}(1)=-6\quad f^{(5)}(1)=24\]&lt;/object&gt;
&lt;p&gt;There's a pattern here: generally, the &lt;em&gt;n&lt;/em&gt;-th derivative at 1 is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7f1528bf6eeacf1268c34b1983e1911719c83be8.svg" style="height: 19px;" type="image/svg+xml"&gt;(n-1)!&lt;/object&gt;
with an alternating sign. Substituting into the Taylor series equation from
above we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/047a288b1f4b33e86bb44c8efe2efe38841900fb.svg" style="height: 36px;" type="image/svg+xml"&gt;\[p_{ln}(x)=(x-1)-\frac{1}{2}(x-1)^2+\frac{1}{3}(x-1)^3-\frac{1}{4}(x-1)^4+\cdots\]&lt;/object&gt;
&lt;p&gt;Here's a plot of approximations with the first &lt;em&gt;k&lt;/em&gt; terms (the function itself
is dark blue, as before):&lt;/p&gt;
&lt;img alt="Successive approximation of ln(x) with Taylor series around a=1" class="align-center" src="https://eli.thegreenplace.net/images/2024/taylor-ln.png" /&gt;
&lt;p&gt;While the approximation looks good in the vicinity of 1, it seems like all
approximations diverge dramatically at some point.
The next section helps understand what's going on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="convergence-of-power-series-and-the-ratio-test"&gt;
&lt;h2&gt;Convergence of power series and the ratio test&lt;/h2&gt;
&lt;p&gt;When approximating a function with power series (e.g. with Maclaurin or Taylor
series), a natural question to ask is: does the series actually converge to the
function it's approximating, and what are the conditions on this convergence?&lt;/p&gt;
&lt;p&gt;Now it's time to treat these questions a bit more rigorously. We'll be using
the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Ratio_test"&gt;ratio test&lt;/a&gt; to check for
convergence. Generally, for a series:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/13c57e0553761f7737331bbccfdc94b42bb038de.svg" style="height: 49px;" type="image/svg+xml"&gt;\[\sum_{n=1}^\infty a_n\]&lt;/object&gt;
&lt;p&gt;We'll administer this test:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/490f7edeea11652357aadca50c49c20606626f9c.svg" style="height: 44px;" type="image/svg+xml"&gt;\[L = \lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|\]&lt;/object&gt;
&lt;p&gt;And check the conditions for which &lt;object class="valign-m2" data="https://eli.thegreenplace.net/images/math/8ecda4d5752e9fa856c4cfc01e67e59c12960eeb.svg" style="height: 14px;" type="image/svg+xml"&gt;L &amp;lt; 1&lt;/object&gt;, meaning that our series
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Absolute_convergence"&gt;converges absolutely&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's start with our Maclaurin series for &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d65e37d74a88b8c2f36e5cdf395111e243986b2c.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p_{cos}(x)=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\frac{x^8}{8!}-\cdots=1+\sum_{n=1}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!}\]&lt;/object&gt;
&lt;p&gt;Ignoring the constant term, we'll write out the ratio limit. Note that because
of the absolute value, we can ignore the power-of-minus-one term too:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9ac968436c92d8abae1906d06c9d561e5b91279d.svg" style="height: 144px;" type="image/svg+xml"&gt;\[\begin{align*}
L &amp;amp;= \lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|\\
  &amp;amp;= \lim_{n\to\infty}\left| \frac{x^{2n+2} (2n)!}{(2n+2)! x^{2n}}\right|\\
  &amp;amp;= \lim_{n\to\infty}\left| \frac{x^2}{(2n+1)(2n+2)}\right|
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Since the limit contents are independent of &lt;em&gt;x&lt;/em&gt;, it's obvious that
that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/eb9fbe260c35041e881bc5c2d5a31041e22f8ca8.svg" style="height: 12px;" type="image/svg+xml"&gt;L=0&lt;/object&gt; for any &lt;em&gt;x&lt;/em&gt;. This means that the series converges to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt;
at any &lt;em&gt;x&lt;/em&gt;, given an infinite number of terms. This matches our intuition for
this function, which is well-behaved (smooth everywhere).&lt;/p&gt;
&lt;p&gt;Now on to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/86d082e11ec9c0d5b2a4df154c6b4a0755b4b512.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(x)&lt;/object&gt; with its Taylor series around &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7caf6056913504f0508c65faf2dc3f94ff65bcfd.svg" style="height: 12px;" type="image/svg+xml"&gt;x=1&lt;/object&gt;. The
series is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/fcd9a0e50cfc2114bd0cbe3d9867d1e2a8b68740.svg" style="height: 50px;" type="image/svg+xml"&gt;\[p_{ln}(x)=(x-1)-\frac{1}{2}(x-1)^2+\frac{1}{3}(x-1)^3-\frac{1}{4}(x-1)^4+\cdots=\sum_{n=1}^{\infty} \frac{(-1)^{n+1} (x-1)^n}{n}\]&lt;/object&gt;
&lt;p&gt;Once again, writing out the ratio limit:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c527a49c5271b5b9df3883ccb0491a77ca9eb6da.svg" style="height: 192px;" type="image/svg+xml"&gt;\[\begin{align*}
L &amp;amp;= \lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|\\
  &amp;amp;= \lim_{n\to\infty}\left| \frac{(x-1)^{n+1} n}{(n+1) (x-1)^n}\right|\\
  &amp;amp;= \lim_{n\to\infty}\left| \frac{n(x-1)}{(n+1)}\right|\\
 &amp;amp;= \left|x-1\right| \lim_{n\to\infty}\left| \frac{n}{(n+1)}\right|=\left| x-1\right|
\end{align*}\]&lt;/object&gt;
&lt;p&gt;To converge, we require:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d741683ff7d9cd9515baf587944b501a778e860f.svg" style="height: 19px;" type="image/svg+xml"&gt;\[L=\left| x-1\right|&amp;lt;1\]&lt;/object&gt;
&lt;p&gt;The solution of this inequality is &lt;object class="valign-m2" data="https://eli.thegreenplace.net/images/math/0019851d0336bfbf91c4645cd3afab9eb4e3d29c.svg" style="height: 14px;" type="image/svg+xml"&gt;0 &amp;lt; x &amp;lt; 2&lt;/object&gt;. Therefore, the series
converges to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/86d082e11ec9c0d5b2a4df154c6b4a0755b4b512.svg" style="height: 19px;" type="image/svg+xml"&gt;ln(x)&lt;/object&gt; only in this range of &lt;em&gt;x&lt;/em&gt;. This is also what we
observe in the latest plot. Another way to say it: the &lt;em&gt;radius of convergence&lt;/em&gt;
of the series around &lt;em&gt;x=1&lt;/em&gt; is 1.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;If this explanation and the plot of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; following it don't
convince you, consider watching &lt;a class="reference external" href="https://www.youtube.com/watch?v=3d6DsjIBzJ4"&gt;this video by 3Blue1Brown&lt;/a&gt; - it
includes more visualizations as well as a compelling alternative intuition
using integrals and area.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;p class="first"&gt;Note that since &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/562597441eed562140c81684902007f6f275c940.svg" style="height: 19px;" type="image/svg+xml"&gt;cos(x)&lt;/object&gt; is cyclic, all we really need is good
approximations in the range &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/0dd5a9ce1ef753379639c54a347fd611ca7a1937.svg" style="height: 19px;" type="image/svg+xml"&gt;[-\pi, \pi)&lt;/object&gt;. Our plot only shows the
positive &lt;em&gt;x&lt;/em&gt; axis; it looks like a mirror image on the negative side, so
we see that a pretty good approximation is achieved by the time we reach
&lt;em&gt;k=5&lt;/em&gt;.&lt;/p&gt;
&lt;p class="last"&gt;This is also a good place to
note that while Maclaurin series are important in Calculus, it's not the
&lt;em&gt;best&lt;/em&gt; approximation for numerical analysis purposes; there are
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Remez_algorithm"&gt;better approximations&lt;/a&gt;
that converge faster.&lt;/p&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Projections and Projection Matrices</title><link href="https://eli.thegreenplace.net/2024/projections-and-projection-matrices/" rel="alternate"></link><published>2024-06-26T05:56:00-07:00</published><updated>2024-06-26T12:58:37-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-06-26:/2024/projections-and-projection-matrices/</id><summary type="html">&lt;p&gt;We'll start with a visual and intuitive representation of what a projection is.
In the following diagram, we have vector &lt;em&gt;b&lt;/em&gt; in the usual 3-dimensional space
and two possible projections - one onto the &lt;em&gt;z&lt;/em&gt; axis, and another onto the &lt;em&gt;x,y&lt;/em&gt;
plane.&lt;/p&gt;
&lt;img alt="Projection of a 3d vector onto axis and plane" class="align-center" src="https://eli.thegreenplace.net/images/2024/projection-3d.png" /&gt;
&lt;p&gt;If we think of 3D space as spanned â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;We'll start with a visual and intuitive representation of what a projection is.
In the following diagram, we have vector &lt;em&gt;b&lt;/em&gt; in the usual 3-dimensional space
and two possible projections - one onto the &lt;em&gt;z&lt;/em&gt; axis, and another onto the &lt;em&gt;x,y&lt;/em&gt;
plane.&lt;/p&gt;
&lt;img alt="Projection of a 3d vector onto axis and plane" class="align-center" src="https://eli.thegreenplace.net/images/2024/projection-3d.png" /&gt;
&lt;p&gt;If we think of 3D space as spanned by the usual basis vectors, a projection
onto the &lt;em&gt;z&lt;/em&gt; axis is simply:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/cb40026197c880dea45a56980b2c16a20248fddc.svg" style="height: 64px;" type="image/svg+xml"&gt;\[b_z=\begin{bmatrix}
0 \\
0 \\
z
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;A couple of intuitive ways to think about what a projection means:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The projection of &lt;em&gt;b&lt;/em&gt; on the &lt;em&gt;z&lt;/em&gt; axis is a vector in the direction of the
&lt;em&gt;z&lt;/em&gt; axis that's closest to &lt;em&gt;b&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The projection of &lt;em&gt;b&lt;/em&gt; on the &lt;em&gt;z&lt;/em&gt; axis is the shadow cast by &lt;em&gt;b&lt;/em&gt; when a flashlight
is pointed at it in the direction of the &lt;em&gt;z&lt;/em&gt; axis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We'll see a more formal definition soon. A projection onto the &lt;em&gt;x,y&lt;/em&gt; plane is
similarly easy to express.&lt;/p&gt;
&lt;div class="section" id="projection-onto-a-line"&gt;
&lt;h2&gt;Projection onto a line&lt;/h2&gt;
&lt;p&gt;Projecting onto an axis is easy - as the diagram shows, it's simply taking the
vector component in the direction of the axis. But how about projections onto
arbitrary lines?&lt;/p&gt;
&lt;img alt="Projection of a 3d vector onto another 3D vector" class="align-center" src="https://eli.thegreenplace.net/images/2024/projection-line.png" /&gt;
&lt;p&gt;In vector space, a line is just all possible scalings of some vector &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Speaking more formally now, we're interested in the projection of
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; onto &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;, where the arrow over a letter means it's a
vector. The projection (which we call &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/507636722a388cbbc6ae26997a38a622bf9108ff.svg" style="height: 21px;" type="image/svg+xml"&gt;\vec{b_a}&lt;/object&gt;) is the
closest vector to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; in the direction of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;. In other
words, the dotted line in the diagram is at a right angle to the line &lt;em&gt;a&lt;/em&gt;;
therefore, the error vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6520ef4731bea7ef9760aa68288a1ba843fbde82.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}&lt;/object&gt; is orthogonal to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;This orthogonality gives us the tools we need to find the projection. We'll want
to find a constant &lt;em&gt;c&lt;/em&gt; such that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8ecbc15ef33b2a9c390302e10702a9a2b0d93af9.svg" style="height: 20px;" type="image/svg+xml"&gt;\[\vec{b_a}=c\vec{a}\]&lt;/object&gt;
&lt;p&gt;&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6520ef4731bea7ef9760aa68288a1ba843fbde82.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}&lt;/object&gt; is orthogonal to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;, meaning that their dot
product is zero: &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/95d0c31e1d2b9ede3ff6136e7d2f93975f66f266.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}\cdot\vec{a}=0&lt;/object&gt;. We'll use the distributive
property of the dot product in what follows:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6bb149c58b4741e8c2c06288a5325080c40b8a33.svg" style="height: 128px;" type="image/svg+xml"&gt;\[\begin{align*}
\vec{a}\cdot\vec{e}&amp;amp;=0 \\
\vec{a}\cdot(\vec{b}-c\vec{a})&amp;amp;=0\\
\vec{a}\cdot\vec{b}-c\vec{a}\cdot\vec{a}&amp;amp;=0\\
c&amp;amp;=\frac{\vec{a}\cdot\vec{b}}{\vec{a}\cdot\vec{a}}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Note that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/7865ca07c8b7f891f073525c24dad20d9095cef5.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}\cdot\vec{a}&lt;/object&gt; is the squared &lt;em&gt;magnitude&lt;/em&gt; of
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;; for a unit vector this would be 1. This is why it doesn't
matter if &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt; is a unit vector or not - we normalize it anyway.&lt;/p&gt;
&lt;p&gt;We have a formula for &lt;em&gt;c&lt;/em&gt; now - we can find it given &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt; and
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;. To prepare for what comes next, however, we'll switch
notations. We'll use matrix notation, in which vectors are - by convention -
column vectors, and a dot product can be expressed by a matrix multiplication
between a row and a column vector. Therefore:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8c4bec17d5f0605dade7d1ad5c95344cbe857db0.svg" style="height: 86px;" type="image/svg+xml"&gt;\[\begin{align*}
c&amp;amp;=\frac{a^T b}{a^T a} \Rightarrow \\
b_a&amp;amp;=\frac{a^T b}{a^T a}a
\end{align*}\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="projection-matrix"&gt;
&lt;h2&gt;Projection matrix&lt;/h2&gt;
&lt;p&gt;Since the fraction representing &lt;em&gt;c&lt;/em&gt; is a constant, we can switch the order of
the multiplication by &lt;em&gt;a&lt;/em&gt;, and then use the fact that matrix multiplication
is associative to write:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ef5a8af0ac0603aeba420410629435920eb636ab.svg" style="height: 86px;" type="image/svg+xml"&gt;\[\begin{align*}
b_a&amp;amp;=a\frac{a^T b}{a^T a}\\
b_a&amp;amp;=\frac{a a^T}{a^T a}b
\end{align*}\]&lt;/object&gt;
&lt;p&gt;In our case, since &lt;em&gt;a&lt;/em&gt; is a 3D vector, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/0fb73bd46a79f79a74feb870a6f2773674cb4144.svg" style="height: 15px;" type="image/svg+xml"&gt;a a^T&lt;/object&gt; is a 3x3 matrix &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;, while
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/a0f226ea3b10935f08820134b2eb4340a3c639e9.svg" style="height: 15px;" type="image/svg+xml"&gt;a^Ta&lt;/object&gt; is a scalar. Thus we get our
&lt;em&gt;projection matrix&lt;/em&gt; - call it &lt;em&gt;P&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b7e89f0b6600dac3697a45c4103066b6c6b16a0b.svg" style="height: 64px;" type="image/svg+xml"&gt;\[\begin{align*}
P&amp;amp;=\frac{a a^T}{a^T a}\\
b_a&amp;amp;=Pb
\end{align*}\]&lt;/object&gt;
&lt;p&gt;A recap: given some vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;, we can construct a projection
matrix &lt;em&gt;P&lt;/em&gt;. This projection matrix can take any vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; and
help us calculate its projection onto &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt; by means of a simple
matrix multiplication!&lt;/p&gt;
&lt;div class="section" id="example-of-line-projection"&gt;
&lt;h3&gt;Example of line projection&lt;/h3&gt;
&lt;p&gt;Consider our original example - projection on the &lt;em&gt;z&lt;/em&gt; axis. First, we'll
find a vector that spans the subspace represented by the &lt;em&gt;z&lt;/em&gt; axis: a trivial
vector is the unit vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e6d1b2131e20e1178d846e12f05db51d719bc20d.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_z=\begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;What's the projection matrix corresponding to this vector?&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/dee6efc0fcfd585574f933869ceaf4af566cdffd.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P = \frac{a_z a_{z}^{T}}{1} = \begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}\begin{bmatrix}0&amp;amp;0&amp;amp;1\end{bmatrix}=\begin{bmatrix}
0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Now, given any arbitrary vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; we can find its projection onto
the &lt;em&gt;z&lt;/em&gt; axis by multiplying with &lt;em&gt;P&lt;/em&gt;. For example:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/32259bb09674a6e6f361944736de956678c1ccbb.svg" style="height: 64px;" type="image/svg+xml"&gt;\[b_a=Pb=\begin{bmatrix}
0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;1
\end{bmatrix}\begin{bmatrix}
x\\
y\\
z
\end{bmatrix}=\begin{bmatrix}
0\\
0\\
z
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Another example - less trivial this time. Say we want to project vectors onto
the line spanned by the vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/45fa0fde893e5e0b211ecd84f96636732570073f.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a=\begin{bmatrix}
1 \\
3 \\
7
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Let's compute the projection matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d583958c635852c142cf22a0d0b6441a9e5bf4c3.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P = \frac{a a^{T}}{a^T a} = \frac{1}{59}\begin{bmatrix}
1 \\
3 \\
7
\end{bmatrix}\begin{bmatrix}1&amp;amp;3&amp;amp;7\end{bmatrix}=\frac{1}{59}\begin{bmatrix}
1&amp;amp;3&amp;amp;7\\
3&amp;amp;9&amp;amp;21\\
7&amp;amp;21&amp;amp;49
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Now we'll use it to calculate the projection of
&lt;object class="valign-m7" data="https://eli.thegreenplace.net/images/math/3b696fddd0995dece7097f93105cb8c3f6095dd2.svg" style="height: 26px;" type="image/svg+xml"&gt;b=\begin{bmatrix}2 &amp;amp; 8 &amp;amp; -4\end{bmatrix}^T&lt;/object&gt; onto this line:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f5f7dc240fa88cae8e630cc2774167ccffeb9812.svg" style="height: 64px;" type="image/svg+xml"&gt;\[b_a=Pb=\frac{1}{59}\begin{bmatrix}
1&amp;amp;3&amp;amp;7\\
3&amp;amp;9&amp;amp;21\\
7&amp;amp;21&amp;amp;49
\end{bmatrix}\begin{bmatrix}
2\\
8\\
-4
\end{bmatrix}=\frac{1}{59}\begin{bmatrix}
-2\\
-6\\
-14
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;To verify this makes sense, we can calculate the error vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6520ef4731bea7ef9760aa68288a1ba843fbde82.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8921404c3b6ce16571a4695866ee4b1d7c04a1fc.svg" style="height: 64px;" type="image/svg+xml"&gt;\[\begin{align*}
e&amp;amp;=b-b_a=\begin{bmatrix}
2\\
8\\
-4
\end{bmatrix}-\frac{1}{59}\begin{bmatrix}
-2\\
-6\\
-14
\end{bmatrix}=\frac{1}{59}\begin{bmatrix}
120\\
478\\
-222
\end{bmatrix}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;And check that it's indeed orthogonal to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f91db167e161815d41ba4a61228aed44d988e419.svg" style="height: 36px;" type="image/svg+xml"&gt;\[a\cdot e = \frac{1}{59}(1\cdot 120 + 3\cdot 478 + 7 \cdot -222)=0\]&lt;/object&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="projection-onto-a-vector-subspace"&gt;
&lt;h2&gt;Projection onto a vector subspace&lt;/h2&gt;
&lt;p&gt;A subspace of a vector space is a subset of vectors from the vector space that's
closed under vector addition and scalar multiplication. For
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt;, some common subspaces include lines that go through the
origin and planes that go through the origin.&lt;/p&gt;
&lt;p&gt;Therefore, the &lt;em&gt;projection onto a line&lt;/em&gt; scenario we've discussed so far is just
a special case of a projection onto a subspace. We'll look at the general case
now.&lt;/p&gt;
&lt;p&gt;Suppose we have an m-dimensional vector space &lt;img alt="\mathbb{R}^m" class="valign-0" src="https://eli.thegreenplace.net/images/math/91d9290b46ace1360a8a715bd7a1fa701277697b.png" style="height: 12px;" /&gt;, and a set
of &lt;em&gt;n&lt;/em&gt; linearly independent vectors &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/51b0b13a9bd883c4eaebdf828454602a9639ae0f.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n} \in \mathbb{R}^m&lt;/object&gt;.
We want to find a combination of these vectors that's closest to some target
vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; - in other words, to find the projection of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;
onto the subspace spanned by &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Arbitrary m-dimensional vectors are difficult to visualize, but the derivation
here follows exactly the path we've taken for projections onto lines in 3D.
There, we were looking for a constant &lt;em&gt;c&lt;/em&gt; such that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/8d264117c06befcad931b56c0e6b39d7160971ec.svg" style="height: 13px;" type="image/svg+xml"&gt;c\vec{a}&lt;/object&gt; was the
closest vector to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;. Now, we're looking for a vector
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/34b58e8a800c7893cec26ee8be79f5713d2f75c9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{c}&lt;/object&gt; which represents a linear combination of &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt;
that is closest to a target &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;If we organize &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt; as columns into a matrix called &lt;em&gt;A&lt;/em&gt;, we
can express this as:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/edde67ce36d6642650ec7aa2fbcce40e1fca6286.svg" style="height: 20px;" type="image/svg+xml"&gt;\[\vec{b_a}=A\vec{c}\]&lt;/object&gt;
&lt;p&gt;This is a matrix multiplication: &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/34b58e8a800c7893cec26ee8be79f5713d2f75c9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{c}&lt;/object&gt; is a list of coefficients
that describes some linear combination of the columns of &lt;em&gt;A&lt;/em&gt;. As before,
we want the error vector &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/7326fd9b29ba58374541d60332b01f85c1c23230.svg" style="height: 21px;" type="image/svg+xml"&gt;\vec{e}=\vec{b}-\vec{b_a}&lt;/object&gt; to be orthogonal to the
subspace onto which we're projecting: this means it's orthogonal to every
one of &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt;.
The fact that vectors &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/537ed914f38fcd3604c0ba95c6d20d9e11e7e47c.svg" style="height: 16px;" type="image/svg+xml"&gt;\vec{a_n}&lt;/object&gt;
are orthogonal to &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6520ef4731bea7ef9760aa68288a1ba843fbde82.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{e}&lt;/object&gt; can be expressed as &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f53ea23a2082c5c678880ead067d369bf5747e98.svg" style="height: 88px;" type="image/svg+xml"&gt;\[\begin{align*}
a_{1}^{T}e&amp;amp;=0\\
\vdots\\
a_{n}^{T}e&amp;amp;=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;This is a system of linear equations, and thus it can be represented as a matrix
multiplication by a matrix with vectors &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/94740c57487dc021c3d8f42cd019025e09b9176f.svg" style="height: 20px;" type="image/svg+xml"&gt;a_{k}^T&lt;/object&gt; in its rows; this matrix
is just &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/e5ea66117060e4e5b2e83c1174d29dfc439d817a.svg" style="height: 15px;" type="image/svg+xml"&gt;A^T&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e05f35974fa739141ca1b4acc7d93882b0195cc2.svg" style="height: 17px;" type="image/svg+xml"&gt;\[A^T e=0\]&lt;/object&gt;
&lt;p&gt;But &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b07fe8d5a1561a36a0dd2ce8df88b750cc46fdc2.svg" style="height: 13px;" type="image/svg+xml"&gt;e=b-Ac&lt;/object&gt;, so:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/55229c55b419090aff119cfb1d680c2684324d90.svg" style="height: 46px;" type="image/svg+xml"&gt;\[\begin{align*}
A^T (b-Ac)&amp;amp;=0 \Rightarrow \\
A^Tb&amp;amp;=A^TAc
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Since the columns of &lt;em&gt;A&lt;/em&gt; are linearly independent, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/ae3d6dabd2cab15dfde53285aaceb75a173848e1.svg" style="height: 15px;" type="image/svg+xml"&gt;A^T A&lt;/object&gt; is an
invertible matrix &lt;a class="footnote-reference" href="#footnote-4" id="footnote-reference-4"&gt;[4]&lt;/a&gt;, so we can isolate &lt;em&gt;c&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6cb5e5ca73be03d0c2bd7d1f2d6fb9d75eefc5b0.svg" style="height: 22px;" type="image/svg+xml"&gt;\[c=(A^T A)^{-1}A^T b\]&lt;/object&gt;
&lt;p&gt;Then the projection &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a839236c9cec6e8fda2ed32f30ae7eb6cb1a74a1.svg" style="height: 21px;" type="image/svg+xml"&gt;\vec_{b_a}&lt;/object&gt; is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/5bf80a966abd492b1f042e69562ca51751e30cd9.svg" style="height: 22px;" type="image/svg+xml"&gt;\[b_a=Ac=A(A^T A)^{-1}A^T b\]&lt;/object&gt;
&lt;p&gt;Similarly to the line example, we can also define a &lt;em&gt;projection matrix&lt;/em&gt;
as:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/cc188c53b2df3c538d277023816599128c49d17e.svg" style="height: 22px;" type="image/svg+xml"&gt;\[P=A(A^T A)^{-1}A^T\]&lt;/object&gt;
&lt;p&gt;Given a vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;, &lt;em&gt;P&lt;/em&gt; projects it onto the subspace spanned by
the vectors &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/62c1dc31d09efc4e5dab47fab2aa34095d63b435.svg" style="height: 17px;" type="image/svg+xml"&gt;\vec{a_1},\dots,\vec{a_n}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c95ad835eda46f7ee7b27dc6b6489a1730e481d8.svg" style="height: 14px;" type="image/svg+xml"&gt;\[b_a=Pb\]&lt;/object&gt;
&lt;p&gt;Let's make sure the dimensions work out. Recall that &lt;em&gt;A&lt;/em&gt; consists of &lt;em&gt;n&lt;/em&gt;
columns, each with &lt;em&gt;m&lt;/em&gt; rows. So we have:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3bfa44eebfc358b6b6dc73f7db4d270698168a07.svg" style="height: 129px;" type="image/svg+xml"&gt;\[\begin{matrix}
A &amp;amp; (m\times n) \\
A^T &amp;amp;  (n\times m)\\
A^T A &amp;amp; (n\times n)  \\
(A^T A)^{-1} &amp;amp; (n\times n) \\
A(A^T A)^{-1} &amp;amp; (m\times n) \\
A(A^T A)^{-1}A^T &amp;amp; (m\times m) \\
\end{matrix}\]&lt;/object&gt;
&lt;p&gt;Since the vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; is m-dimensional, &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/93f582c0800ee17439c9ba6b47d0cdf0ccf0c8f5.svg" style="height: 12px;" type="image/svg+xml"&gt;Pb&lt;/object&gt; is valid and the
result is another m-dimensional vector - the projection &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/03ebc36d8150942788540d021bc3c47fedf9a0c3.svg" style="height: 21px;" type="image/svg+xml"&gt;\vec{b}_a&lt;/object&gt;.&lt;/p&gt;
&lt;div class="section" id="example-of-subspace-projection"&gt;
&lt;h3&gt;Example of subspace projection&lt;/h3&gt;
&lt;p&gt;At the beginning of this post there's a diagram showing the projection of
an arbitrary vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; onto a line and onto a
plane. We'll find the projection matrix for the plane case now. The projection
is onto the &lt;em&gt;xy&lt;/em&gt; plane, which is spanned by these vectors:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/610517a466bd35bd4c63c165b3d4870784ce2b6a.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_x=\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
a_y=\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Collecting them into a single matrix &lt;em&gt;A&lt;/em&gt;, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/67a17831ae3518fac78c604c82710fe6a5590c5f.svg" style="height: 64px;" type="image/svg+xml"&gt;\[A=\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1\\
0 &amp;amp; 0
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;To find &lt;em&gt;P&lt;/em&gt;, let's first calculate &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/ae3d6dabd2cab15dfde53285aaceb75a173848e1.svg" style="height: 15px;" type="image/svg+xml"&gt;A^T A&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ec23a776b18efb2511bc583293d7d70cf549f258.svg" style="height: 64px;" type="image/svg+xml"&gt;\[A^T A=
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1\\
0 &amp;amp; 0
\end{bmatrix}=
\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;This happens to be the identity matrix, so its inverse is itself. Thus, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e618c6d4ea50cf468e938b555c0ea335aa7c5006.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P=A(A^T A)^{-1}A^T=AIA^T=AA^T=
\begin{bmatrix}
1 &amp;amp; 0\\
0 &amp;amp; 1\\
0 &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0
\end{bmatrix}=
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;We can now project an arbitrary vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; onto this plane by
multiplying it with this P:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c273b8d9daf567854bb34be9f8cf6c2fe091dcac.svg" style="height: 64px;" type="image/svg+xml"&gt;\[b_a=Pb=
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}=
\begin{bmatrix}
x \\
y \\
0
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Granted, this is a fairly trivial example - but it works in the general case. As
an exercise, pick a different pair of independent vectors and find the
projection matrix onto the plane spanned by them; then, verify that the
resulting error is orthogonal to the plane.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="properties-of-projection-matrices"&gt;
&lt;h2&gt;Properties of projection matrices&lt;/h2&gt;
&lt;p&gt;Projection matrices have some interesting properties that are educational to
review.&lt;/p&gt;
&lt;p&gt;First, projection matrices are &lt;strong&gt;symmetric&lt;/strong&gt;. To understand why, first recall
how a transpose of a matrix product is done:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ef0a9adc5755ad2121bd29acf8c1acae37069fe9.svg" style="height: 22px;" type="image/svg+xml"&gt;\[(AB)^T=B^T A^T\]&lt;/object&gt;
&lt;p&gt;As a warm-up, we can show that &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/ae3d6dabd2cab15dfde53285aaceb75a173848e1.svg" style="height: 15px;" type="image/svg+xml"&gt;A^T A&lt;/object&gt; is symmetric:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ed041fc5a1e961cb015cce0aa43bae716ff760ec.svg" style="height: 22px;" type="image/svg+xml"&gt;\[(A^T A)^T=A^T (A^T)^T=A^T A\]&lt;/object&gt;
&lt;p&gt;Now, let's transpose &lt;em&gt;P&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9891c74fb6f968ef5fd79fc46586671ae7398a0f.svg" style="height: 109px;" type="image/svg+xml"&gt;\[\begin{align*}
P&amp;amp;=A(A^T A)^{-1}A^T \\
P^T&amp;amp;=(A(A^T A)^{-1}A^T)^T\\
&amp;amp;=((A^T A)^{-1}A^T)^T A^T\\
&amp;amp;=A(A^T A)^{-1}A^T=P
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Here we've used the fact that the inverse of a symmetric matrix is also
symmetric, and we see that indeed &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/359c9df289ef29f45b04ffcbdedb39c5e1929bf5.svg" style="height: 15px;" type="image/svg+xml"&gt;P^T=P&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Second, projection matrices are &lt;strong&gt;idempotent&lt;/strong&gt;: &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/a1becab925d990bbb1bc40503523bc757abd4ad1.svg" style="height: 15px;" type="image/svg+xml"&gt;P^2=P&lt;/object&gt;; this isn't
hard to prove either:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ab435c0da6c2d7337c546531b022fc8ce34545b7.svg" style="height: 139px;" type="image/svg+xml"&gt;\[\begin{align*}
P^2&amp;amp;=A(A^T A)^{-1}A^T A(A^T A)^{-1}A^T\\
   &amp;amp;=A(A^T A)^{-1}(A^T A)(A^T A)^{-1}A^T\\
   &amp;amp;=A(A^T A)^{-1}[(A^T A)(A^T A)^{-1}]A^T\\
   &amp;amp;=A(A^T A)^{-1}IA^T\\
   &amp;amp;=A(A^T A)^{-1}A^T=P
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Intuitive explanation: think about what a projection does - given some
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;, it calculates
the closest vector to it in the desired subspace. If we
try to project this projection again - what will we get? Well, still the closest
vector in that subspace - itself! In other words:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f008b6de1991c31650bfddea89ff0b8aa55cef90.svg" style="height: 19px;" type="image/svg+xml"&gt;\[b_a=Pb=P(Pb)\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="projections-onto-orthogonal-subspaces"&gt;
&lt;h2&gt;Projections onto orthogonal subspaces&lt;/h2&gt;
&lt;p&gt;There's another special case of projections that is interesting to discuss:
projecting a vector onto orthogonal subspaces. We'll work through this using an
example.&lt;/p&gt;
&lt;p&gt;Consider the vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/148626e74e82c6b2d9e8c6cd31f482fbabeaead8.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_1=\begin{bmatrix}
1 \\
-2 \\
3
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;We'll find the projection matrix for this vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f6497d632c11809e51a21587f2a037eddc927946.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P_1=\frac{a_1 a_{1}^T}{a_{1}^T a_1}=
\frac{1}{14}
\begin{bmatrix}
1 &amp;amp; -2 &amp;amp; 3\\
-2 &amp;amp; 4 &amp;amp; -6\\
3 &amp;amp; -6 &amp;amp; 9
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Now, consider the following vector, which is orthogonal to &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b7524de3f703a23d5d20820341776bfe30276686.svg" style="height: 16px;" type="image/svg+xml"&gt;\vec{a_1}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b310d91fb1cb0978c5480837b024fe809885fd8c.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_2=\begin{bmatrix}
-3 \\
0 \\
1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Its projection matrix is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2363b2e0296d2c0f409d1a5c122622127e0f84e7.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P_2=\frac{a_2 a_{2}^T}{a_{2}^T a_2}=
\frac{1}{10}
\begin{bmatrix}
9 &amp;amp; 0 &amp;amp; -3\\
0 &amp;amp; 0 &amp;amp; 0\\
-3 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;It's trivial to check that both &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/3aba26d01e9d3547d4804518e43131ca778dc418.svg" style="height: 15px;" type="image/svg+xml"&gt;P_1&lt;/object&gt; and &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/97240d9f331df7b57f3d0766ffa6fba38a888857.svg" style="height: 15px;" type="image/svg+xml"&gt;P_2&lt;/object&gt; satisfy the
properties of projective matrices; what's more interesting is that
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/f32736e70c594bb14a4d8b9acb30d27d688689a0.svg" style="height: 15px;" type="image/svg+xml"&gt;P_1 + P_2&lt;/object&gt; does as well - so it's also a proper projection matrix!&lt;/p&gt;
&lt;p&gt;To take it a step further, consider yet another vector:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b430bb9358af10100ffe7777868b175aed0c0298.svg" style="height: 64px;" type="image/svg+xml"&gt;\[a_3=\begin{bmatrix}
-1 \\
-5 \\
-3
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;The vectors &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/412ba4caae8a1f28842a91cc0c19e7db9d1150d7.svg" style="height: 19px;" type="image/svg+xml"&gt;(\vec{a_1},\vec{a_2},\vec{a_3})&lt;/object&gt; are all
mutually orthogonal, and thus form an orthogonal basis for &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt;.
We can calculate &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/389d569332ecfb93594c5e799a493f37c74f4759.svg" style="height: 15px;" type="image/svg+xml"&gt;P_3&lt;/object&gt; in the usual way, and get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ea023279be309ce07524a111053220e4cf43138f.svg" style="height: 64px;" type="image/svg+xml"&gt;\[P_3=\frac{a_3 a_{3}^T}{a_{3}^T a_3}=
\frac{1}{35}
\begin{bmatrix}
1 &amp;amp; 5 &amp;amp; 3\\
5 &amp;amp; 25 &amp;amp; 15\\
3 &amp;amp; 15 &amp;amp; 9
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;Not only is &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/42161e8e714578e74e3c1d193afac51c51eb7cd2.svg" style="height: 15px;" type="image/svg+xml"&gt;P_1+P_2+P_3&lt;/object&gt; is a projection matrix, it's a very familiar
matrix in general:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e2a2624bda06b15bfa1595a26f13a0d20d46848f.svg" style="height: 14px;" type="image/svg+xml"&gt;\[P_1+P_2+P_3=I\]&lt;/object&gt;
&lt;p&gt;This is equivalent to saying that for any vector &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3e8d17f216fa7fa3ed3f7f5e36b9412d2f9c24d2.svg" style="height: 19px;" type="image/svg+xml"&gt;\[(P_1+P_2+P_3)b=b\]&lt;/object&gt;
&lt;p&gt;Hopefully this makes intuitive sense because it's just expressing
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/71fa108edb785ca9f729fa3cd5ad18556dd682e4.svg" style="height: 18px;" type="image/svg+xml"&gt;\vec{b}&lt;/object&gt; in an alternative basis for &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt; &lt;a class="footnote-reference" href="#footnote-5" id="footnote-reference-5"&gt;[5]&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;We're dealing
with vector spaces, where we don't really have lines - only vectors.
A line is just a visual way to think about certain subspaces of the
vector space &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt;. Specifically, a line through the
origin (lines that don't go through the origin belong in
&lt;a class="reference external" href="https://eli.thegreenplace.net/2018/affine-transformations/"&gt;affine spaces&lt;/a&gt;)
is a way to represent &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/0dea7e472c25bda0904291774c1a1a5c72aa09d1.svg" style="height: 17px;" type="image/svg+xml"&gt;\forall c, c\vec{a}&lt;/object&gt; where &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;
is a vector in the same direction as this line and &lt;em&gt;c&lt;/em&gt; is a constant;
in other words it's the subspace of &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b15d4bbfe66586a67fc56425a1b94e0466f3e319.svg" style="height: 15px;" type="image/svg+xml"&gt;\mathbb{R}^3&lt;/object&gt; spanned by
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/1e37c650a8e07c81d1a1b03f075bdf45139d65e9.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{a}&lt;/object&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;By the rules of matrix multiplication: we're multiplying a column vector
(a 3x1 matrix) by a row vector (a 1x3 matrix). The multiplication is
allowed because the inner dimensions match, and the result is a 3x3
matrix.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Recall from the earlier example: we're dropping the explicit vector
markings to be able to write matrix arithmetic more naturally. By
default vectors are column vectors, so &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/3ef5d107cf603c9f2987896f4a574a0967ad3059.svg" style="height: 15px;" type="image/svg+xml"&gt;v^T w&lt;/object&gt; expresses the
dot product between vectors &lt;img alt="\vec{v}" class="valign-0" src="https://eli.thegreenplace.net/images/math/39a3a59a8f524cf72620db07b9ba7cdce9fc9391.png" style="height: 13px;" /&gt; and &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/d45128696127d3ae74860c6f8b14ce6ca20d15e7.svg" style="height: 13px;" type="image/svg+xml"&gt;\vec{w}&lt;/object&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;It's possible to prove this statement, but this post is already long
enough.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This is a special case of a &lt;a class="reference external" href="https://eli.thegreenplace.net/2015/change-of-basis-in-linear-algebra/"&gt;change of basis&lt;/a&gt;,
in which the basis is orthogonal.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Method of differences and Newton polynomials</title><link href="https://eli.thegreenplace.net/2024/method-of-differences-and-newton-polynomials/" rel="alternate"></link><published>2024-04-16T05:54:00-07:00</published><updated>2024-06-26T12:58:37-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-04-16:/2024/method-of-differences-and-newton-polynomials/</id><summary type="html">&lt;p&gt;I was reading about Babbage's &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Difference_engine"&gt;Difference engine&lt;/a&gt; the other
day, and stumbled upon a very interesting application of the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Divided_differences"&gt;forward differences&lt;/a&gt;
method.
It turns out that if we get a sequence generated by a polynomial, under certain
conditions we can find the generating polynomial from just a few elements in â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was reading about Babbage's &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Difference_engine"&gt;Difference engine&lt;/a&gt; the other
day, and stumbled upon a very interesting application of the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Divided_differences"&gt;forward differences&lt;/a&gt;
method.
It turns out that if we get a sequence generated by a polynomial, under certain
conditions we can find the generating polynomial from just a few elements in
the sequence.&lt;/p&gt;
&lt;p&gt;For example, 0, 1, 5, 12, 22, 35, 51... is a sequence known as
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Pentagonal_number"&gt;the pentagonal numbers&lt;/a&gt;,
and we can use this technique to figure out that the polynomial
&lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/5b260a9d5bbd5288c720d246b76f459a34dc43b2.svg" style="height: 25px;" type="image/svg+xml"&gt;\frac{3n^2}{2}-\frac{n}{2}&lt;/object&gt; generates it &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="notation"&gt;
&lt;h2&gt;Notation&lt;/h2&gt;
&lt;p&gt;Let's start with some mathematical notation. We'll call the underlying function
generating the sequence &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt;. In our example, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e5a1616b377fb66cb2a07343c5d3cd1eb232f69f.svg" style="height: 19px;" type="image/svg+xml"&gt;f(0)=0&lt;/object&gt;,
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a259694d0b5a8eb2ab2bdaf5233a65b6564b6971.svg" style="height: 19px;" type="image/svg+xml"&gt;f(1)=1&lt;/object&gt;, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/462967692d2cae32bb7c97fff3eba434f18fa36b.svg" style="height: 19px;" type="image/svg+xml"&gt;f(2)=5&lt;/object&gt;, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/28e07a49a51fb9429d839ceba23f0f210b7e227f.svg" style="height: 19px;" type="image/svg+xml"&gt;f(3)=12&lt;/object&gt; and so on.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The &lt;em&gt;first difference&lt;/em&gt; is the sequence &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/90c44106d8d77c32cb18afdcb981a76f7560e2e0.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(0)=f(1)-f(0)&lt;/object&gt;,
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/304708f95d4d8aa49001cce85a05201cf46b7613.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(1)=f(2)-f(1)&lt;/object&gt;, etc.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;second difference&lt;/em&gt; is the sequence &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/165a28def741d96ae27b295d3afe0845566e4c41.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(0)=\Delta f(1) - \Delta f(0)&lt;/object&gt;,
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e54a7d96a0cdf8a5507cf4242033555f5e9f4129.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(1)=\Delta f(2) - \Delta f(1)&lt;/object&gt; etc.&lt;/li&gt;
&lt;li&gt;In general, the k-th difference is: &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/1c79f2519166202a7529b72c5aecb083b285a8df.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta ^k f(n)=\Delta ^{k-1}f(n+1) - \Delta ^{k-1}f(n)&lt;/object&gt;.&lt;/li&gt;
&lt;li&gt;As a starting condition in the induction of differences, we can say that
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; itself is the 0-th difference.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="difference-table"&gt;
&lt;h2&gt;Difference table&lt;/h2&gt;
&lt;p&gt;We can construct the &lt;em&gt;difference table&lt;/em&gt; for our sequence and observe its
properties. In a difference table, the first column is &lt;em&gt;n&lt;/em&gt;, which runs from 0
to whatever number of elements we have for the sequence. The second column is
the values of the sequence at these &lt;em&gt;n&lt;/em&gt;. Then come the first difference, the
second difference and so on. For our sample sequence we get the table:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/ddf012f5699fff847f59b58ff97285c32fa72e99.svg" style="height: 148px;" type="image/svg+xml"&gt;\[\begin{matrix}
n &amp;amp; f(n) &amp;amp; \Delta f(n) &amp;amp; \Delta ^2 f(n)  &amp;amp;  \Delta ^3 f(n) \\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 3 &amp;amp; 0 \\
1 &amp;amp; 1 &amp;amp; 4 &amp;amp; 3 &amp;amp; 0 \\
2 &amp;amp; 5 &amp;amp; 7 &amp;amp; 3 &amp;amp; 0 \\
3 &amp;amp; 12 &amp;amp; 10 &amp;amp; 3 &amp;amp; \\
4 &amp;amp; 22 &amp;amp; 13 &amp;amp;  &amp;amp;  \\
5 &amp;amp; 35 &amp;amp; &amp;amp; &amp;amp;
\end{matrix}\]&lt;/object&gt;
&lt;p&gt;Notice how at some point the differences become all-zero! We'll soon see why.&lt;/p&gt;
&lt;p&gt;Obviously, we can construct such a table from only the column of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; -
that's what we just did! A more interesting observation is that if we accept
that all differences (columns) are 0 from a certain point, we can also construct
this table from just the first row! For example, with &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/233b23c15ba8894ccefa4845ae4b1954cc2c0ad7.svg" style="height: 19px;" type="image/svg+xml"&gt;f(0)&lt;/object&gt; and
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/607ab76bc5cd741a68f8fa4a934735835cda03c8.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(0)&lt;/object&gt; in hand, we know &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/3c70ae40a98b1cf4bb34daa3ba2573e31e4bfd34.svg" style="height: 19px;" type="image/svg+xml"&gt;f(1)&lt;/object&gt;; with &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/607ab76bc5cd741a68f8fa4a934735835cda03c8.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(0)&lt;/object&gt; and
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/673fc969775d75c8845c11bf81e042c20012dedf.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(0)&lt;/object&gt;, we know &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/90bb6aa06dd392c5af19397ce81dc975b17b5bba.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(1)&lt;/object&gt; etc. Try it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="inferring-the-polynomial-s-degree-from-the-table"&gt;
&lt;h2&gt;Inferring the polynomial's degree from the table&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; if &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; has degree &lt;em&gt;k&lt;/em&gt;, then the &lt;em&gt;k&lt;/em&gt;-th difference column
in the table is constant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; this is a general &lt;em&gt;k&lt;/em&gt;-th degree polynomial with coefficients
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b93e6f239fad8d0444d74634490a6e0e067b8954.svg" style="height: 11px;" type="image/svg+xml"&gt;a_k&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/49c985cb16cfb154ce40e4fbc738712a6f649723.svg" style="height: 22px;" type="image/svg+xml"&gt;\[f(n)=a_k n^k + a_{k-1} n^{k-1} + \cdots + a_1 n + a_0\]&lt;/object&gt;
&lt;p&gt;By definition of the first difference, if we expand the polynomial form and
perform the subtraction per power of &lt;em&gt;n&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/84a9b56982e89d627d41a311f021d7dae2c4938a.svg" style="height: 87px;" type="image/svg+xml"&gt;\[\begin{align*}
  \Delta f(n) &amp;amp;= f(n+1)-f(n) = a_k (n+1)^{k} - a_k n^k + a_{k-1} (n+1)^{k-1} - a_{k-1} n^{k-1}+\cdots \\
              &amp;amp;= \sum_{j=0}^{k} a_j(n+1)^j-a_j n^j
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Using the binomial theorem we know that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/07156a3c7411c2da37b620fbb22a4b835c53fcfb.svg" style="height: 104px;" type="image/svg+xml"&gt;\[\begin{align*}
(n+1)^j &amp;amp;= \sum_{i=0}^{j} \binom{j}{i}n^{j-i} \cdot 1^{i} \\
        &amp;amp;= n^j + \binom{j}{1}n^{j-1}+\binom{j}{2}n^{j-2}+ \cdots
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9160683e6cffa23bd374bf3005f43e65dbae8cce.svg" style="height: 43px;" type="image/svg+xml"&gt;\[(n+1)^j - n^j = \binom{j}{1}n^{j-1}+\binom{j}{2}n^{j-2}+ \cdots\]&lt;/object&gt;
&lt;p&gt;Now if we look at the sum we got for &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e2aebbedea1d397bc0f3ea7fe157e95855cabc52.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(n)&lt;/object&gt; again, we'll notice
that in each term, the &lt;em&gt;j&lt;/em&gt;-th power of &lt;em&gt;n&lt;/em&gt; gets canceled out. This means that
the highest power of &lt;em&gt;n&lt;/em&gt; in &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e2aebbedea1d397bc0f3ea7fe157e95855cabc52.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(n)&lt;/object&gt; is going to be &lt;em&gt;k-1&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can similarly show that in &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4587a8f8a107fe7ff03102fbe212d11dfb602119.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(n)&lt;/object&gt;, the highest power
of &lt;em&gt;n&lt;/em&gt; is going to be &lt;em&gt;k-2&lt;/em&gt;. Therefore, the &lt;em&gt;k&lt;/em&gt;-th difference
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/20c595809e47fb8b48d6bc1cc331009539b6aa11.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^k f(n)&lt;/object&gt; will be constant &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Two observations for extra credit:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Note that the claim goes one way - &lt;em&gt;if&lt;/em&gt; &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; is a &lt;em&gt;k&lt;/em&gt;-th degree
polynomial, the &lt;em&gt;k&lt;/em&gt;-th difference is constant. What we observe in the table
is the &lt;em&gt;k&lt;/em&gt;-th difference is constant, so can we infer that the function is
a &lt;em&gt;k&lt;/em&gt;-th degree polynomial? Not in the general case! The sequence could be
generated by some higher-degree polynomial, or by a completely different
kind of function. That said, since we assume &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; is polynomial and
are seeking to find the simplest (lowest degree) one, this inference is
valid.&lt;/li&gt;
&lt;li&gt;Did you notice the equivalence to derivatives of polynomials? The &lt;em&gt;k&lt;/em&gt;-th
difference of a polynomial of degree &lt;em&gt;k&lt;/em&gt; is constant... but the same is true
for the &lt;em&gt;k&lt;/em&gt;-th derivative! This is not by chance - since we have a discrete
domain, differences play largely the same role as derivatives for continuous
functions. This isn't a rigorous proof - but think about the definition of
derivatives (the one with the limit) - what do you get when you take
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6d56447973863053dfb94416852d0392187be5b6.svg" style="height: 13px;" type="image/svg+xml"&gt;\Delta x&lt;/object&gt; (also sometimes called &lt;em&gt;h&lt;/em&gt;) and set it to 1?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="finding-the-coefficients-with-the-newton-polynomial"&gt;
&lt;h2&gt;Finding the coefficients with the Newton polynomial&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Polynomial_interpolation"&gt;Polynomial interpolation&lt;/a&gt; can fit any N points
(with distinct &lt;em&gt;x&lt;/em&gt; values) with a N-1 degree polynomial. One way of finding such
a polynomial was discovered by Isaac Newton and is called the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Newton_polynomial"&gt;Newton polynomial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our problem of finding a polynomial that generates a given set of points can
be reduced to this interpolation problem, since we've just figured out the
degree of the generating polynomial! Looking at the difference table, we've
found when the difference becomes constant, and that gives us the polynomial's
degree &lt;em&gt;k&lt;/em&gt;. So all we need is the first &lt;em&gt;k+1&lt;/em&gt; points.&lt;/p&gt;
&lt;p&gt;Here's how to develop the Newton polynomial from scratch; we'll start with the
first few coefficients and will then generalize for any &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The Newton polynomial for our set of forward differences can be expressed as
follows:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/8986e72a8615d60d63a271921b27ac86f2e8cbd7.svg" style="height: 19px;" type="image/svg+xml"&gt;\[f(n) = b_0 + b_1 n + b_2 n (n-1) + \cdots + b_k n(n-1)(n-2)\cdots (n-k+1)\]&lt;/object&gt;
&lt;p&gt;This polynomial is constructed in a clever way; notice that for any &lt;em&gt;p&lt;/em&gt;, when
we calculate &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/bcc84476cc6a5841b58dda8e57994f3c1f5e225e.svg" style="height: 19px;" type="image/svg+xml"&gt;f(p)&lt;/object&gt; all the elements starting with &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/5e9438c006a97c292f46c16b8f25dde4990d3280.svg" style="height: 18px;" type="image/svg+xml"&gt;b_{p+1}&lt;/object&gt; will be
multiplied by zero and vanish. This helps us determine this polynomial's
coefficients in a gradual manner.&lt;/p&gt;
&lt;p&gt;We'll start with the point &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/96703e3d12513bb33306250114aadebf96461433.svg" style="height: 19px;" type="image/svg+xml"&gt;(0, f(0))&lt;/object&gt;, substituting it into the Newton
polynomial:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/7b9efff8279fe3ddb8aa308a1eedfb728080d9f2.svg" style="height: 19px;" type="image/svg+xml"&gt;\[f(0) = b_0\]&lt;/object&gt;
&lt;p&gt;This gives us the first coefficient &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/2a38c2c0c99a2e9da009fc149ea917276bbbd847.svg" style="height: 15px;" type="image/svg+xml"&gt;b_0&lt;/object&gt;. Next, let's look at
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/b303eb9d4aa35ebf9200e8672e6a87f1d2132c35.svg" style="height: 19px;" type="image/svg+xml"&gt;(1, f(1))&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/788b381cf3eac4de29eac423286ce810063e80de.svg" style="height: 19px;" type="image/svg+xml"&gt;\[f(1) = b_0 + b_1 \cdot 1 = b_0 + b_1\]&lt;/object&gt;
&lt;p&gt;Since we know that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6799ba33e6a6f88f516dbec5dac8da3208c87712.svg" style="height: 19px;" type="image/svg+xml"&gt;b_0=f(0)&lt;/object&gt;, we can infer that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ecffab1104bbfa1700790488d1592833fd3fc123.svg" style="height: 19px;" type="image/svg+xml"&gt;b_1=f(1)-f(0)&lt;/object&gt;.
Another way to express that is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4772e72ac67c63e4f75cde07fd0185c2581657d0.svg" style="height: 19px;" type="image/svg+xml"&gt;b_1=\Delta f(0)&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;Continuing to &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/45af72369888dfa1a11fdc7346a5cc2d32001013.svg" style="height: 19px;" type="image/svg+xml"&gt;(2, f(2))&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/765c6057d2eacc86cc48329a5c11ba27d2899b65.svg" style="height: 72px;" type="image/svg+xml"&gt;\[\begin{align*}
 f(2) &amp;amp;= b_0 + b_1 \cdot 2 + b_2 \cdot 2 \cdot 1 \\
      &amp;amp;= b_0 + 2 b_1 + 2! b_2 \\
      &amp;amp;= f(0) + 2 \Delta f(0) + 2! b_2
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We've substituted the values of &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/2a38c2c0c99a2e9da009fc149ea917276bbbd847.svg" style="height: 15px;" type="image/svg+xml"&gt;b_0&lt;/object&gt; and &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/c7cd24d955e66b8fe5ce45ded69fd98da5c68ba8.svg" style="height: 17px;" type="image/svg+xml"&gt;b_1&lt;/object&gt; that we've found
earlier; let's solve for &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/5c438c10871f01d09b0425b8ddd878e495bf7ff8.svg" style="height: 15px;" type="image/svg+xml"&gt;b_2&lt;/object&gt; now:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e4a3173cf23501f41b1900321533ef57e4cce580.svg" style="height: 170px;" type="image/svg+xml"&gt;\[\begin{align*}
 b_2 &amp;amp;= \frac{f(2) - f(0) - 2 \Delta f(0)}{2!} \\
     &amp;amp;= \frac{f(2) - f(1) + f(1) - f(0) - 2 \Delta f(1)}{2!} \\
     &amp;amp;= \frac{\Delta f(1) + \Delta f(0) - 2 \Delta f(0)}{2!} \\
     &amp;amp;= \frac{\Delta f(1) - \Delta f(0)}{2!}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;The last line's numerator is - by definition - &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/673fc969775d75c8845c11bf81e042c20012dedf.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(0)&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d388e0413c1a96596556e8c123aed73c45dd2e8d.svg" style="height: 39px;" type="image/svg+xml"&gt;\[b_2 = \frac{\Delta^2 f(0)}{2!}\]&lt;/object&gt;
&lt;p&gt;We can keep going with this (feel free to do &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/1d56380c7fc945b51537de66595a5cbbbadd1d70.svg" style="height: 19px;" type="image/svg+xml"&gt;(3, f(3))&lt;/object&gt; as an exercise),
but the emerging generalization is that:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9e565e1e9470e65eb5df737dfd57248b82e6b22d.svg" style="height: 39px;" type="image/svg+xml"&gt;\[b_i = \frac{\Delta^i f(0)}{i!}\]&lt;/object&gt;
&lt;p&gt;And a concise way to write Newton's polynomial for forward differences is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/3136178faae047dc301b8c8e1f3f3bad4e43f1f9.svg" style="height: 53px;" type="image/svg+xml"&gt;\[f(n) = f(0) + \sum_{i=1}^{k} \frac{\Delta^i f(0)}{i!}g_i(n)\]&lt;/object&gt;
&lt;p&gt;Where &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/cfe177e3e161876e13bc7171b7f0314fdc73a651.svg" style="height: 19px;" type="image/svg+xml"&gt;g_i(n)&lt;/object&gt; is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/48bb7fa934e5b366190e384edb51b60b29693aaf.svg" style="height: 55px;" type="image/svg+xml"&gt;\[\prod_{j=0}^{i-1} (n-j)\]&lt;/object&gt;
&lt;p&gt;Note that we only use the differences for &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/233b23c15ba8894ccefa4845ae4b1954cc2c0ad7.svg" style="height: 19px;" type="image/svg+xml"&gt;f(0)&lt;/object&gt;, meaning that we need
just the first row of the difference table! Let's try it for the pentagonal
numbers example.&lt;/p&gt;
&lt;p&gt;First, we've determined that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4587a8f8a107fe7ff03102fbe212d11dfb602119.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(n)&lt;/object&gt; is a constant, so the degree
of the polynomial is 2. We only need to calculate until &lt;em&gt;i=2&lt;/em&gt; in the sum:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/79edb417f3a560fdb280f2c76f68aa0d32093210.svg" style="height: 39px;" type="image/svg+xml"&gt;\[f(n)=f(0)+\frac{\Delta f(0)}{1!} n + \frac{\Delta^2 f(0)}{2!} n(n-1)\]&lt;/object&gt;
&lt;p&gt;Substituting the values from the difference table we have for &lt;em&gt;n=0&lt;/em&gt;, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/25a6b43d36edb1d8a473a661e93eff0e74ce5370.svg" style="height: 127px;" type="image/svg+xml"&gt;\[\begin{align*}
 f(n)&amp;amp;=0+n+\frac{3}{2}n(n-1) \\
     &amp;amp;=n + \frac{3n^2-3n}{2} \\
     &amp;amp;=\frac{3 n^2}{2} -\frac{n}{2}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Which is exactly what we expected!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="another-example"&gt;
&lt;h2&gt;Another example&lt;/h2&gt;
&lt;p&gt;Let's work through another example, taking the sequence -8, -12, -6, 16, 60...
We'll start by constructing the difference table:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c2924d0eb862dbb9fd08ff306c53d4429266ff78.svg" style="height: 126px;" type="image/svg+xml"&gt;\[\begin{matrix}
n &amp;amp; f(n) &amp;amp; \Delta f(n) &amp;amp; \Delta ^2 f(n)  &amp;amp;  \Delta ^3 f(n) \\
0 &amp;amp; -8 &amp;amp; -4 &amp;amp; 10 &amp;amp; 6 \\
1 &amp;amp; -12 &amp;amp; 6 &amp;amp; 16 &amp;amp; 6 \\
2 &amp;amp; -6 &amp;amp; 22 &amp;amp; 22 &amp;amp; \\
3 &amp;amp; 16 &amp;amp; 44 &amp;amp; &amp;amp; \\
4 &amp;amp; 60 &amp;amp;  &amp;amp;  &amp;amp;  \\
\end{matrix}\]&lt;/object&gt;
&lt;p&gt;The difference &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a59fa130d9d0319438e151bf14b2f6536d5db631.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^3 f(n)&lt;/object&gt; appears to be constant, so we can generate
this sequence with a degree 3 polynomial. Let's use the first line of the table
to construct the Newton polynomial for it.&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/39486c9596bdb41b9d4fbf24ab3c66674f187fd9.svg" style="height: 135px;" type="image/svg+xml"&gt;\[\begin{align*}
 f(n)&amp;amp;=f(0)+\frac{\Delta f(0)}{1!} n + \frac{\Delta^2 f(0)}{2!} n(n-1) + \frac{\Delta^3 f(0)}{3!} n(n-1)(n-2)\\
     &amp;amp;= -8 + \frac{-4}{1}n + \frac{10}{2}n(n-1) + \frac{6}{6}n(n-1)(n-2) \\
     &amp;amp;= -8-4n+5n^2-5n+n^3-3n^2+2n \\
     &amp;amp;= n^3+2n^2-7n-8
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Verifying that this polynomial generates our sequence as its first 5 elements
is an easy exercise.&lt;/p&gt;
&lt;p&gt;Note that given 5 elements, we can always find a 4th-degree polynomial fitting
it. Here we found a 3rd-degree one, though, leveraging the technique of
differences. This becomes more acute if we have more elements in the sequence -
using differences it's often possible to find significantly simpler polynomials.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="recap"&gt;
&lt;h2&gt;Recap&lt;/h2&gt;
&lt;p&gt;For an integer sequence, if this sequence is generated by a polynomial we can
figure out which polynomial it is - given enough elements. We start by
constructing a difference table and noticing if a column becomes constant from
some point on. This tells us the degree of the generating polynomial. With that
in hand, we can use the Newton polynomial to discover a polynomial that
generates the sequence.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="resources"&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://ztoz.blog/posts/method-differences/"&gt;This blog post&lt;/a&gt; was the
original inspiration. The post explains how
Babbage's difference engine worked, and makes off-hand remarks like &amp;quot;for
a 2nd degree polynomial, we only need up to the second difference to know
everything&amp;quot;. &lt;a class="reference external" href="https://ztoz.blog/posts/differences-applications/"&gt;This follow-up by the same author&lt;/a&gt;
mentioned going from sequences back to polynomials.&lt;/li&gt;
&lt;li&gt;The old &lt;a class="reference external" href="https://oeis.org/EIStext.pdf"&gt;Encyclopedia of integer sequences&lt;/a&gt;
has an intriguing section 2.5, which unfortunately presents several lemmas
with no proofs.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://brilliant.org/wiki/method-of-differences/"&gt;This wiki page from brilliant.org&lt;/a&gt;
is the best single resource, though its proof of constructing the Newton
polynomial is lacking, IMHO.&lt;/li&gt;
&lt;li&gt;Wikipedia:
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Divided_differences"&gt;Divided differences&lt;/a&gt;
and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Newton_polynomial"&gt;Newton polynomial&lt;/a&gt;
pages.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.youtube.com/watch?v=xd7V0OKkEEg"&gt;Newton's forward differences YouTube lecture&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Knuth covers this &lt;em&gt;very briefly&lt;/em&gt; in section 4.6.4 of Part 2 of TAOCP,
relegating the derivation of the Newton polynomial to an exercise that has
a terse solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="appendix-another-way-to-find-the-polynomial-s-coefficients"&gt;
&lt;h2&gt;Appendix: Another way to find the polynomial's coefficients&lt;/h2&gt;
&lt;p&gt;Here's another way to discover the coefficients of the polynomial; the following
discusses the coefficients of the highest power, but can be generalized to
other powers as well. Using Newton's polynomial is simpler, though, so this
is just an appendix for some extra practice in manipulating such polynomials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Claim:&lt;/strong&gt; if &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; has degree &lt;em&gt;k&lt;/em&gt;, its coefficient &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b93e6f239fad8d0444d74634490a6e0e067b8954.svg" style="height: 11px;" type="image/svg+xml"&gt;a_k&lt;/object&gt;
is equal to &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/1f8e42ade75eff67a62076b8124b881e652cc799.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\Delta^k f(n)}{k!}&lt;/object&gt;. Since we've proven that
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/20c595809e47fb8b48d6bc1cc331009539b6aa11.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^k f(n)&lt;/object&gt; is a constant, we can find the precise
value of &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/b93e6f239fad8d0444d74634490a6e0e067b8954.svg" style="height: 11px;" type="image/svg+xml"&gt;a_k&lt;/object&gt; this way.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let's go back to the sum formulation of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e2aebbedea1d397bc0f3ea7fe157e95855cabc52.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(n)&lt;/object&gt; from
the previous proof.&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/17a09986b22f77c6241ff30054322e6cce33da9b.svg" style="height: 56px;" type="image/svg+xml"&gt;\[\sum_{j=0}^{k} a_j(n+1)^j-a_j n^j\]&lt;/object&gt;
&lt;p&gt;Expanding the binomial, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1f0d1ec102ab7eab83dff4cd8478847ce6ddd450.svg" style="height: 56px;" type="image/svg+xml"&gt;\[\sum_{j=0}^{k} a_j \left [ \binom{j}{1}n^{j-1}+\binom{j}{2}n^{j-2}+ \cdots \right ]\]&lt;/object&gt;
&lt;p&gt;The highest power of &lt;em&gt;n&lt;/em&gt; here is &lt;em&gt;k-1&lt;/em&gt;; its coefficient comes only from the
first term of the binomial expansion for &lt;em&gt;j=k&lt;/em&gt;, and is equal to &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/e192c5d30114375941e53a39e20ffb7ea5d3c971.svg" style="height: 15px;" type="image/svg+xml"&gt;k a_k&lt;/object&gt;.
In order not to deal with long sums, let's just focus on the highest-degree
term in &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e2aebbedea1d397bc0f3ea7fe157e95855cabc52.svg" style="height: 19px;" type="image/svg+xml"&gt;\Delta f(n)&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/4890eedc2566f89862ec1917a6fdc4cacbc0ceab.svg" style="height: 22px;" type="image/svg+xml"&gt;\[\Delta f(n) = k a_k n^{k-1} + \gamma\]&lt;/object&gt;
&lt;p&gt;Where &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/67833ee2012ec1c6254b6c009dc72bf0dc48aa6d.svg" style="height: 12px;" type="image/svg+xml"&gt;\gamma&lt;/object&gt; represents other elements with lower powers of &lt;em&gt;k&lt;/em&gt;, so we
don't care about them for this discussion &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;. Let's move on to the next
difference:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/c8fed37ba44785e2230649048abdc723c1f8cb3e.svg" style="height: 178px;" type="image/svg+xml"&gt;\[\begin{align*}
\Delta^2 f(n) &amp;amp;= \Delta f(n+1) - \Delta f(n) \\
 &amp;amp;= k a_k (n+1)^{k-1} + \gamma_1 - k a_k n^{k-1} + \gamma_2 \\
 &amp;amp;= k a_k \left [ n^{k-1} + \binom{k-1}{1} n^{k-2} + \cdots \right ] - k a_k n^{k-1} + \gamma \\
 &amp;amp;= k a_k \left [ \binom{k-1}{1} n^{k-2} + \cdots \right ] + \gamma \\
 &amp;amp;= k(k-1)a_k n^{k-2} + \gamma
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We've just found the coefficient of the highest power of &lt;em&gt;n&lt;/em&gt; in
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/4587a8f8a107fe7ff03102fbe212d11dfb602119.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^2 f(n)&lt;/object&gt;.
It's clear that if we continue doing this, &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/20c595809e47fb8b48d6bc1cc331009539b6aa11.svg" style="height: 20px;" type="image/svg+xml"&gt;\Delta^k f(n)&lt;/object&gt; will be:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1e260e31360f67ec73b700fd95039aea2fae707b.svg" style="height: 22px;" type="image/svg+xml"&gt;\[\Delta^k f(n)= a_k k(k-1)(k-2)\cdots 1 = a_k k!\]&lt;/object&gt;
&lt;p&gt;In other words, &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/750c7571c53a8b53e71d1e645f7ae98060742ecc.svg" style="height: 26px;" type="image/svg+xml"&gt;a_k = \frac{\Delta^k f(n)}{k!}&lt;/object&gt;, meaning that we can know
the coefficient of the highest power of &lt;em&gt;n&lt;/em&gt; in &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a53ffd447bcf898b6cc05d3a4f5cf05db89f6f08.svg" style="height: 19px;" type="image/svg+xml"&gt;f(n)&lt;/object&gt; from the &lt;em&gt;k&lt;/em&gt;-th
difference &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/4a4e9e431da45a27bc880a8a1ca44d8b1b9bc143.svg" style="height: 12px;" type="image/svg+xml"&gt;\blacksquare&lt;/object&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Naturally, there's an infinitude of potential functions that generate
this sequence; it's more precise to say we're looking for the simplest
(lowest degree)
polynomial that would do this.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;In the following calculation, we're playing loose with the
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/67833ee2012ec1c6254b6c009dc72bf0dc48aa6d.svg" style="height: 12px;" type="image/svg+xml"&gt;\gamma&lt;/object&gt;s to represent &amp;quot;anything with lower powers of &lt;em&gt;n&lt;/em&gt;
that we don't care about&amp;quot;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category></entry><entry><title>Cubic spline interpolation</title><link href="https://eli.thegreenplace.net/2023/cubic-spline-interpolation/" rel="alternate"></link><published>2023-10-12T05:57:00-07:00</published><updated>2024-09-14T13:15:30-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-10-12:/2023/cubic-spline-interpolation/</id><summary type="html">&lt;p&gt;This post explains how cubic spline interpolation works, and presents a full
implementation in JavaScript, hooked up to a SVG-based visualization.
As a side effect, it also covers Gaussian elimination and presents a JavaScript
implementation of that as well.&lt;/p&gt;
&lt;p&gt;I love topics that mix math and programming in a meaningful â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post explains how cubic spline interpolation works, and presents a full
implementation in JavaScript, hooked up to a SVG-based visualization.
As a side effect, it also covers Gaussian elimination and presents a JavaScript
implementation of that as well.&lt;/p&gt;
&lt;p&gt;I love topics that mix math and programming in a meaningful way, and cubic
spline interpolation is an excellent example of such a topic. There's a bunch
of linear algebra here and some calculus, all connected with code to create
a useful tool.&lt;/p&gt;
&lt;div class="section" id="motivation"&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;In an &lt;em&gt;interpolation&lt;/em&gt; problem, we're given a set of points (we'll be using
2D points &lt;em&gt;X,Y&lt;/em&gt; throughout this post) and are asked to estimate Y values for
Xs not in this original set, specifically for Xs that lie between Xs of the
original set (estimation for Xs outside the bounds of the original set
is called &lt;em&gt;extrapolation&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;As a concrete example, consider the set of points (0, 1), (1, 3), (2, 2); here
they are plotted in the usual coordinate system:&lt;/p&gt;
&lt;img alt="Three points on a 2D plot" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-3points.png" /&gt;
&lt;p&gt;Interpolation is estimating the value of Y for Xs between 0 and 2, given just
this data set. Obviously, the more complex the underlying function/phenomenon,
and the fewer original points we're given, interpolation becomes more difficult
to do accurately.&lt;/p&gt;
&lt;p&gt;There are many techniques to interpolate between a given set of points.
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Polynomial_interpolation"&gt;Polynomial interpolation&lt;/a&gt; can perfectly fit N
points with an N-1 degree polynomial, but this approach can be problematic for
large a N; high-degree polynomials tend to overfit their data, and suffer from
other numerical issues like &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Runge's_phenomenon"&gt;Runge's phenomenon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Instead of interpolating all the points with a single function, a very popular
alternative is using &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Spline_(mathematics)"&gt;Splines&lt;/a&gt;, which are piece-wise
polynomials. The idea is to fit a low-degree polynomial between every pair of
adjacent points in the original data set; for N points, we get N-1 different
polynomials. The simplest and best known variant of this technique is linear
interpolation:&lt;/p&gt;
&lt;img alt="Three points on a 2D plot with linear interpolation connecting them" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-linear.png" /&gt;
&lt;p&gt;Linear interpolation has clear benefits: it's very fast, and when N is large
it produces reasonable results. However, for small Ns the result isn't great,
and the approximation is very crude. Here's the linear spline interpolation of
the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sinc_function"&gt;Sinc function&lt;/a&gt; sampled
at 7 points:&lt;/p&gt;
&lt;img alt="Sinc function with linear interpolation" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-sinc-linear.png" /&gt;
&lt;p&gt;We can certainly do much better.&lt;/p&gt;
&lt;p&gt;How about higher-degree splines? We can try second degree polynomials, but it's
better to jump straight to cubic (third degree). Here's why: to make our
interpolation realistic and aesthetically pleasing, we want the neighboring
polynomials not only to touch at the original points (the linear splines already
do this), but to actually look like they're part of the same curve. For this
purpose, we want the &lt;em&gt;slope&lt;/em&gt; of the polynomials to be continuous, meaning that
if two polynomials meet at point P, their first derivatives at this point are
equal. Moreover, to ensure smoothness and to minimize needless bending &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;, we
also want the second derivatives of the two polynomials to be equal at P. The
lowest degree of polynomial that gives us this level of control is 3 (since the
second derivative of a quadratic polynomial is constant); hence cubic splines.&lt;/p&gt;
&lt;p&gt;Here's a cubic spline interpolating between the three points of the original
example:&lt;/p&gt;
&lt;img alt="Three points on a 2D plot with cubic spline interpolation connecting them" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-cubic.png" /&gt;
&lt;p&gt;And the &lt;em&gt;Sinc&lt;/em&gt; function:&lt;/p&gt;
&lt;img alt="Sinc function with cubic spline interpolation connecting them" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-sinc-cubic.png" /&gt;
&lt;p&gt;Because of the continuity of first and second derivatives, cubic splines look
very natural; on the other hand, since the degree of each polynomial remains
at most 3, they don't overfit too much. Hence they're such a popular tool for
interpolation and design/graphics.&lt;/p&gt;
&lt;p&gt;All the plots in this post have been produced by &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/js-gauss-spline"&gt;JavaScript code&lt;/a&gt;
that implements cubic spline interpolation from scratch. Let's move on to learn
how it works.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="setting-up-equations-for-cubic-spline-interpolation"&gt;
&lt;h2&gt;Setting up equations for cubic spline interpolation&lt;/h2&gt;
&lt;p&gt;Given a set of N points, we want to produce N-1 cubic polynomials between these
points. While these are distinct polynomials, they are connected through mutual
constraints on the original points, as we'll see soon.&lt;/p&gt;
&lt;p&gt;More formally, we're going to define N-1 polynomials in the inclusive range
&lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/9b9b2d7bb5787b4e4c6c897fdedd72630c5bb16a.svg" style="height: 19px;" type="image/svg+xml"&gt;i \in\{0 ...N-2\}&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2ac0b93cd5715f7041d230d1d057cd4797bdb458.svg" style="height: 22px;" type="image/svg+xml"&gt;\[p_i(x)=a_ix^3+b_ix^2+c_ix+d_i\]&lt;/object&gt;
&lt;p&gt;For each polynomial, we have to find 4 coefficients: &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, &lt;em&gt;c&lt;/em&gt; and &lt;em&gt;d&lt;/em&gt;;
in total, for N-1 polynomials we'll need 4N-4 coefficients. We're going to
find these coefficients by expressing the constraints we have as linear
equations, and then solving a system of linear equations. We'll need 4N-4
equations to ensure we can find a unique solution for 4N-4 unknowns.&lt;/p&gt;
&lt;p&gt;Let's use our sample set of three original points to demonstrate how this
calculation works: (0, 1), (1, 3), (2, 2). Since N is 3, we'll be looking for
two polynomials and a total of 8 coefficients.&lt;/p&gt;
&lt;p&gt;The first set of constraints is obvious - each polynomial has to pass through
the two points it's interpolating between. The first polynomial passes through
the points (0, 1) and (1, 3), so we can write the equations:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/00090cc844b4a8b06c6ed084b0a814a86c13c148.svg" style="height: 46px;" type="image/svg+xml"&gt;\[\begin{align*}
p_0(0)&amp;amp;=0a_0 + 0b_0 + 0c_0 + d_0=1\\
p_0(1)&amp;amp;=a_0+b_0+c_0+d_0=3
 \end{align*}\]&lt;/object&gt;
&lt;p&gt;The second polynomial passes through the points (1, 3) and (2, 2), resulting
in the equations:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/20edf7b504d79094c9fdcabe836664bf10b42fc6.svg" style="height: 46px;" type="image/svg+xml"&gt;\[\begin{align*}
p_1(1)&amp;amp;=a_1+b_1+c_1+d_1=3\\
p_1(2)&amp;amp;=8a_1 + 4b_1 + 2c_1 + d_1=2
 \end{align*}\]&lt;/object&gt;
&lt;p&gt;We have 4 equations, and need 4 more.&lt;/p&gt;
&lt;p&gt;We constrain the first and second derivatives of the polynomials to be equal at
the points where they meet. In our example, there are only two polynomials that
meet at a single point, so we'll get two equations: their derivatives are equal
at point (1, 3).&lt;/p&gt;
&lt;p&gt;Recall that the first and second derivatives of a cubic polynomial are:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d26d85730cd1985330ab76d27cb48bb03a9278ae.svg" style="height: 48px;" type="image/svg+xml"&gt;\[\begin{align*}
p_i&amp;#x27;(x)&amp;amp;=3a_ix^2+2b_ix+c_i\\
p_i&amp;#x27;&amp;#x27;(x)&amp;amp;=6a_ix+2b_i
 \end{align*}\]&lt;/object&gt;
&lt;p&gt;The equation we get from equating the first derivatives is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/49c0538e2adf8ce5aae6a0e1fcfb48e9d8ccb946.svg" style="height: 21px;" type="image/svg+xml"&gt;\[p_0&amp;#x27;(1)=3a_0+2b_0+c_0=p_1&amp;#x27;(1)=3a_1+2b_1+c_1\]&lt;/object&gt;
&lt;p&gt;Or, expressed as a linear equation of all coefficients:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/d6d310b5eebfe69c6b3adc5f8fb5c33758077afe.svg" style="height: 14px;" type="image/svg+xml"&gt;\[3a_0+2b_0+c_0-3a_1-2b_1-c_1=0\]&lt;/object&gt;
&lt;p&gt;Similarly, the equation we get from equating the second derivatives is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a7803f84d767481ca8cc77b667b511592722d149.svg" style="height: 21px;" type="image/svg+xml"&gt;\[p_0&amp;#x27;&amp;#x27;(1)=6a_0+2=p_1&amp;#x27;&amp;#x27;(1)=6a_1+2\]&lt;/object&gt;
&lt;p&gt;Expressed as a linear equation of all coefficients:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1adf5f1b5b352afa02fab154f8f14488c580cfb9.svg" style="height: 14px;" type="image/svg+xml"&gt;\[6a_0+2-6a_1-2=0\]&lt;/object&gt;
&lt;p&gt;This brings us to a total of 6 equations. The last two equations will come from
&lt;em&gt;boundary conditions&lt;/em&gt;. Notice that - so far - we didn't say much about how our
interpolating polynomials behave at the end points, except that they pass
through them. Boundary conditions are constraints we create to define how our
polynomials behave at these end points.
There are several approaches to this,
but here we'll just discuss the most commonly-used one: a &lt;em&gt;natural&lt;/em&gt; spline.
Mathematically it says that the first polynomial has a second derivative of 0
at the first original point, and the last polynomial has a second derivative of
0 at the last original point. In our example:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/378d9e0421bc85fe06ade8ad93ffd620e47274db.svg" style="height: 48px;" type="image/svg+xml"&gt;\[\begin{align*}
p_0&amp;#x27;&amp;#x27;(0)=0\\
p_1&amp;#x27;&amp;#x27;(2)=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Substituting the second derivative equations:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6cc066ad9122be577dacad8a36b8e7ae1ce387b3.svg" style="height: 48px;" type="image/svg+xml"&gt;\[\begin{align*}
p_0&amp;#x27;&amp;#x27;(0)&amp;amp;=2b_0=0\\
p_1&amp;#x27;&amp;#x27;(2)&amp;amp;=12a_1+2b_1=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We have 8 equations now:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a9200a1d69c62aa7c22ce23b2d29d3f4f891f22b.svg" style="height: 203px;" type="image/svg+xml"&gt;\[\begin{align*}
d_0&amp;amp;=1\\
a_0+b_0+c_0+d_0&amp;amp;=3\\
a_1+b_1+c_1+d_1&amp;amp;=3\\
8a_1 + 4b_1 + 2c_1 + d_1&amp;amp;=2\\
3a_0+2b_0+c_0-3a_1-2b_1-c_1&amp;amp;=0\\
6a_0+2-6a_1-2&amp;amp;=0\\
2b_0&amp;amp;=0\\
12a_1+2b_1&amp;amp;=0
\end{align*}\]&lt;/object&gt;
&lt;p&gt;To restate the obvious - while our example only uses 2 polynomials, this
approach generalizes to any number. For N original points, we'll interpolate
with N-1 polynomials, resulting in 4N-4 coefficients. We'll get:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;2N-2 equations from setting the points these polynomials pass through&lt;/li&gt;
&lt;li&gt;N-2 equations from equating first derivatives at internal points&lt;/li&gt;
&lt;li&gt;N-2 equations from equating second derivatives at internal points&lt;/li&gt;
&lt;li&gt;2 equations from boundary conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a total of 4N-4 equations.&lt;/p&gt;
&lt;p&gt;The code that constructs these equations from a given set of points is available
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2023/js-gauss-spline/spline.js"&gt;in this file&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="solving-the-equations"&gt;
&lt;h2&gt;Solving the equations&lt;/h2&gt;
&lt;p&gt;We now have 8 equations with 8 variables. Some of them are trivial, so it's
tempting to just solve the system by hand, and indeed one can do it very easily.
In the general case, however, it would be quite difficult - imagine
interpolating 10 polynomials resulting in 36 equations!&lt;/p&gt;
&lt;p&gt;Fortunately, the full power of linear algebra is now at our disposal. We can
express this set of linear equations as a matrix multiplication problem
&lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/e7d3683a610f89a991289fc2c2c64ba38eb6a004.svg" style="height: 13px;" type="image/svg+xml"&gt;Ax=b&lt;/object&gt;, where &lt;em&gt;A&lt;/em&gt; is a matrix of coefficients, &lt;em&gt;x&lt;/em&gt; is a vector of
unknowns and &lt;em&gt;b&lt;/em&gt; is the vector of right-hand side constants:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/5243a06db4c849ae927dff870e2db1f3ab2c217d.svg" style="height: 170px;" type="image/svg+xml"&gt;\[Ax=b\Rightarrow \begin{pmatrix}
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 8 &amp;amp; 4 &amp;amp; 2 &amp;amp; 1\\
3 &amp;amp; 2 &amp;amp; 1 &amp;amp; 0 &amp;amp; -3 &amp;amp; -2 &amp;amp; -1 &amp;amp; 0\\
6 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; -6 &amp;amp; -2 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 12 &amp;amp; 2 &amp;amp; 0 &amp;amp; 0\\
\end{pmatrix}\begin{pmatrix}
a_0 \\
b_0 \\
c_0 \\
d_0 \\
a_1 \\
b_1 \\
c_1 \\
d_1\end{pmatrix}=\begin{pmatrix}
1\\
3\\
3\\
2\\
0\\
0\\
0\\
0
\end{pmatrix}\]&lt;/object&gt;
&lt;p&gt;Solving this system is straightforward using &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination"&gt;Gaussian elimination&lt;/a&gt;.
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2023/js-gauss-spline/eqsolve.js"&gt;Our JavaScript implementation&lt;/a&gt;
does this in a few steps:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Performs Gaussian elimination to bring &lt;em&gt;A&lt;/em&gt; into row-echelon form, using the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_elimination#Pseudocode"&gt;algorithm outlined on Wikipedia&lt;/a&gt;. This
approach tries to preserve numerical stability by selecting the row with the
largest (in absolute value) value for each column &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Further transforms the resulting matrix into &lt;em&gt;reduced&lt;/em&gt; row-echelon form
(a.k.a. Gauss-Jordan elimination)&lt;/li&gt;
&lt;li&gt;Extracts the solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our example, the solution ends up being the vector (-0.75, 0, 2.75, 1, 0.75,
-4.5, 7.25, -0.5); therefore, the interpolating polynomials are:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/04c71afa0281eb99662ce3d99d4c4546ef08af89.svg" style="height: 50px;" type="image/svg+xml"&gt;\[\begin{align*}
p_0(x)&amp;amp;=-0.75x^3+2.75x+1\\
p_1(x)&amp;amp;=0.75x^3-4.5x^2+7.25x-0.5
\end{align*}\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="performing-the-interpolation-itself"&gt;
&lt;h2&gt;Performing the interpolation itself&lt;/h2&gt;
&lt;p&gt;Now that we have the interpolating polynomials, we can generate any number of
interpolated points. For all &lt;em&gt;x&lt;/em&gt; between 0 and 1 we use &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ad5cb52cf88277ad5a1880722c8ae8b3a6edfd42.svg" style="height: 19px;" type="image/svg+xml"&gt;p_0(x)&lt;/object&gt;,
and for &lt;em&gt;x&lt;/em&gt; between 1 and 2 we use &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/7c2338e3575da884f060665a36a3503d970957a5.svg" style="height: 19px;" type="image/svg+xml"&gt;p_1(x)&lt;/object&gt;. In our JavaScript
code this is done by the &lt;tt class="docutils literal"&gt;doInterpolate&lt;/tt&gt; function. We've already seen
the result:&lt;/p&gt;
&lt;img alt="Three points on a 2D plot with cubic spline interpolation connecting them" class="align-center" src="https://eli.thegreenplace.net/images/2023/interp-cubic.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The complete code sample for this post &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/js-gauss-spline"&gt;is available on GitHub&lt;/a&gt;.
It includes functions for constructing equations for cubic splines from an
original set of points, code for solving linear equations with Gauss-Jordan
elimination, and a demo HTML page that plots the points and linear/spline
interpolations.&lt;/p&gt;
&lt;p&gt;The code is readable, heavily-commented JavaScript with no dependencies (except
D3 for the plotting).&lt;/p&gt;
&lt;p&gt;An additional demo that uses similar functionality is &lt;a class="reference external" href="https://eliben.github.io/line-plotting/"&gt;line-plotting&lt;/a&gt;; it plots arbitrary mathematical
functions with optional interpolation (when the number of sampled points is
low).&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This requirement actually has neat historical roots. In the days before
computers, &amp;quot;splines&amp;quot; were elastic rulers engineers and drafters would
use to interpolate between points by hand. These rulers would bend and
connect at the original points, and it was considered best practice to
minimize bending.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This helps avoid division by very small numbers, which may cause issues
when using finite-precision floating point.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="JavaScript"></category></entry><entry><title>My favorite prime number generator</title><link href="https://eli.thegreenplace.net/2023/my-favorite-prime-number-generator/" rel="alternate"></link><published>2023-08-22T20:01:00-07:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-08-22:/2023/my-favorite-prime-number-generator/</id><summary type="html">&lt;p&gt;Many years ago I've re-posted a &lt;a class="reference external" href="https://stackoverflow.com/a/568618/"&gt;Stack Overflow answer&lt;/a&gt; with Python code for a terse prime sieve
function that generates a potentially infinite sequence of prime
numbers (&amp;quot;potentially&amp;quot; because it &lt;em&gt;will&lt;/em&gt; run out of memory eventually). Since
then, I've used this code &lt;em&gt;many&lt;/em&gt; times - mostly because it's short and clear â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Many years ago I've re-posted a &lt;a class="reference external" href="https://stackoverflow.com/a/568618/"&gt;Stack Overflow answer&lt;/a&gt; with Python code for a terse prime sieve
function that generates a potentially infinite sequence of prime
numbers (&amp;quot;potentially&amp;quot; because it &lt;em&gt;will&lt;/em&gt; run out of memory eventually). Since
then, I've used this code &lt;em&gt;many&lt;/em&gt; times - mostly because it's short and clear. In
this post I will explain how this code works, where it comes from (I didn't come
up with it), and some potential optimizations. If you want a teaser, here it is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generate an infinite sequence of prime numbers.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setdefault&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="the-sieve-of-eratosthenes"&gt;
&lt;h2&gt;The sieve of Eratosthenes&lt;/h2&gt;
&lt;p&gt;To understand what this code does, we should first start with the basic Sieve
of Eratosthenes; if you're familiar with it, feel free to skip this section.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes"&gt;Sieve of Eratosthenes&lt;/a&gt; is a well-known
algorithm from ancient Greek times for finding all the primes below a certain
number reasonably efficiently using a tabular representation. This animation
from Wikipedia explains it pretty well:&lt;/p&gt;
&lt;img alt="Animated GIF of the Sieve of Eratosthenes in action" class="align-center" src="https://eli.thegreenplace.net/images/2023/eratosthenes-animation-wikipedia.gif" /&gt;
&lt;p&gt;Starting with the first prime (2) it marks all its multiples until the requested
limit. It then takes the next unmarked number, assumes it's a prime (because it
is not a multiple of a smaller prime), and marks &lt;em&gt;its&lt;/em&gt; multiples, and so on
until all the multiples below the limit are marked. The remaining
unmarked numbers are primes.&lt;/p&gt;
&lt;p&gt;Here's a well-commented, basic Python implementation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes_upto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generates a sequence of primes &amp;lt; n.&lt;/span&gt;

&lt;span class="sd"&gt;    Uses the full sieve of Eratosthenes with O(n) memory.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="c1"&gt;# Initialize table; True means &amp;quot;prime&amp;quot;, initially assuming all numbers&lt;/span&gt;
    &lt;span class="c1"&gt;# are prime.&lt;/span&gt;
    &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="n"&gt;sqrtn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="c1"&gt;# Starting with 2, for each True (prime) number I in the table, mark all&lt;/span&gt;
    &lt;span class="c1"&gt;# its multiples as composite (starting with I*I, since earlier multiples&lt;/span&gt;
    &lt;span class="c1"&gt;# should have already been marked as multiples of smaller primes).&lt;/span&gt;
    &lt;span class="c1"&gt;# At the end of this process, the remaining True items in the table are&lt;/span&gt;
    &lt;span class="c1"&gt;# primes, and the False items are composites.&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sqrtn&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;

    &lt;span class="c1"&gt;# Yield all the primes in the table.&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When we want a list of all the primes below some known limit,
&lt;tt class="docutils literal"&gt;gen_primes_upto&lt;/tt&gt; is great, and performs fairly well. There are two issues
with it, though:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;We have to know what the limit is ahead of time; this isn't always possible
or convenient.&lt;/li&gt;
&lt;li&gt;Its memory usage is high - O(n); this can be significantly optimized,
however; see the bonus section at the end of the post for details.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="the-infinite-prime-generator"&gt;
&lt;h2&gt;The infinite prime generator&lt;/h2&gt;
&lt;p&gt;Back to the infinite prime generator that's in the focus of this post. Here is
its code again, now with some comments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generate an infinite sequence of prime numbers.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Maps composites to primes witnessing their compositeness.&lt;/span&gt;
    &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="c1"&gt;# The running integer that&amp;#39;s checked for primeness&lt;/span&gt;
    &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# q is a new prime.&lt;/span&gt;
            &lt;span class="c1"&gt;# Yield it and mark its first multiple that isn&amp;#39;t&lt;/span&gt;
            &lt;span class="c1"&gt;# already marked in previous iterations&lt;/span&gt;
            &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# q is composite. D[q] holds some of the primes that&lt;/span&gt;
            &lt;span class="c1"&gt;# divide it. Since we&amp;#39;ve reached q, we no longer&lt;/span&gt;
            &lt;span class="c1"&gt;# need it in the map, but we&amp;#39;ll mark the next&lt;/span&gt;
            &lt;span class="c1"&gt;# multiples of its witnesses to prepare for larger&lt;/span&gt;
            &lt;span class="c1"&gt;# numbers&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setdefault&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The key to the algorithm is the map &lt;tt class="docutils literal"&gt;D&lt;/tt&gt;. It holds all the primes encountered
so far, but not as keys! Rather, they are stored as values, with the keys being
the next composite number they divide. This lets the program avoid having to
divide each number it encounters by all the primes known so far - it can simply
look in the map. A number that's not in the map is a new prime, and the way
the map updates is not unlike the sieve of Eratosthenes - when a composite is
removed, we add the &lt;em&gt;next&lt;/em&gt; composite multiple of the same prime(s). This is
guaranteed to cover all the composite numbers, while prime numbers should never
be keys in &lt;tt class="docutils literal"&gt;D&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I highly recommend instrumenting this function with some printouts and running
through a sample invocation - it makes it easy to understand how the algorithm
makes progress.&lt;/p&gt;
&lt;p&gt;Compared to the full sieve &lt;tt class="docutils literal"&gt;gen_primes_upto&lt;/tt&gt;, this function doesn't require
us to know the limit ahead of time - it will keep producing prime numbers ad
infinitum (but will run out of memory eventually). As for memory usage, the
&lt;tt class="docutils literal"&gt;D&lt;/tt&gt; map has all the primes in it &lt;em&gt;somewhere&lt;/em&gt;, but each one appears only once.
So its size is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/d047b6d9c65037f42dcfda7db0732cf2163b8ee7.svg" style="height: 19px;" type="image/svg+xml"&gt;O(\pi(n))&lt;/object&gt;, where &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/ac6df731942da5bd58234248a7aa9bd85b9100bd.svg" style="height: 19px;" type="image/svg+xml"&gt;\pi(n)&lt;/object&gt; is the
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Prime-counting_function"&gt;Prime-counting function&lt;/a&gt;,
the number of primes smaller or equal to &lt;em&gt;n&lt;/em&gt;. This can be
approximated by &lt;object class="valign-m10" data="https://eli.thegreenplace.net/images/math/8ed6967b3dea41c3ce34ed6e0bd449b2adf5699a.svg" style="height: 24px;" type="image/svg+xml"&gt;O(\frac{n}{ln(n)})&lt;/object&gt; &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I don't remember where I first saw this approach mentioned, but all the
breadcrumbs lead to &lt;a class="reference external" href="https://code.activestate.com/recipes/117119-sieve-of-eratosthenes/"&gt;this ActiveState Recipe by David Eppstein&lt;/a&gt; from
way back in 2002.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizing-the-generator"&gt;
&lt;h2&gt;Optimizing the generator&lt;/h2&gt;
&lt;p&gt;I really like &lt;tt class="docutils literal"&gt;gen_primes&lt;/tt&gt;; it's short, easy to understand and gives me as
many primes as I need without forcing me to know what limit to use, and its
memory usage is much more reasonable than the full-blown sieve of Eratosthenes.
It is, however, also quite slow, over 5x slower than &lt;tt class="docutils literal"&gt;gen_primes_upto&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;The aforementioned ActiveState Recipe thread has several optimization ideas;
here's a version that incorporates ideas from Alex Martelli, Tim Hochberg and
Wolfgang Beneicke:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes_opt&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;itertools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;  &lt;span class="c1"&gt;# get odd multiples&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
            &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The optimizations are:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Instead of holding a list as the value of &lt;tt class="docutils literal"&gt;D&lt;/tt&gt;, just have a single number.
In cases where we need more than one witness to a composite, find the next
multiple of the witness and assign that instead (this is the &lt;tt class="docutils literal"&gt;while x in D&lt;/tt&gt;
inner loop in the &lt;tt class="docutils literal"&gt;else&lt;/tt&gt; clause). This is a bit like using linear probing
in a hash table instead of having a list per bucket.&lt;/li&gt;
&lt;li&gt;Skip even numbers by starting with 2 and then proceeding from 3 in steps
of 2.&lt;/li&gt;
&lt;li&gt;The loop assigning the next multiple of witnesses may land on even numbers
(when &lt;tt class="docutils literal"&gt;p&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;q&lt;/tt&gt; are both odd). So instead jump to &lt;tt class="docutils literal"&gt;q + p + p&lt;/tt&gt;
directly, which is guaranteed to be odd.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these in place, the function is more than 3x faster than before, and is
now only within 40% or so of &lt;tt class="docutils literal"&gt;gen_primes_upto&lt;/tt&gt;, while remaining short and
reasonably clear.&lt;/p&gt;
&lt;p&gt;There are even fancier algorithms that use interesting mathematical tricks to do
less work. Here's &lt;a class="reference external" href="https://stackoverflow.com/a/19391111/"&gt;an approach by Will Ness and Tim Peters&lt;/a&gt; (yes, &lt;em&gt;that&lt;/em&gt; Tim Peters) that's
reportedly faster. It uses the &lt;em&gt;wheels&lt;/em&gt; idea from &lt;a class="reference external" href="https://research.cs.wisc.edu/techreports/1990/TR909.pdf"&gt;this paper by Sorenson&lt;/a&gt;. Some additional
details on this approach are available &lt;a class="reference external" href="https://stackoverflow.com/a/30563958"&gt;here&lt;/a&gt;. This algorithm is both faster and
consumes less memory; on the other hand, it's no longer short and simple.&lt;/p&gt;
&lt;p&gt;To be honest, it always feels a bit odd to me to painfully optimize Python code,
when switching languages provides vastly bigger benefits. For example, I threw
together the same algorithms &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/prime-sieve/go-with-range-over-func"&gt;using Go&lt;/a&gt;
and its &lt;a class="reference external" href="https://github.com/golang/go/issues/61405"&gt;experimental iterator support&lt;/a&gt;; it's 3x faster than the
Python version, with very little effort (even though the new Go iterators and
&lt;tt class="docutils literal"&gt;yield&lt;/tt&gt; functions are still in the proposal stage and aren't optimized). I
can't try to rewrite it in C++ or Rust for now, due to the lack of generator
support; the &lt;tt class="docutils literal"&gt;yield&lt;/tt&gt; statement is what makes this code so nice and elegant,
and alternative idioms are much less convenient.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bonus-segmented-sieve-of-eratosthenes"&gt;
&lt;h2&gt;Bonus: segmented sieve of Eratosthenes&lt;/h2&gt;
&lt;p&gt;The Wikipedia article on the sieve of Eratosthenes mentions a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes#Segmented_sieve"&gt;segmented
approach&lt;/a&gt;, which
is also described in the &lt;a class="reference external" href="https://research.cs.wisc.edu/techreports/1990/TR909.pdf"&gt;Sorenson paper&lt;/a&gt; in section 5.&lt;/p&gt;
&lt;p&gt;The main insight is that we only need the primes up to &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/712f9a224d6c7824add37b6cd766c21f73a40d59.svg" style="height: 18px;" type="image/svg+xml"&gt;\sqrt{n}&lt;/object&gt; to
be able to sieve a table all the way to N. This results in a sieve that uses
only &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/5a41da22acdba46e7c8eeeaddbc58625f49cbfe5.svg" style="height: 19px;" type="image/svg+xml"&gt;O(\sqrt{n})&lt;/object&gt; memory. Here's a commented Python implementation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gen_primes_upto_segmented&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generates a sequence of primes &amp;lt; n.&lt;/span&gt;

&lt;span class="sd"&gt;    Uses the segmented sieve or Eratosthenes algorithm with O(âˆšn) memory.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Simplify boundary cases by hard-coding some small primes.&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="c1"&gt;# We break the range [0..n) into segments of size âˆšn&lt;/span&gt;
    &lt;span class="n"&gt;segsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="c1"&gt;# Find the primes in the first segment by calling the basic sieve on that&lt;/span&gt;
    &lt;span class="c1"&gt;# segment (its memory usage will be O(âˆšn)). We&amp;#39;ll use these primes to&lt;/span&gt;
    &lt;span class="c1"&gt;# sieve all subsequent segments.&lt;/span&gt;
    &lt;span class="n"&gt;baseprimes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gen_primes_upto&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;segsize&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;baseprimes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;segsize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;segsize&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Create a new table of size âˆšn for each segment; the old table&lt;/span&gt;
        &lt;span class="c1"&gt;# is thrown away, so the total memory use here is âˆšn&lt;/span&gt;
        &lt;span class="c1"&gt;# seg[i] represents the number segstart+i&lt;/span&gt;
        &lt;span class="n"&gt;seg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;segsize&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;baseprimes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# The first multiple of bp in this segment can be calculated using&lt;/span&gt;
            &lt;span class="c1"&gt;# modulo.&lt;/span&gt;
            &lt;span class="n"&gt;first_multiple&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# Mark all multiples of bp in the segment as composite.&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first_multiple&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;segsize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bp&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;

        &lt;span class="c1"&gt;# Sieving is done; yield all composites in the segment (iterating only&lt;/span&gt;
        &lt;span class="c1"&gt;# over the odd ones).&lt;/span&gt;
        &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;seg&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;break&lt;/span&gt;
                &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;segstart&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The full code for this post - along with tests and benchmarks - is available
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/prime-sieve"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;While this is a strong improvement over &lt;tt class="docutils literal"&gt;O(n)&lt;/tt&gt; (e.g. for a billion
primes, memory usage here is only 5% of the full sieve version), it
still depends on the size of the input. In the unlikely event that you
need to generate truly gigantic primes starting from 2, even the
square-root-space solutions become infeasible. In this case, the whole
approach should be changed; instead, one would just generate random huge
numbers and use probabilistic primality testing to check for their
primeness. This is what real libraries like Go's &lt;a class="reference external" href="https://pkg.go.dev/crypto/rand#Prime"&gt;crypto/rand.Prime&lt;/a&gt;
do.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="Python"></category><category term="Go"></category></entry><entry><title>Demystifying Tupper's formula</title><link href="https://eli.thegreenplace.net/2023/demystifying-tuppers-formula/" rel="alternate"></link><published>2023-05-22T19:45:00-07:00</published><updated>2024-09-14T13:15:30-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2023-05-22:/2023/demystifying-tuppers-formula/</id><summary type="html">&lt;p&gt;A &lt;a class="reference external" href="https://makeanddo4d.com/"&gt;book I was recently reading&lt;/a&gt; mentioned a
mathematical curiosity I haven't seen before - &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tupper%27s_self-referential_formula"&gt;Tupper's self-referential
formula&lt;/a&gt;.
There are some resources about it online, but this post is &lt;em&gt;my&lt;/em&gt; attempt to
explain how it works - along with an interactive implementation you can try
in the browser.&lt;/p&gt;
&lt;div class="section" id="tupper-s-formula"&gt;
&lt;h2&gt;Tupper's formula&lt;/h2&gt;
&lt;p&gt;Here is â€¦&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;A &lt;a class="reference external" href="https://makeanddo4d.com/"&gt;book I was recently reading&lt;/a&gt; mentioned a
mathematical curiosity I haven't seen before - &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tupper%27s_self-referential_formula"&gt;Tupper's self-referential
formula&lt;/a&gt;.
There are some resources about it online, but this post is &lt;em&gt;my&lt;/em&gt; attempt to
explain how it works - along with an interactive implementation you can try
in the browser.&lt;/p&gt;
&lt;div class="section" id="tupper-s-formula"&gt;
&lt;h2&gt;Tupper's formula&lt;/h2&gt;
&lt;p&gt;Here is the formula:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a4f48d2debe2ad234574a9cf2a0c2d1b327963c7.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{1}{2}&amp;lt; \left \lfloor mod\left ( \left \lfloor \frac{y}{17}\right \rfloor 2^{-17\lfloor x \rfloor - mod(\lfloor y \rfloor, 17)}, 2 \right ) \right \rfloor\]&lt;/object&gt;
&lt;p&gt;We want to plot this formula, but how?&lt;/p&gt;
&lt;p&gt;For this purpose, it's more useful to think of Tupper's formula not as a
function but as a &lt;em&gt;relation&lt;/em&gt;, in the mathematical sense. In Tupper's paper
this is a relation on &lt;img alt="\mathbb{R}" class="valign-0" src="https://eli.thegreenplace.net/images/math/0ed839b111fe0e3ca2b2f618b940893eaea88a57.png" style="height: 12px;" /&gt;, meaning that it's a set of pairs
in &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/6d731263787f024f927178eb8fc44f5e91a79bde.svg" style="height: 12px;" type="image/svg+xml"&gt;\mathbb{R} \times \mathbb{R}&lt;/object&gt; that satisfy the inequality.&lt;/p&gt;
&lt;p&gt;For our task we'll use discrete indices for &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;, so the relation is
on &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/536c886d7863df5a4e250a73547be5d968c290c7.svg" style="height: 12px;" type="image/svg+xml"&gt;\mathbb{N}&lt;/object&gt;. We'll plot the relation by using a dark pixel (or
square) for a &lt;tt class="docutils literal"&gt;x,y&lt;/tt&gt; coordinate where the inequality holds and a light pixel
for a coordinate where it doesn't hold.&lt;/p&gt;
&lt;p&gt;The &amp;quot;mind-blowing&amp;quot; fact about Tupper's formula is that when plotted for
a certain range of &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;, it produces this:&lt;/p&gt;
&lt;img alt="Tupper's formula own plot" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-plot.png" /&gt;
&lt;p&gt;Note that while &lt;em&gt;x&lt;/em&gt; runs in the inclusive range of 0-105 on the plot, &lt;em&gt;y&lt;/em&gt; starts
at a mysterious &lt;em&gt;K&lt;/em&gt; and ends at &lt;em&gt;K+16&lt;/em&gt;. For the plot above, &lt;em&gt;K&lt;/em&gt; needs to be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;4858450636189713423582095962494202044581400587983244549483
0930850619347047088099284506447698655243648499972470249151
1911041160573917740785691975432657185544205721044573588368
1829823754139634338225199452191651284348332905131193199953
5024137587652392648746133949068701305622958132194811136853
3953556529085002387509285689269455597428154638651073004910
6723058933586052544096664351265349363643957125565695936815
1843348576052669401612512669514215505395545191537854575257
5659074054015792900176596796548006442782913148854825991472
1248506352686630476300
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The amazement subsides slightly when we discover that for a different &lt;em&gt;K&lt;/em&gt; &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;,
we get a different plot:&lt;/p&gt;
&lt;img alt="Tupper's formula producing a pacman plot" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-pacman.png" /&gt;
&lt;p&gt;And, in fact, this formula can produce any 2D grid of 106x17 pixels, given the
right coordinates. Since the formula itself is so simple, it is quite apparent
that the value of &lt;em&gt;K&lt;/em&gt; is the key here; these are huge numbers with hundreds of
digits, so clearly they encode the image information somehow. Read on to see
how this actually works.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-javascript-demo"&gt;
&lt;h2&gt;A JavaScript demo&lt;/h2&gt;
&lt;p&gt;I've implemented a simple online demo of plotting the Tupper formula - available
at &lt;a class="reference external" href="https://eliben.github.io/tupperformula/"&gt;https://eliben.github.io/tupperformula/&lt;/a&gt; (with &lt;a class="reference external" href="https://github.com/eliben/tupperformula"&gt;source code on GitHub&lt;/a&gt;). It was used to produce the images
shown above. The code is fairly straightforward, so I'll just focus on the
interesting part.&lt;/p&gt;
&lt;p&gt;The core of the code is a 2D grid that's plotted for &lt;em&gt;x&lt;/em&gt; running from 0 to
105 and &lt;em&gt;y&lt;/em&gt; from &lt;em&gt;K&lt;/em&gt; to &lt;em&gt;K+16&lt;/em&gt; (both ranges inclusive). The grid is populated
every time the number changes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridWidth&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;106&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridHeight&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;17&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;K&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Knum&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridWidth&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridHeight&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;Grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;tupperFormula&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;K&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)));&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the use of JavaScript's &lt;tt class="docutils literal"&gt;BigInt&lt;/tt&gt; types here - very handy when dealing
with such huge numbers. Here is &lt;tt class="docutils literal"&gt;tupperFormula&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;tupperFormula&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It looks quite different from the mathematical formula at the top of this post;
why? Because - as mentioned before - while Tupper's original formula works on
real numbers, our program only needs the discrete integer range of
&lt;tt class="docutils literal"&gt;x in [0, 105]&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;y in [K, K+16]&lt;/tt&gt;. When we deal with discrete numbers,
the formula can be simplified greatly.&lt;/p&gt;
&lt;p&gt;Let's start with the original formula and simplify it step by step:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/a4f48d2debe2ad234574a9cf2a0c2d1b327963c7.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{1}{2}&amp;lt; \left \lfloor mod\left ( \left \lfloor \frac{y}{17}\right \rfloor 2^{-17\lfloor x \rfloor - mod(\lfloor y \rfloor, 17)}, 2 \right ) \right \rfloor\]&lt;/object&gt;
&lt;p&gt;First of all, since &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; are natural numbers, the floor operations on
them don't do anything, so we can drop them (including on the division by
17, if we just assume integer division that rounds down by default):&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f8e9f298cf8e514787979eaa4d7cc6b2b2489cb0.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{1}{2}&amp;lt; \left \lfloor mod\left ( \left ( \frac{y}{17}\right ) 2^{-17x - mod(y, 17)}, 2 \right ) \right \rfloor\]&lt;/object&gt;
&lt;p&gt;Next, since the result of the &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/6bfbbf950c2eba80fdd316385a8c430702ef839f.svg" style="height: 19px;" type="image/svg+xml"&gt;mod(N,2)&lt;/object&gt; operation for a natural &lt;em&gt;N&lt;/em&gt; is
either 0 or 1, the comparison to half is just a fancy way of saying &amp;quot;equals 1&amp;quot;;
we can replace the inequality by:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b0e581251daff5ef47433e7e7c50bfc94ad4051a.svg" style="height: 33px;" type="image/svg+xml"&gt;\[mod\left ( \left ( \frac{y}{17}\right ) 2^{-17x - mod(y, 17)}, 2 \right )=1\]&lt;/object&gt;
&lt;p&gt;Note the negative power of 2; multiplying by it is the same as dividing by its
positive counterpart. Another way to express division by &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/339f03051f685e4ffbec605928020a75cc9c05d1.svg" style="height: 12px;" type="image/svg+xml"&gt;2^p&lt;/object&gt; for natural
numbers is a bit shift right by &lt;em&gt;p&lt;/em&gt; bits. So we get the code of the
&lt;tt class="docutils literal"&gt;tupperFormula&lt;/tt&gt; function shown above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;d&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="how-the-tupper-formula-works"&gt;
&lt;h2&gt;How the Tupper formula works&lt;/h2&gt;
&lt;p&gt;The distillation of the Tupper to JS code already peels off a few layers of
mystery. Let's now remove the rest of the curtain on its inner workings.&lt;/p&gt;
&lt;p&gt;I'll start by explaining how to take an image we want the formula
to produce and encode it into &lt;em&gt;K&lt;/em&gt;. Here are the first three columns of the
Tupper formula plot:&lt;/p&gt;
&lt;img alt="Closeup of tupper plot with encoding of pixels" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-closeup.png" /&gt;
&lt;p&gt;Each pixel in the plot is converted to a bit (0 for light, 1 for dark). We
start at the bottom left corner (&lt;em&gt;x=0&lt;/em&gt; and &lt;em&gt;y=K&lt;/em&gt;), which is the LSB
(least-significant bit) and move up through the first column; when we reach the
top (&lt;em&gt;x=0&lt;/em&gt; and &lt;em&gt;y=K+16&lt;/em&gt;), we continue from the bottom of the next column
(&lt;em&gt;x=1&lt;/em&gt; and &lt;em&gt;y=K&lt;/em&gt;). In the example above, the first bits (from lowest to highest)
of the number are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;00110010101000100 00101010101111100 ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once we're done with the whole number (106x17 = 1802 bits), we convert it to
decimal - let's call this number &lt;em&gt;IMG&lt;/em&gt;, and multiply by 17. The result is &lt;em&gt;K&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now back to &lt;tt class="docutils literal"&gt;tupperFormula&lt;/tt&gt;, looking at how it decodes the image back from
&lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; (recall that &lt;em&gt;y&lt;/em&gt; runs from &lt;em&gt;K&lt;/em&gt; to &lt;em&gt;K+16&lt;/em&gt;). Let's work through
the first coordinate in detail:&lt;/p&gt;
&lt;p&gt;For &lt;em&gt;x=0&lt;/em&gt; and &lt;em&gt;y=K&lt;/em&gt;, in &lt;tt class="docutils literal"&gt;tupperFormula&lt;/tt&gt; we get:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;d = (y/17) &amp;gt;&amp;gt; (17x + y%17)
...
substitute x=0, y=K (and recall that K = IMG * 17)
...
d = IMG &amp;gt;&amp;gt; 0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In other words, &lt;em&gt;d&lt;/em&gt; is the lowest bit of &lt;em&gt;IMG&lt;/em&gt; - the lowest bit of our image!
We can continue for &lt;em&gt;x=0&lt;/em&gt; and &lt;em&gt;y=K+1&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;d = (y/17) &amp;gt;&amp;gt; (17x + y%17)
...
substitute x=0, y=K+1 (and recall that K = IMG * 17)
...
d = IMG &amp;gt;&amp;gt; 1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here &lt;em&gt;d&lt;/em&gt; is the second lowest bit of &lt;em&gt;IMG&lt;/em&gt;. The pattern should be clear by now.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;d = (y/17) &amp;gt;&amp;gt; (17x + y%17)
...
x=0  y=K+2:  IMG &amp;gt;&amp;gt; (0 + 2)
x=0  y=K+3:  IMG &amp;gt;&amp;gt; (0 + 3)
...
x=0  y=K+16  IMG &amp;gt;&amp;gt; (0 + 16)
x=1  y=K:    IMG &amp;gt;&amp;gt; (17 + 0)
x=1  y=K+1:  IMG &amp;gt;&amp;gt; (17 + 1)
x=1  y=K+2:  IMG &amp;gt;&amp;gt; (17 + 2)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The formula simply calculates the correct bit of &lt;em&gt;IMG&lt;/em&gt; given &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;, using
a modular arithmetic trick to &amp;quot;fold&amp;quot; the 2D &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; into a 1D
sequence (this is just customary &lt;a class="reference external" href="https://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays"&gt;column-major layout&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This is why the formula can plot any 106x17 grid, given the right &lt;em&gt;K&lt;/em&gt;. In the
formula, 17 is not some piece of magic - it's just the height of the grid. As an
exercise, you can modify the formula and code to plot larger or smaller grids.&lt;/p&gt;
&lt;p&gt;As a bonus, the JavaScript demo can also encode a grid back to its
representative &lt;em&gt;K&lt;/em&gt;; here's the code for it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;// Calculate K value from the grid.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;encodeGridToK&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;kval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Build up K from MSB to LSB, scanning from the top-right corner down and&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// then moving left by column.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridWidth&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GridHeight&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nx"&gt;kval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;kval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;Grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;getCell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;kval&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;17n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It constructs &lt;em&gt;K&lt;/em&gt; starting with the MSB, but otherwise the code is
straightforward to follow.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="background"&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;The formula was first describe by Jeff Tupper in a 2001 paper titled
&amp;quot;Reliable Two-Dimensional Graphing Methods for Mathematical Formulae with Two
Free Variables&amp;quot;. The paper itself focuses on methods of precisely graphing
relations and presents several algorithms to do so. This formula is described
in passing in section 12, and presented as follows:&lt;/p&gt;
&lt;img alt="Screenshot from Tupper's paper describing the formula" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-paper-crop1.png" /&gt;
&lt;p&gt;And Figure 13 is:&lt;/p&gt;
&lt;img alt="Screenshot from Tupper's paper showing the formula itself" class="align-center" src="https://eli.thegreenplace.net/images/2023/tupper-paper-crop2.png" /&gt;
&lt;p&gt;Interestingly, the &lt;em&gt;K&lt;/em&gt; provided by Tupper's paper renders the formula flipped
on both the &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; axes using the standard grid used in this post &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.
This is why my JavaScript demo has flip toggles that let you flip the axes of any
plot.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This would be&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1445202489708975828479425373371945674812777822151507024797
1881396854908873568298734888825132090576643817888323197692
3440016667764749242125128995265907053708020473915320841631
7920255490054180047686572016997304663833949016013743197155
2099618114524978194501906835950051065780432564080119786755
6863142280259694206254096081665642417367403946384170774537
4273196064438999230103793989386750257869294552344763192918
6095761834543224800492172803334941981620674985447203819393
9738513848960476759782673313437697051994580681869819330446
336774047268864
&lt;/pre&gt;&lt;/div&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I can totally see why the &lt;em&gt;y&lt;/em&gt; axis would be flipped: in computer programs
the concept of the &lt;em&gt;y&lt;/em&gt; axis is represented as &lt;em&gt;rows&lt;/em&gt; in a grid which
typically count from 0 on top and downwards. It's less clear to me how
the inversion on the &lt;em&gt;x&lt;/em&gt; axis came to be.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="JavaScript"></category></entry></feed>