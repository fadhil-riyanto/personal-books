<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Eli Bendersky's website - Machine Learning</title><link href="https://eli.thegreenplace.net/" rel="alternate"></link><link href="https://eli.thegreenplace.net/feeds/machine-learning.atom.xml" rel="self"></link><id>https://eli.thegreenplace.net/</id><updated>2025-02-14T13:49:31-08:00</updated><entry><title>Decorator JITs - Python as a DSL</title><link href="https://eli.thegreenplace.net/2025/decorator-jits-python-as-a-dsl/" rel="alternate"></link><published>2025-02-03T06:22:00-08:00</published><updated>2025-02-14T13:49:31-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2025-02-03:/2025/decorator-jits-python-as-a-dsl/</id><summary type="html">&lt;p&gt;Spend enough time looking at Python programs and packages for machine learning,
and you'll notice that the &amp;quot;JIT decorator&amp;quot; pattern is pretty popular. For
example, this JAX snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jax.numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;jnp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jax&lt;/span&gt;

&lt;span class="nd"&gt;@jax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Use &amp;quot;add&amp;quot; as a â€¦&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Spend enough time looking at Python programs and packages for machine learning,
and you'll notice that the &amp;quot;JIT decorator&amp;quot; pattern is pretty popular. For
example, this JAX snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jax.numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;jnp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jax&lt;/span&gt;

&lt;span class="nd"&gt;@jax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;jnp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Use &amp;quot;add&amp;quot; as a regular Python function&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or the &lt;a class="reference external" href="https://triton-lang.org/main/index.html"&gt;Triton language&lt;/a&gt;
for writing GPU kernels directly in Python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;triton&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;triton.language&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tl&lt;/span&gt;

&lt;span class="nd"&gt;@triton&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_ptr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;y_ptr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;output_ptr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;n_elements&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="n"&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constexpr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;program_id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;block_start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pid&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;BLOCK_SIZE&lt;/span&gt;
    &lt;span class="n"&gt;offsets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;block_start&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BLOCK_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;offsets&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n_elements&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_ptr&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;offsets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_ptr&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;offsets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;
    &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;store&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_ptr&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;offsets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In both cases, the function decorated with &lt;tt class="docutils literal"&gt;jit&lt;/tt&gt; doesn't get executed by the
Python interpreter in the normal sense. Instead, the code inside is more like
a DSL (Domain Specific Language) processed by a special purpose compiler built
into the library (JAX or Triton). Another way to think about it is that Python
is used as a &lt;em&gt;meta language&lt;/em&gt; to describe computations.&lt;/p&gt;
&lt;p&gt;In this post I will describe some implementation strategies used by libraries to
make this possible.&lt;/p&gt;
&lt;div class="section" id="preface-where-we-re-going"&gt;
&lt;h2&gt;Preface - where we're going&lt;/h2&gt;
&lt;p&gt;The goal is to explain how different kinds of &lt;tt class="docutils literal"&gt;jit&lt;/tt&gt; decorators work by using
a simplified, educational example that implements several approaches from
scratch. All the approaches featured in this post will be using this flow:&lt;/p&gt;
&lt;img alt="Flow of Python source --&amp;gt; Expr IR --&amp;gt; LLVM IR --&amp;gt; Execution" class="align-center" src="https://eli.thegreenplace.net/images/2025/decjit-python.png" /&gt;
&lt;p&gt;These are the steps that happen when a Python function wrapped with
our educational &lt;tt class="docutils literal"&gt;jit&lt;/tt&gt; decorator is called:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;The function is translated to an &amp;quot;expression IR&amp;quot; - &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;This expression IR is converted to LLVM IR.&lt;/li&gt;
&lt;li&gt;Finally, the LLVM IR is JIT-executed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Steps (2) and (3) use &lt;a class="reference external" href="https://github.com/numba/llvmlite"&gt;llvmlite&lt;/a&gt;; I've
written about llvmlite before, see &lt;a class="reference external" href="https://eli.thegreenplace.net/2015/building-and-using-llvmlite-a-basic-example/"&gt;this post&lt;/a&gt;
and also the &lt;a class="reference external" href="https://github.com/eliben/pykaleidoscope"&gt;pykaleidoscope project&lt;/a&gt;.
For an introduction to JIT compilation, be sure to &lt;a class="reference external" href="https://eli.thegreenplace.net/2013/11/05/how-to-jit-an-introduction"&gt;read this&lt;/a&gt;
and maybe also the series of posts &lt;a class="reference external" href="https://eli.thegreenplace.net/2017/adventures-in-jit-compilation-part-1-an-interpreter/"&gt;starting here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, let's look at the &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; IR. Here we'll make a big simplification -
only supporting functions that define a single expression, e.g.:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;expr2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Naturally, this can be easily generalized - after all, LLVM IR can be used to
express fully general computations.&lt;/p&gt;
&lt;p&gt;Here are the &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; data structures:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Expr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="nd"&gt;@dataclass&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ConstantExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Expr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;

&lt;span class="nd"&gt;@dataclass&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;VarExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Expr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
    &lt;span class="n"&gt;arg_idx&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Enum&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;ADD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;+&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;SUB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;-&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;MUL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;DIV&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;

&lt;span class="nd"&gt;@dataclass&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BinOpExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Expr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Expr&lt;/span&gt;
    &lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Expr&lt;/span&gt;
    &lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To convert an &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; into LLVM IR and JIT-execute it, we'll use this function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;llvm_jit_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Use LLVM JIT to evaluate the given expression with *args.&lt;/span&gt;

&lt;span class="sd"&gt;    expr is an instance of Expr. *args are the arguments to the expression, each&lt;/span&gt;
&lt;span class="sd"&gt;    a float. The arguments must match the arguments the expression expects.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns the result of evaluating the expression.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;llvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;llvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_native_target&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;llvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_native_asmprinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;llvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_native_asmparser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;cg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_LLVMCodeGenerator&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;modref&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;llvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_assembly&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codegen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

    &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;llvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_default_triple&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;target_machine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create_target_machine&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;llvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create_mcjit_compiler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;modref&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_machine&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;ee&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;ee&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;finalize_object&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;cfptr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ee&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_function_address&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;func&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;cfunc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CFUNCTYPE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c_double&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;c_double&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)))(&lt;/span&gt;&lt;span class="n"&gt;cfptr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cfunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It uses the &lt;tt class="docutils literal"&gt;_LLVMCodeGenerator&lt;/tt&gt; class to actually generate LLVM IR from &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt;.
This process is straightforward and covered extensively in the resources I
linked to earlier; take a look at &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2025/decjit/exprcode.py"&gt;the full code here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My goal with this architecture is to make things simple, but &lt;em&gt;not too simple&lt;/em&gt;.
On one hand - there are several simplifications: only single expressions are
supported, very limited set of operators, etc. It's very easy to extend this!
On the other hand, we could have just trivially evaluated the &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt;
without resorting to LLVM IR; I do want to show a more complete compilation
pipeline, though, to demonstrate that an arbitrary amount of complexity can
be hidden behind these simple interfaces.&lt;/p&gt;
&lt;p&gt;With these building blocks in hand, we can review the strategies used by
&lt;tt class="docutils literal"&gt;jit&lt;/tt&gt; decorators to convert Python functions into &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt;s.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ast-based-jit"&gt;
&lt;h2&gt;AST-based JIT&lt;/h2&gt;
&lt;p&gt;Python comes with powerful code reflection and introspection capabilities out
of the box. Here's the &lt;tt class="docutils literal"&gt;astjit&lt;/tt&gt; decorator:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;astjit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nd"&gt;@functools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;ASTJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Keyword arguments are not supported&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inspect&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getsource&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tree&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ast&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;emitter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_ExprCodeEmitter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;emitter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;visit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tree&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;llvm_jit_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emitter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wrapper&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is a standard Python decorator. It takes a function and returns another
function that will be used in its place (&lt;tt class="docutils literal"&gt;functools.wraps&lt;/tt&gt; ensures that
function attributes like the name and docstring of the wrapper match the
wrapped function).&lt;/p&gt;
&lt;p&gt;Here's how it's used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;astjit&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;astjit&lt;/span&gt;

&lt;span class="nd"&gt;@astjit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;some_expr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;some_expr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After &lt;tt class="docutils literal"&gt;astjit&lt;/tt&gt; is applied to &lt;tt class="docutils literal"&gt;some_expr&lt;/tt&gt;, what &lt;tt class="docutils literal"&gt;some_expr&lt;/tt&gt; holds is the
wrapper. When &lt;tt class="docutils literal"&gt;some_expr(2, 16, 3)&lt;/tt&gt; is called, the wrapper is invoked with
&lt;tt class="docutils literal"&gt;*args = [2, 16, 3]&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;The wrapper obtains the AST of the wrapped function, and then uses
&lt;tt class="docutils literal"&gt;_ExprCodeEmitter&lt;/tt&gt; to convert this AST into an &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;_ExprCodeEmitter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ast&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NodeVisitor&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_expr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;ast&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Add&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ADD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;ast&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sub&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SUB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;ast&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Mult&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MUL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;ast&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Div&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DIV&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;visit_FunctionDef&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ast&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Return&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;ASTJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Function must consist of a single return statement&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;visit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;visit_Return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_expr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;visit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;visit_Name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;ASTJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Unknown variable &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;VarExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;visit_Constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ConstantExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;visit_BinOp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;visit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;visit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op_map&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;BinOpExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;KeyError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;ASTJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Unsupported operator &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When &lt;tt class="docutils literal"&gt;_ExprCodeEmitter&lt;/tt&gt; finishes visiting the AST it's given, its
&lt;tt class="docutils literal"&gt;return_expr&lt;/tt&gt; field will contain the &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; representing the function's
return value. The wrapper then invokes &lt;tt class="docutils literal"&gt;llvm_jit_evaluate&lt;/tt&gt; with this &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Note how our decorator interjects into the regular Python execution process.
When &lt;tt class="docutils literal"&gt;some_expr&lt;/tt&gt; is called, instead of the standard Python compilation and
execution process (code is compiled into bytecode, which is then executed
by the VM), we translate its code to our own representation and emit LLVM from
it, and then JIT execute the LLVM IR. While it seems kinda pointless in this
artificial example, in reality this means we can execute the function's code
in any way we like.&lt;/p&gt;
&lt;div class="section" id="ast-jit-case-study-triton"&gt;
&lt;h3&gt;AST JIT case study: Triton&lt;/h3&gt;
&lt;p&gt;This approach is almost exactly how the Triton language works. The body of a
function decorated with &lt;tt class="docutils literal"&gt;&amp;#64;triton.jit&lt;/tt&gt; gets parsed to a Python AST, which then
- through a series of internal IRs - ends up in LLVM IR; this in turn is lowered
to &lt;a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/"&gt;PTX&lt;/a&gt; by the
&lt;a class="reference external" href="https://llvm.org/docs/NVPTXUsage.html"&gt;NVPTX LLVM backend&lt;/a&gt;.
Then, the code runs on a GPU using a standard CUDA pipeline.&lt;/p&gt;
&lt;p&gt;Naturally, the subset of Python that can be compiled down to a GPU is limited;
but it's sufficient to run performant kernels, in a language that's much
friendlier than CUDA and - more importantly - lives in the same file with the
&amp;quot;host&amp;quot; part written in regular Python. For example, if you want testing and
debugging, you can run Triton in &amp;quot;interpreter mode&amp;quot; which will just run the
same kernels locally on a CPU.&lt;/p&gt;
&lt;p&gt;Note that Triton lets us import names from the &lt;tt class="docutils literal"&gt;triton.language&lt;/tt&gt; package
and use them inside kernels; these serve as the &lt;em&gt;intrinsics&lt;/em&gt; for the language
- special calls the compiler handles directly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="bytecode-based-jit"&gt;
&lt;h2&gt;Bytecode-based JIT&lt;/h2&gt;
&lt;p&gt;Python is a fairly complicated language with &lt;em&gt;a lot&lt;/em&gt; of features. Therefore,
if our JIT has to support some large portion of Python semantics, it may make
sense to leverage more of Python's own compiler. Concretely, we can have it
compile the wrapped function all the way &lt;a class="reference external" href="https://github.com/python/cpython/blob/main/InternalDocs/interpreter.md"&gt;to bytecode&lt;/a&gt;,
and start our translation from there.&lt;/p&gt;
&lt;p&gt;Here's the &lt;tt class="docutils literal"&gt;bytecodejit&lt;/tt&gt; decorator that does just this &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bytecodejit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nd"&gt;@functools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;BytecodeJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Keyword arguments are not supported&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;expr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_emit_exprcode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;llvm_jit_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wrapper&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_emit_exprcode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;bc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__code__&lt;/span&gt;
    &lt;span class="n"&gt;stack&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_instructions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;match&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opname&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;LOAD_FAST&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arg&lt;/span&gt;
                &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VarExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;co_varnames&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;LOAD_CONST&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ConstantExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argval&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;BINARY_OP&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;right&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="n"&gt;left&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="k"&gt;match&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argrepr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;+&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                        &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BinOpExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ADD&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;-&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                        &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BinOpExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SUB&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                        &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BinOpExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MUL&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                        &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BinOpExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DIV&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;BytecodeJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Unsupported operator &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argval&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;RETURN_VALUE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;BytecodeJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Invalid stack state&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;RESUME&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;CACHE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="c1"&gt;# Skip nops&lt;/span&gt;
                &lt;span class="k"&gt;pass&lt;/span&gt;
            &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;BytecodeJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Unsupported opcode &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opname&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The Python VM is a stack machine; so we emulate a stack to convert the
function's bytecode to &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; IR (a bit like an &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Reverse_Polish_notation"&gt;RPN evaluator&lt;/a&gt;).
As before, we then use our &lt;tt class="docutils literal"&gt;llvm_jit_evaluate&lt;/tt&gt; utility function to lower
&lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; to LLVM IR and JIT execute it.&lt;/p&gt;
&lt;p&gt;Using this JIT is as simple as the previous one - just swap &lt;tt class="docutils literal"&gt;astjit&lt;/tt&gt;
for &lt;tt class="docutils literal"&gt;bytecodejit&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bytecodejit&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;bytecodejit&lt;/span&gt;

&lt;span class="nd"&gt;@bytecodejit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;some_expr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;some_expr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="section" id="bytecode-jit-case-study-numba"&gt;
&lt;h3&gt;Bytecode JIT case study: Numba&lt;/h3&gt;
&lt;p&gt;&lt;a class="reference external" href="https://numba.pydata.org/"&gt;Numba&lt;/a&gt; is a compiler for Python itself. The idea
is that you can speed up specific functions in your code by slapping a
&lt;tt class="docutils literal"&gt;numba.njit&lt;/tt&gt; decorator on them. What happens next is similar in spirit to
our simple &lt;tt class="docutils literal"&gt;bytecodejit&lt;/tt&gt;, but of course much more complicated because it
supports a very large portion of Python semantics.&lt;/p&gt;
&lt;p&gt;Numba uses the Python compiler to emit bytecode, just as we did; it then
converts it into its own IR, and then to LLVM using &lt;tt class="docutils literal"&gt;llvmlite&lt;/tt&gt; &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;By starting with the bytecode, Numba makes its life easier (no need to rewrite
the entire Python compiler). On the other hand, it also makes some analyses
&lt;em&gt;harder&lt;/em&gt;, because by the time we're in bytecode, a lot of semantic information
existing in higher-level representations is lost. For example, Numba has to
sweat a bit to recover control flow information from the bytecode (by
running it through a special interpreter first).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="tracing-based-jit"&gt;
&lt;h2&gt;Tracing-based JIT&lt;/h2&gt;
&lt;p&gt;The two approaches we've seen so far are similar in many ways - both rely on
Python's introspection capabilities to compile the source code of the JIT-ed
function to some extent (one to AST, the other all the way to bytecode), and
then work on this lowered representation.&lt;/p&gt;
&lt;p&gt;The tracing strategy is very different. It doesn't analyze the source code of
the wrapped function at all - instead, it &lt;em&gt;traces&lt;/em&gt; its execution by means of
specially-boxed arguments, leveraging overloaded operators and functions, and
then works on the generated trace.&lt;/p&gt;
&lt;p&gt;The code implementing this for our smile demo is surprisingly compact:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;tracejit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nd"&gt;@functools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wraps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;TraceJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Keyword arguments are not supported&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;argspec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;inspect&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getfullargspec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;argboxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arg&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;argspec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;TraceJITError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Too many arguments&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;argboxes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VarExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;argspec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="n"&gt;out_box&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;argboxes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;llvm_jit_evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wrapper&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Each runtime argument of the wrapped function is assigned a &lt;tt class="docutils literal"&gt;VarExpr&lt;/tt&gt;, and
that is placed in a &lt;tt class="docutils literal"&gt;_Box&lt;/tt&gt;, a placeholder class which lets us
do operator overloading:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@dataclass&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;_Box&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Expr&lt;/span&gt;

&lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__add__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__radd__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_register_binary_op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ADD&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__sub__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_register_binary_op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SUB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__rsub__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_register_binary_op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SUB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__mul__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__rmul__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_register_binary_op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MUL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__truediv__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_register_binary_op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DIV&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__rtruediv__&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_register_binary_op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Op&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DIV&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The remaining key function is &lt;tt class="docutils literal"&gt;_register_binary_op&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_register_binary_op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;opcode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Registers a binary opcode for Boxes.&lt;/span&gt;

&lt;span class="sd"&gt;    If reverse is True, the operation is registered as arg2 &amp;lt;op&amp;gt; arg1,&lt;/span&gt;
&lt;span class="sd"&gt;    instead of arg1 &amp;lt;op&amp;gt; arg2.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_op&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arg2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;arg1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arg2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arg1&lt;/span&gt;
        &lt;span class="n"&gt;box1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ConstantExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;box2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arg2&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ConstantExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arg2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;_Box&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BinOpExpr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;box1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;box2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;opcode&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;_op&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To understand how this works, consider this trivial example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@tracejit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After the decorated function is defined, &lt;tt class="docutils literal"&gt;add&lt;/tt&gt; holds the wrapper function
defined inside &lt;tt class="docutils literal"&gt;tracejit&lt;/tt&gt;. When &lt;tt class="docutils literal"&gt;add(1, 2)&lt;/tt&gt; is called, the wrapper runs:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;For each argument of &lt;tt class="docutils literal"&gt;add&lt;/tt&gt; itself (that is &lt;tt class="docutils literal"&gt;a&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;b&lt;/tt&gt;), it creates
a new &lt;tt class="docutils literal"&gt;_Box&lt;/tt&gt; holding a &lt;tt class="docutils literal"&gt;VarExpr&lt;/tt&gt;. This denotes a named variable in
the &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; IR.&lt;/li&gt;
&lt;li&gt;It then calls the wrapped function, passing it the boxes as runtime
parameters.&lt;/li&gt;
&lt;li&gt;When (the wrapped) &lt;tt class="docutils literal"&gt;add&lt;/tt&gt; runs, it invokes &lt;tt class="docutils literal"&gt;a + b&lt;/tt&gt;. This is caught by the overloaded
&lt;tt class="docutils literal"&gt;__add__&lt;/tt&gt; operator of &lt;tt class="docutils literal"&gt;_Box&lt;/tt&gt;, and it creates a new &lt;tt class="docutils literal"&gt;BinOpExpr&lt;/tt&gt; with
the &lt;tt class="docutils literal"&gt;VarExpr&lt;/tt&gt;s representing &lt;tt class="docutils literal"&gt;a&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;b&lt;/tt&gt; as children. This
&lt;tt class="docutils literal"&gt;BinOpExpr&lt;/tt&gt; is then returned &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The wrapper unboxes the returned &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; and passes it to
&lt;tt class="docutils literal"&gt;llvm_jit_evaluate&lt;/tt&gt; to emit LLVM IR from it and JIT execute it with the
actual runtime arguments of the call: &lt;tt class="docutils literal"&gt;1, 2&lt;/tt&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This might be a little mind-bending at first, because there are two different
executions that happen:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The first is calling the wrapped &lt;tt class="docutils literal"&gt;add&lt;/tt&gt; function itself, letting the Python
interpreter run it as usual, but with special arguments that build up the IR
instead of doing any computations. This is the &lt;em&gt;tracing step&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The second is lowering this IR our tracing step built into LLVM IR and then
JIT executing it with the actual runtime argument values &lt;tt class="docutils literal"&gt;1, 2&lt;/tt&gt;; this is
the &lt;em&gt;execution step&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tracing approach has some interesting characteristics. Since we don't
have to analyze the source of the wrapped functions but only trace through
the execution, we can &amp;quot;magically&amp;quot; support a much richer set of programs, e.g.:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@tracejit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;use_locals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;use_locals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This &lt;em&gt;just works&lt;/em&gt; with our basic &lt;tt class="docutils literal"&gt;tracejit&lt;/tt&gt;. Since Python variables are
placeholders (references) for values, our tracing step is oblivious to them - it
follows the flow of values. Another example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@tracejit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;use_loop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;use_loop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This also just works! The created &lt;tt class="docutils literal"&gt;Expr&lt;/tt&gt; will be a long chain of &lt;tt class="docutils literal"&gt;BinExpr&lt;/tt&gt;
additions of &lt;tt class="docutils literal"&gt;i&lt;/tt&gt;'s runtime values through the loop, added to the &lt;tt class="docutils literal"&gt;BinExpr&lt;/tt&gt;
for &lt;tt class="docutils literal"&gt;b * c&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;This last example also leads us to a limitation of the tracing approach; the
loop cannot be &lt;em&gt;data-dependent&lt;/em&gt; - it cannot depend on the function's arguments,
because the tracing step has no concept of runtime values and wouldn't know
how many iterations to run through; or at least, it doesn't know this unless
we want to perform the tracing run for every runtime execution &lt;a class="footnote-reference" href="#footnote-4" id="footnote-reference-4"&gt;[4]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The tracing approach is useful in several domains, most notably
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation"&gt;automatic differentiation&lt;/a&gt; (AD).
For a slightly deeper taste, check out my &lt;a class="reference external" href="https://github.com/eliben/radgrad"&gt;radgrad&lt;/a&gt; project.&lt;/p&gt;
&lt;div class="section" id="tracing-jit-case-study-jax"&gt;
&lt;h3&gt;Tracing JIT case study: JAX&lt;/h3&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://jax.readthedocs.io/en/latest/"&gt;JAX ML framework&lt;/a&gt; uses a tracing
approach very similar to the one described here. The first code sample in this
post shows the JAX notation. JAX cleverly wraps Numpy with its own version which
is traced (similar to our &lt;tt class="docutils literal"&gt;_Box&lt;/tt&gt;, but JAX calls these boxes &amp;quot;tracers&amp;quot;),
letting you write regular-feeling Numpy code that can be JIT optimized and
executed on accelerators like GPUs and TPUs via &lt;a class="reference external" href="https://github.com/openxla"&gt;XLA&lt;/a&gt;. JAX's tracer builds up an underlying IR (called
&lt;a class="reference external" href="https://jax.readthedocs.io/en/latest/jaxpr.html"&gt;jaxpr&lt;/a&gt;) which can then be
emitted to XLA ops and passed to XLA for further lowering and execution.&lt;/p&gt;
&lt;p&gt;For a fairly deep overview of how JAX works, I recommend reading the
&lt;a class="reference external" href="https://jax.readthedocs.io/en/latest/autodidax.html"&gt;autodidax doc&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As mentioned earlier, JAX has &lt;a class="reference external" href="https://jax.readthedocs.io/en/latest/jit-compilation.html"&gt;some limitations&lt;/a&gt;
with things like data-dependent control flow in native Python. This won't work,
because there's control flow
that depends on a runtime value (&lt;tt class="docutils literal"&gt;count&lt;/tt&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jax&lt;/span&gt;

&lt;span class="nd"&gt;@jax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sum_datadep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sum_datadep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When &lt;tt class="docutils literal"&gt;sum_datadep&lt;/tt&gt; is executed, JAX will throw an exception, saying something
like:&lt;/p&gt;
&lt;blockquote&gt;
This concrete value was not available in Python because it depends on the
value of the argument count.&lt;/blockquote&gt;
&lt;p&gt;As a remedy, JAX has its
own built-in intrinsics from the &lt;a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.lax.html"&gt;jax.lax package&lt;/a&gt;.
Here's the example rewritten in a way that actually works:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;jax&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;jax&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;lax&lt;/span&gt;

&lt;span class="nd"&gt;@jax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jit&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sum_datadep_fori&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;body&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fori_loop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;fori_loop&lt;/tt&gt; (and many other built-ins in the &lt;tt class="docutils literal"&gt;lax&lt;/tt&gt; package) is something JAX
can trace through, generating a corresponding XLA operation (XLA has support for
&lt;a class="reference external" href="https://openxla.org/xla/operation_semantics"&gt;While loops&lt;/a&gt;, to which this
&lt;tt class="docutils literal"&gt;lax.fori_loop&lt;/tt&gt; can be lowered).&lt;/p&gt;
&lt;p&gt;The tracing approach has clear benefits for JAX as well; because it only cares
about the flow of values, it can handle arbitrarily complicated Python code,
as long as the flow of values can be traced. Just like the local variables and
data-independent loops shown earlier, but also things like closures. This makes
meta-programming and templating easy &lt;a class="footnote-reference" href="#footnote-5" id="footnote-reference-5"&gt;[5]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The full code for this post is available &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2025/decjit"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Once again, this is a very simplified example. A more realistic
translator would have to support &lt;a class="reference external" href="https://docs.python.org/3/library/dis.html#python-bytecode-instructions"&gt;many, many more&lt;/a&gt;
Python bytecode instructions.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;In fact, &lt;tt class="docutils literal"&gt;llvmlite&lt;/tt&gt; itself is a Numba sub-project and is maintained
by the Numba team, for which I'm grateful!&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;For a fun exercise, try adding constant folding to the wrapped &lt;tt class="docutils literal"&gt;_op&lt;/tt&gt;:
when both its arguments are constants (not boxes), instead placing
each in a &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;_Box(ConstantExpr(...))&lt;/span&gt;&lt;/tt&gt;, it could perform the mathematical
operation on them and return a single constant box. This is a common
optimization in compilers!&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;p class="first"&gt;In all the JIT approaches showed in this post, the expectation is that
compilation happens once, but the compiled function can be executed
many times (perhaps in a loop). This means that the compilation step
cannot depend on the runtime values of the function's arguments, because
it has no access to them. You could say that it &lt;em&gt;does&lt;/em&gt;, but that's just
for the very first time the function is run (in the tracing approach);
it has no way of knowing their values the next times the function will
run.&lt;/p&gt;
&lt;p class="last"&gt;JAX has &lt;a class="reference external" href="https://jax.readthedocs.io/en/latest/jit-compilation.html#marking-arguments-as-static"&gt;some provisions&lt;/a&gt;
for cases where a function is invoked with a small set of runtime
values and we want to separately JIT each of them.&lt;/p&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;A reader pointed out that &lt;a class="reference external" href="https://blog.tensorflow.org/2018/07/autograph-converts-python-into-tensorflow-graphs.html"&gt;TensorFlow's AutoGraph&lt;/a&gt;
feature combines the AST and tracing approaches. TF's &lt;em&gt;eager mode&lt;/em&gt;
performs tracing, but it also uses AST analyses to rewrite Python loops
and conditions into builtins like &lt;tt class="docutils literal"&gt;tf.cond&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;tf.while_loop&lt;/tt&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Python"></category><category term="Compilation"></category><category term="Machine Learning"></category></entry><entry><title>Reverse mode Automatic Differentiation</title><link href="https://eli.thegreenplace.net/2025/reverse-mode-automatic-differentiation/" rel="alternate"></link><published>2025-01-13T19:02:00-08:00</published><updated>2025-01-17T21:51:01-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2025-01-13:/2025/reverse-mode-automatic-differentiation/</id><summary type="html">&lt;p&gt;Automatic Differentiation (AD) is an important algorithm for calculating the
derivatives of arbitrary functions that can be expressed by a computer program.
One of my favorite CS papers is
&lt;a class="reference external" href="https://arxiv.org/abs/1502.05767"&gt;&amp;quot;Automatic differentiation in machine learning: a survey&amp;quot;&lt;/a&gt; by
Baydin, Perlmutter, Radul and Siskind (ADIMLAS from here on).
While this post attempts â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Automatic Differentiation (AD) is an important algorithm for calculating the
derivatives of arbitrary functions that can be expressed by a computer program.
One of my favorite CS papers is
&lt;a class="reference external" href="https://arxiv.org/abs/1502.05767"&gt;&amp;quot;Automatic differentiation in machine learning: a survey&amp;quot;&lt;/a&gt; by
Baydin, Perlmutter, Radul and Siskind (ADIMLAS from here on).
While this post attempts to be useful on its own, it serves best as a followup
to the ADIMLAS paper - so I strongly encourage you to read that first.&lt;/p&gt;
&lt;p&gt;The main idea of AD is to treat a computation as a nested sequence of function
compositions, and then calculate the derivative of the outputs w.r.t. the inputs
using repeated applications of the chain rule. There are two methods of AD:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Forward mode: where derivatives are computed starting at the inputs&lt;/li&gt;
&lt;li&gt;Reverse mode: where derivatives are computed starting at the outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reverse mode AD is a generalization of the &lt;em&gt;backpropagation&lt;/em&gt; technique used
in training neural networks. While backpropagation starts from a single scalar
output, reverse mode AD works for any number of function outputs. In this post
I'm going to be describing how reverse mode AD works in detail.&lt;/p&gt;
&lt;p&gt;While reading the ADIMLAS paper is strongly recommended but not required,
there &lt;em&gt;is&lt;/em&gt; one mandatory pre-requisite for this post: a good understanding of
the chain rule of calculus, including its multivariate formulation. Please
read &lt;a class="reference external" href="https://eli.thegreenplace.net/2016/the-chain-rule-of-calculus"&gt;my earlier post on the subject&lt;/a&gt;
first if you're not familiar with it.&lt;/p&gt;
&lt;div class="section" id="linear-chain-graphs"&gt;
&lt;h2&gt;Linear chain graphs&lt;/h2&gt;
&lt;p&gt;Let's start with a simple example where the computation is a linear chain of
primitive operations: the Sigmoid function.&lt;/p&gt;
&lt;img alt="\[S(x)=\frac{1}{1+e^{-x}}\]" class="align-center" src="https://eli.thegreenplace.net/images/math/9a39d0495ce32da5840b76adaf508a0349394c49.png" style="height: 38px;" /&gt;
&lt;p&gt;This is a basic Python implementation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To apply the chain rule, we'll break down the calculation of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/fa94a6b584da9e970e40fbcbe4d615031ac59bc2.svg" style="height: 19px;" type="image/svg+xml"&gt;S(x)&lt;/object&gt; to
a sequence of function compositions, as follows:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/33494603270cd9090af303e2e99cf367173f588b.svg" style="height: 119px;" type="image/svg+xml"&gt;\[\begin{align*}
  f(x)&amp;amp;=-x\\
  g(f)&amp;amp;=e^f\\
  w(g)&amp;amp;=1+g\\
  v(w)&amp;amp;=\frac{1}{w}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Take a moment to convince yourself that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/fa94a6b584da9e970e40fbcbe4d615031ac59bc2.svg" style="height: 19px;" type="image/svg+xml"&gt;S(x)&lt;/object&gt; is equivalent to
the composition &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/2496463ea05f38acb598c84a70bfcc4692591511.svg" style="height: 19px;" type="image/svg+xml"&gt;v\circ(w\circ(g\circ f))(x)&lt;/object&gt;.&lt;/p&gt;
&lt;p&gt;The same decomposition of &lt;tt class="docutils literal"&gt;sigmoid&lt;/tt&gt; into primitives in Python would look as
follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;
    &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Yet another representation is this computational graph:&lt;/p&gt;
&lt;img alt="Computational graph showing sigmoid" class="align-center" src="https://eli.thegreenplace.net/images/2025/sigmoid-graph.png" /&gt;
&lt;p&gt;Each box (graph node) represents a primitive operation, and the name assigned
to it (the green rectangle on the right of each box). An arrows (graph edge)
represent the flow of values between operations.&lt;/p&gt;
&lt;p&gt;Our goal is to find the derivative of &lt;em&gt;S&lt;/em&gt; w.r.t. &lt;em&gt;x&lt;/em&gt; at some point &lt;img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /&gt;,
denoted as &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/e4ba6fb484fe0c504010c93b6144b99a885a0b97.svg" style="height: 19px;" type="image/svg+xml"&gt;S&amp;#x27;(x_0)&lt;/object&gt;. The process starts by running the computational
graph forward with our value of &lt;img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /&gt;. As an example, we'll use
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/064a529ef0eed72cc7839b45e015ec432a83c9b2.svg" style="height: 16px;" type="image/svg+xml"&gt;x_0=0.5&lt;/object&gt;:&lt;/p&gt;
&lt;img alt="Computational graph with forward calculation at 0.5" class="align-center" src="https://eli.thegreenplace.net/images/2025/sigmoid-graph-forward-calc.png" /&gt;
&lt;p&gt;Since all the functions in this graph have a single input and a single output,
it's sufficient to use the single-variable formulation of the chain rule.&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/28362ed858958e6d1e4754338cb3e6bc6aca6026.svg" style="height: 21px;" type="image/svg+xml"&gt;\[(g \circ f)&amp;#x27;(x_0)={g}&amp;#x27;(f(x_0)){f}&amp;#x27;(x_0)\]&lt;/object&gt;
&lt;p&gt;To avoid confusion, let's switch notation so we can explicitly see which
derivatives are involved. For &lt;img alt="f(x)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png" style="height: 18px;" /&gt; and &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/5ad8d30dbab83209f45a10850ffbcacf6662321e.svg" style="height: 19px;" type="image/svg+xml"&gt;g(f)&lt;/object&gt; as before, we can
write the derivatives like this:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2a365d03b211d0f53930958bcf200b18409b32b6.svg" style="height: 40px;" type="image/svg+xml"&gt;\[f&amp;#x27;(x)=\frac{df}{dx}\quad g&amp;#x27;(f)=\frac{dg}{df}\]&lt;/object&gt;
&lt;p&gt;Each of these is a function we can evaluate at some point; for example, we
denote the evaluation of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/9567f23affb8a7bd391269e4572b0200f9747cb8.svg" style="height: 19px;" type="image/svg+xml"&gt;f&amp;#x27;(x)&lt;/object&gt; at &lt;img alt="x_0" class="valign-m3" src="https://eli.thegreenplace.net/images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /&gt; as &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/5bf185ec15f166ab5130fbff525b5dc8a2d91546.svg" style="height: 23px;" type="image/svg+xml"&gt;\frac{df}{dx}(x_0)&lt;/object&gt;.
So we can rewrite the chain rule like this:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9ec7442f9adf5913013decd3f4959574375fec58.svg" style="height: 42px;" type="image/svg+xml"&gt;\[\frac{d(g \circ f)}{dx}(x_0)=\frac{dg}{df}(f(x_0))\frac{df}{dx}(x_0)\]&lt;/object&gt;
&lt;p&gt;Reverse mode AD means applying the chain rule to our computation graph, starting
with the last operation and ending at the first.
Remember that our final goal is to calculate:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e2774a76c051cd50d2dbf1ae13593fa7f1e773db.svg" style="height: 36px;" type="image/svg+xml"&gt;\[\frac{dS}{dx}(x_0)\]&lt;/object&gt;
&lt;p&gt;Where &lt;em&gt;S&lt;/em&gt; is a composition of multiple functions. The first composition we
unravel is the last node in the graph, where &lt;em&gt;v&lt;/em&gt; is calculated from &lt;em&gt;w&lt;/em&gt;. This is
the chain rule for it:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/234154080611fa2d641c60b299b89513aed5dad6.svg" style="height: 38px;" type="image/svg+xml"&gt;\[\frac{dS}{dw}=\frac{d(S \circ v)}{dw}(x_0)=\frac{dS}{dv}(v(x_0))\frac{dv}{dw}(x_0)\]&lt;/object&gt;
&lt;p&gt;The formula for &lt;em&gt;S&lt;/em&gt; is &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/f5dec59579550096a611d2ed38b64a0935219947.svg" style="height: 19px;" type="image/svg+xml"&gt;S(v)=v&lt;/object&gt;, so its derivative is 1. The formula for
&lt;em&gt;v&lt;/em&gt; is &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/15935505468369a4c3d1371b6092dd47fb843936.svg" style="height: 22px;" type="image/svg+xml"&gt;v(w)=\frac{1}{w}&lt;/object&gt;, so its derivative is &lt;object class="valign-m7" data="https://eli.thegreenplace.net/images/math/26a341f4d7d5266620d3ad953a2b3b6cf45d1b42.svg" style="height: 23px;" type="image/svg+xml"&gt;-\frac{1}{w^2}&lt;/object&gt;.
Substituting the value of &lt;em&gt;w&lt;/em&gt; computed in the forward pass, we get:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/f4fac2ae8c9004597295e076cb4bbabac5a364f1.svg" style="height: 45px;" type="image/svg+xml"&gt;\[\frac{dS}{dw}(x_0)=1\cdot\frac{-1}{w^2}\bigg\rvert_{w=1.61}=-0.39\]&lt;/object&gt;
&lt;p&gt;Continuing backwards from &lt;em&gt;v&lt;/em&gt; to &lt;em&gt;w&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b16fa07dd10b039349e4addc1948ffe7c1207bed.svg" style="height: 40px;" type="image/svg+xml"&gt;\[\frac{dS}{dg}(x_0)=\frac{dS}{dw}(x_0)\frac{dw}{dg}(x_0)\]&lt;/object&gt;
&lt;p&gt;We've already calculated &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/7f843eaca6c53d1341ffd2285f643d0f233d18cb.svg" style="height: 22px;" type="image/svg+xml"&gt;\frac{dS}{dw}(x_0)&lt;/object&gt; in the previous step. Since
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/f16c67e0ba9a5df1ffa4ebe6e9d13dcf9a9df0e1.svg" style="height: 16px;" type="image/svg+xml"&gt;w=1+g&lt;/object&gt;, we know that &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/93fc061d1af1e6125431be32a6d07be34ae82976.svg" style="height: 19px;" type="image/svg+xml"&gt;w&amp;#x27;(g)=1&lt;/object&gt;, so:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/bead5038f022559d95b89314911e713bc88d0a05.svg" style="height: 40px;" type="image/svg+xml"&gt;\[\frac{dS}{dg}(x_0)=-0.39\cdot1=-0.39\]&lt;/object&gt;
&lt;p&gt;Continuing similarly down the chain, until we get to the input &lt;em&gt;x&lt;/em&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e9b20f826ab6a143e8dc08f0e1d4bffdef1739f1.svg" style="height: 94px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{dS}{df}(x_0)&amp;amp;=\frac{dS}{dg}(x_0)\frac{dg}{df}(x_0)=-0.39\cdot e^f\bigg\rvert_{f=-0.5}=-0.24\\
  \frac{dS}{dx}(x_0)&amp;amp;=\frac{dS}{df}(x_0)\frac{df}{dx}(x_0)=-0.24\cdot -1=0.24
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We're done; the value of the derivative of the sigmoid function at &lt;object class="valign-0" data="https://eli.thegreenplace.net/images/math/b252e76d8a58888c218a5a4c2d463bdd8f0c0b20.svg" style="height: 13px;" type="image/svg+xml"&gt;x=0.5&lt;/object&gt;
is 0.24; this can be easily verified with a calculator using the analytical
derivative of this function.&lt;/p&gt;
&lt;p&gt;As you can see, this procedure is rather mechanical and it's not surprising that
it can be automated. Before we get to automation, however, let's review the
more common scenario where the computational graph is a DAG rather than a linear
chain.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="general-dags"&gt;
&lt;h2&gt;General DAGs&lt;/h2&gt;
&lt;p&gt;The sigmoid sample we worked though above has a very simple, linear
computational graph. Each node has a single predecessor and a single successor;
moreover, the function itself has a single input and single output. Therefore,
the single-variable chain rule is sufficient here.&lt;/p&gt;
&lt;p&gt;In the more general case, we'll encounter functions that have multiple inputs,
may also have multiple outputs &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;, and the internal nodes are connected in
non-linear patterns. To compute their derivatives, we have to use the
multivariate chain rule.&lt;/p&gt;
&lt;p&gt;As a reminder, in the most general case we're dealing with a function that has
&lt;em&gt;n&lt;/em&gt; inputs, denoted &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/ea64bb914bc3a768b4557a09b6e79badac50efb6.svg" style="height: 12px;" type="image/svg+xml"&gt;a=a_1,a_2\cdots a_n&lt;/object&gt;, and &lt;em&gt;m&lt;/em&gt;
outputs, denoted &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/97ca4dff67272bbafc9631d485c7687d07dffe90.svg" style="height: 16px;" type="image/svg+xml"&gt;f_1,f_2\cdots f_m&lt;/object&gt;. In other words, the function is
mapping &lt;img alt="f:\mathbb{R}^{n} \to \mathbb{R}^{m}" class="valign-m4" src="https://eli.thegreenplace.net/images/math/13f219789047343729036279bb11630db317d98d.png" style="height: 16px;" /&gt;.&lt;/p&gt;
&lt;p&gt;The partial derivative of output &lt;em&gt;i&lt;/em&gt; w.r.t. input &lt;em&gt;j&lt;/em&gt; at some point &lt;em&gt;a&lt;/em&gt; is:&lt;/p&gt;
&lt;img alt="\[D_j f_i(a)=\frac{\partial f_i}{\partial a_j}(a)\]" class="align-center" src="https://eli.thegreenplace.net/images/math/30881b5a92e45259714ba01c7a12fbf8f6c56109.png" style="height: 42px;" /&gt;
&lt;p&gt;Assuming &lt;em&gt;f&lt;/em&gt; is differentiable at &lt;em&gt;a&lt;/em&gt;, then the complete derivative of &lt;em&gt;f&lt;/em&gt;
w.r.t. its inputs can be represented by the &lt;em&gt;Jacobian matrix&lt;/em&gt;:&lt;/p&gt;
&lt;img alt="\[Df(a)=\begin{bmatrix} D_1 f_1(a) &amp;amp;amp; \cdots &amp;amp;amp; D_n f_1(a) \\ \vdots &amp;amp;amp;  &amp;amp;amp; \vdots \\ D_1 f_m(a) &amp;amp;amp; \cdots &amp;amp;amp; D_n f_m(a) \\ \end{bmatrix}\]" class="align-center" src="https://eli.thegreenplace.net/images/math/ab09367d48e9ef4d8bc2314a60313dec700193af.png" style="height: 76px;" /&gt;
&lt;p&gt;The multivariate chain rule then states that if we compose &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/fc2de39fe9cb2349e17b06356b230494023e2663.svg" style="height: 16px;" type="image/svg+xml"&gt;f\circ g&lt;/object&gt;
(and assuming all the dimensions are correct), the derivative is:&lt;/p&gt;
&lt;img alt="\[D(f \circ g)(a)=Df(g(a)) \cdot Dg(a)\]" class="align-center" src="https://eli.thegreenplace.net/images/math/00bdefa904bd34df2dfb50cc385e6497c4e5096e.png" style="height: 18px;" /&gt;
&lt;p&gt;This is the matrix multiplication of &lt;img alt="Df(g(a))" class="valign-m4" src="https://eli.thegreenplace.net/images/math/e567730c48bb2f95c258b630b4d6e997043e09ab.png" style="height: 18px;" /&gt; and &lt;img alt="Dg(a)" class="valign-m4" src="https://eli.thegreenplace.net/images/math/2575fc98e794a733a7aa6237fe67246a41e6c8c5.png" style="height: 18px;" /&gt;.&lt;/p&gt;
&lt;div class="section" id="linear-nodes"&gt;
&lt;h3&gt;Linear nodes&lt;/h3&gt;
&lt;p&gt;As a warmup, let's start with a linear node that has a single input and a single
output:&lt;/p&gt;
&lt;img alt="A single node f(x) with one input and one output" class="align-center" src="https://eli.thegreenplace.net/images/2025/linear-node.png" /&gt;
&lt;p&gt;In all these examples, we assume the full graph output is &lt;em&gt;S&lt;/em&gt;, and its
derivative by the node's outputs is
&lt;object class="valign-m9" data="https://eli.thegreenplace.net/images/math/f57c6269b575f0c225f18e0bef2f974a5a048802.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial f}&lt;/object&gt;.
We're then interested in finding &lt;object class="valign-m6" data="https://eli.thegreenplace.net/images/math/0367612a0ebda078dea5f14a1fe973802a982d52.svg" style="height: 23px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial x}&lt;/object&gt;.
Since since &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/77d5e13c951f5a369089f05a0505728896773ed3.svg" style="height: 16px;" type="image/svg+xml"&gt;f:\mathbb{R}\to\mathbb{R}&lt;/object&gt;, the Jacobian is just a scalar:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/507412aef50458ce55fd10110906db71979a548b.svg" style="height: 37px;" type="image/svg+xml"&gt;\[Df=\frac{\partial f}{\partial x}\]&lt;/object&gt;
&lt;p&gt;And the chain rule is:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/9d9fc987a32c5c653bccfce4a89ee860b14286ff.svg" style="height: 41px;" type="image/svg+xml"&gt;\[D(S\circ f)=DS(f)\cdot Df=\frac{\partial S}{\partial f}\frac{\partial f}{\partial x}\]&lt;/object&gt;
&lt;p&gt;No surprises so far - this is just the single variable chain rule!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fan-in"&gt;
&lt;h3&gt;Fan-in&lt;/h3&gt;
&lt;p&gt;Let's move on to the next scenario, where &lt;em&gt;f&lt;/em&gt; has two inputs:&lt;/p&gt;
&lt;img alt="A single node f(x1,x2) with two inputs and one output" class="align-center" src="https://eli.thegreenplace.net/images/2025/fan-in-node.png" /&gt;
&lt;p&gt;Once again, we already have the derivative &lt;object class="valign-m9" data="https://eli.thegreenplace.net/images/math/f57c6269b575f0c225f18e0bef2f974a5a048802.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial f}&lt;/object&gt;
available, and we're interested in finding the derivative of &lt;em&gt;S&lt;/em&gt; w.r.t. the
inputs.&lt;/p&gt;
&lt;p&gt;In this case, &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/20b92036dc896227ef062a8bbe58ba8c022805ab.svg" style="height: 19px;" type="image/svg+xml"&gt;f:\mathbb{R}^2\to\mathbb{R}&lt;/object&gt;, so the Jacobian is a 1x2
matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/0d81f11639f8d8506ca7bdc461d5e5d3a9cc2e5b.svg" style="height: 42px;" type="image/svg+xml"&gt;\[Df=\left [
  \frac{\partial f}{\partial x_1} \quad \frac{\partial f}{\partial x_2}
\right ]\]&lt;/object&gt;
&lt;p&gt;And the chain rule here means multiplying a 1x1 matrix by a 1x2 matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/057bba77db0d7abde54de3542704c4f5e0c32ff9.svg" style="height: 42px;" type="image/svg+xml"&gt;\[D(S\circ f)=DS(f)\cdot Df=
  \left [ \frac{\partial S}{\partial f} \right ]
  \left [ \frac{\partial f}{\partial x_1} \quad \frac{\partial f}{\partial x_2} \right ]
= \left [ \frac{\partial S}{\partial f} \frac{\partial f}{\partial x_1} \quad \frac{\partial S}{\partial f} \frac{\partial f}{\partial x_2} \right ]\]&lt;/object&gt;
&lt;p&gt;Therefore, we see that the output derivative propagates to each input
separately:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/781df9a4c6233932820bc19c4a6b155b86522aa5.svg" style="height: 87px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial S}{\partial x_1}&amp;amp;=\frac{\partial S}{\partial f} \frac{\partial f}{\partial x_1}\\
  \frac{\partial S}{\partial x_2}&amp;amp;=\frac{\partial S}{\partial f} \frac{\partial f}{\partial x_2}
\end{align*}\]&lt;/object&gt;
&lt;/div&gt;
&lt;div class="section" id="fan-out"&gt;
&lt;h3&gt;Fan-out&lt;/h3&gt;
&lt;p&gt;In the most general case, &lt;em&gt;f&lt;/em&gt; may have multiple inputs but its output may also
be used by more than one other node. As a concrete example, here's a node with
three inputs and an output that's used in two places:&lt;/p&gt;
&lt;img alt="A single node f(x1,x2,x3) with three inputs and two outputs" class="align-center" src="https://eli.thegreenplace.net/images/2025/fan-out-node.png" /&gt;
&lt;p&gt;While we denote each output edge from &lt;em&gt;f&lt;/em&gt; with a different name, &lt;em&gt;f&lt;/em&gt;
has a single output! This point is a bit subtle and important to dwell on:
yes, &lt;em&gt;f&lt;/em&gt; has a single output, so in the forward calculation both &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/0b35cc5e94a0d7682217d901f757b70990808891.svg" style="height: 16px;" type="image/svg+xml"&gt;f_1&lt;/object&gt;
and &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/9f68396d20ac17fe4705c1bf347774699bc27a3e.svg" style="height: 16px;" type="image/svg+xml"&gt;f_2&lt;/object&gt; will have the same value. However, we have to treat them
differently for the derivative calculation, because it's very possible that
&lt;object class="valign-m9" data="https://eli.thegreenplace.net/images/math/18c0d7d012453644b8448317709f2e54ebf0599c.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial f_1}&lt;/object&gt; and &lt;object class="valign-m9" data="https://eli.thegreenplace.net/images/math/ceaa592c34f5079817c6abcfa6fabf5b2a3ed061.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial S}{\partial f_2}&lt;/object&gt;
are different!&lt;/p&gt;
&lt;p&gt;In other words, we're reusing the machinery of multi-output functions here.
If &lt;em&gt;f&lt;/em&gt; had multiple outputs (e.g. a vector function), everything would work
exactly the same.&lt;/p&gt;
&lt;p&gt;In this case, since we treat &lt;em&gt;f&lt;/em&gt; as &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/c0ac304ca884452f91df6662e7f6919db48d69f9.svg" style="height: 19px;" type="image/svg+xml"&gt;f:\mathbb{R}^3\to\mathbb{R}^2&lt;/object&gt;,
its Jacobian is a 2x3 matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1a29aa7661d6293f39df5cf65181e04848ed6b82.svg" style="height: 75px;" type="image/svg+xml"&gt;\[Df=
\begin{bmatrix}
  \frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \frac{\partial f_1}{\partial x_3} \\ \\
  \frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \frac{\partial f_2}{\partial x_3} \\
\end{bmatrix}\]&lt;/object&gt;
&lt;p&gt;The Jacobian &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/a6de1a158410623c078f4274fe7c35e95fa36d98.svg" style="height: 19px;" type="image/svg+xml"&gt;DS(f)&lt;/object&gt; is a 1x2 matrix:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/7aae6e867bb005f41aed0591aee6d837dff93795.svg" style="height: 42px;" type="image/svg+xml"&gt;\[DS(f)=\left [ \frac{\partial S}{\partial f_1} \quad \frac{\partial S}{\partial f_2} \right ]\]&lt;/object&gt;
&lt;p&gt;Applying the chain rule:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/885af1372b513038aa1d6f52fc9e2d69e4e8f9a1.svg" style="height: 123px;" type="image/svg+xml"&gt;\[\begin{align*}
D(S\circ f)=DS(f)\cdot Df&amp;amp;=
\left [ \frac{\partial S}{\partial f_1} \quad \frac{\partial S}{\partial f_2} \right ]
  \begin{bmatrix}
  \frac{\partial f_1}{\partial x_1} &amp;amp; \frac{\partial f_1}{\partial x_2} &amp;amp; \frac{\partial f_1}{\partial x_3} \\ \\
  \frac{\partial f_2}{\partial x_1} &amp;amp; \frac{\partial f_2}{\partial x_2} &amp;amp; \frac{\partial f_2}{\partial x_3} \\
\end{bmatrix}\\
&amp;amp;=
\left [
\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_1}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_1}\qquad
\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_2}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_2}\qquad
\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_3}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_3}
\right ]
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Therefore, we have:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/5148bfc79c2c82a178c9a0818931405d8067befd.svg" style="height: 134px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial S}{\partial x_1}&amp;amp;=\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_1}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_1}\\
  \frac{\partial S}{\partial x_2}&amp;amp;=\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_2}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_2}\\
  \frac{\partial S}{\partial x_3}&amp;amp;=\frac{\partial S}{\partial f_1}\frac{\partial f_1}{\partial x_3}+\frac{\partial S}{\partial f_2}\frac{\partial f_2}{\partial x_3}
\end{align*}\]&lt;/object&gt;
&lt;p&gt;The key point here - which we haven't encountered before - is that the derivatives
through &lt;em&gt;f&lt;/em&gt; add up for each of its outputs (or for each copy of its output).
Qualitatively, it means that the sensitivity of &lt;em&gt;f&lt;/em&gt;'s input to the output is
the sum of its sensitivities across each output separately. This makes logical
sense, and mathematically it's just the consequence of the dot product inherent
in matrix multiplication.&lt;/p&gt;
&lt;p&gt;Now that we understand how reverse mode AD works for the more general case
of DAG nodes, let's work through a complete example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="general-dags-full-example"&gt;
&lt;h2&gt;General DAGs - full example&lt;/h2&gt;
&lt;p&gt;Consider this function (a sample used in the ADIMLAS paper):&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/1568c8ae7ffdb24afc990d937585b7a779e185b2.svg" style="height: 19px;" type="image/svg+xml"&gt;\[f(x_1, x_2)=ln(x_1)+x_1 x_2-sin(x_2)\]&lt;/object&gt;
&lt;p&gt;It has two inputs and a single output; once we decompose it to primitive
operations, we can represent it with the following computational graph &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="Computational graph of f as function of x_1 and x_2" class="align-center" src="https://eli.thegreenplace.net/images/2025/fpaper-graph.png" /&gt;
&lt;p&gt;As before, we begin by running the computation forward for the values of
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/c5ea3aabcca7242c957a64cb671c96944ec70bcc.svg" style="height: 12px;" type="image/svg+xml"&gt;x_1,x_2&lt;/object&gt; at which we're interested to find the derivative. Let's take
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a1e99f7b67c1a129f77df5026f3701a5cfbdcc3f.svg" style="height: 15px;" type="image/svg+xml"&gt;x_1=2&lt;/object&gt; and &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/c2025f310759a37b724da44e26ea744680a2743e.svg" style="height: 16px;" type="image/svg+xml"&gt;x_2=5&lt;/object&gt;:&lt;/p&gt;
&lt;img alt="Computational graph with forward calculation at 2, 5" class="align-center" src="https://eli.thegreenplace.net/images/2025/fpaper-graph-forward-calc.png" /&gt;
&lt;p&gt;Recall that our goal is to calculate &lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/92a7ef7129edaf480d777987e2b69306fff75f87.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial f}{\partial x_1}&lt;/object&gt;
and &lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/0af90c749150f76175dd2af7dc930774ab7579c0.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial f}{\partial x_2}&lt;/object&gt;. Initially we know that
&lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/dc5d153e1b607ee33620d8c97cc46edf61fa7696.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial f}{\partial v_5}=1&lt;/object&gt; &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Starting with the &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/ca513d085e9f35047a4c85c1759e63b9f3d23e5b.svg" style="height: 11px;" type="image/svg+xml"&gt;v_5&lt;/object&gt; node, let's use the fan-in formulas developed
earlier:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/b45f5a7e39c1f64899b95d2ea3ef7562401b3eeb.svg" style="height: 85px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial f}{\partial v_4}&amp;amp;=\frac{\partial f}{\partial v_5} \frac{\partial v_5}{\partial v_4}=1\cdot 1=1\\
  \frac{\partial f}{\partial v_3}&amp;amp;=\frac{\partial f}{\partial v_5} \frac{\partial v_5}{\partial v_3}=1\cdot -1=-1
\end{align*}\]&lt;/object&gt;
&lt;p&gt;Next, let's tackle &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/9a69ccb973264d203d460f9a7e0df867faf7e918.svg" style="height: 11px;" type="image/svg+xml"&gt;v_4&lt;/object&gt;. It also has a fan-in configuration, so we'll
use similar formulas, plugging in the value of &lt;object class="valign-m8" data="https://eli.thegreenplace.net/images/math/39e8a55c9a3ae7936e835846a6ed4c631922e039.svg" style="height: 26px;" type="image/svg+xml"&gt;\frac{\partial f}{\partial v_4}&lt;/object&gt; we've just
calculated:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/e735c4d3d5ef555607e8acde0f84310291277101.svg" style="height: 85px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial f}{\partial v_1}&amp;amp;=\frac{\partial f}{\partial v_4} \frac{\partial v_4}{\partial v_1}=1\cdot 1=1\\
  \frac{\partial f}{\partial v_2}&amp;amp;=\frac{\partial f}{\partial v_4} \frac{\partial v_4}{\partial v_2}=1\cdot 1=1
\end{align*}\]&lt;/object&gt;
&lt;p&gt;On to &lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/9b12bbf79036cb3e904f971fd86838db1dade1aa.svg" style="height: 12px;" type="image/svg+xml"&gt;v_1&lt;/object&gt;. It's a simple linear node, so:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/55bb2c892485048d8ccb9796017d2002cd205231.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial x_1}^{(1)}=\frac{\partial f}{\partial v_1} \frac{\partial v_1}{\partial x_1}=1\cdot \frac{1}{x_1}=0.5\]&lt;/object&gt;
&lt;p&gt;Note the (1) superscript though! Since &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/593f4cff5d4210d46e140db57bafc4f692493f76.svg" style="height: 11px;" type="image/svg+xml"&gt;x_1&lt;/object&gt; is a fan-out node, it will have
more than one contribution to its derivative; we've just computed the one from
&lt;object class="valign-m4" data="https://eli.thegreenplace.net/images/math/9b12bbf79036cb3e904f971fd86838db1dade1aa.svg" style="height: 12px;" type="image/svg+xml"&gt;v_1&lt;/object&gt;. Next, let's compute the one from &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/2e84f52c0f54659a1f533b25591adb924f2a4131.svg" style="height: 11px;" type="image/svg+xml"&gt;v_2&lt;/object&gt;. That's another
fan-in node:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/abb4615332fa4d17f90efc39ef6574223a66c901.svg" style="height: 94px;" type="image/svg+xml"&gt;\[\begin{align*}
  \frac{\partial f}{\partial x_1}^{(2)}&amp;amp;=\frac{\partial f}{\partial v_2} \frac{\partial v_2}{\partial x_1}=1\cdot x_2=5\\
  \frac{\partial f}{\partial x_2}^{(1)}&amp;amp;=\frac{\partial f}{\partial v_2} \frac{\partial v_2}{\partial x_2}=1\cdot x_1=2
\end{align*}\]&lt;/object&gt;
&lt;p&gt;We've calculated the other contribution to the &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/593f4cff5d4210d46e140db57bafc4f692493f76.svg" style="height: 11px;" type="image/svg+xml"&gt;x_1&lt;/object&gt; derivative, and the
first out of two contributions for the &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/a8728ff397f08f1999170f64ff5838333f755380.svg" style="height: 11px;" type="image/svg+xml"&gt;x_2&lt;/object&gt; derivative. Next, let's
handle &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/73db09ec63501c91f822ea29cefadf3bb9837084.svg" style="height: 11px;" type="image/svg+xml"&gt;v_3&lt;/object&gt;:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/6a9dce7fcdeee31cdf6dead194b8d3898b04db61.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial x_2}^{(2)}=\frac{\partial f}{\partial v_3} \frac{\partial v_3}{\partial x_2}=-1\cdot cos(x_2)=-0.28\]&lt;/object&gt;
&lt;p&gt;Finally, we're ready to add up the derivative contributions for the input arguments.
&lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/593f4cff5d4210d46e140db57bafc4f692493f76.svg" style="height: 11px;" type="image/svg+xml"&gt;x_1&lt;/object&gt; is a &amp;quot;fan-out&amp;quot; node, with two outputs. Recall from the section above
that we just sum their contributions:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/2905b796b72079d5677b85735f0e8715a5bfd0da.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial x_1}=\frac{\partial f}{\partial x_1}^{(1)}+\frac{\partial f}{\partial x_1}^{(2)}=0.5+5=5.5\]&lt;/object&gt;
&lt;p&gt;And:&lt;/p&gt;
&lt;object class="align-center" data="https://eli.thegreenplace.net/images/math/98bfd4fb2ad61c35ffc3d5e45ff26c0886829a6e.svg" style="height: 44px;" type="image/svg+xml"&gt;\[\frac{\partial f}{\partial x_2}=\frac{\partial f}{\partial x_2}^{(1)}+\frac{\partial f}{\partial x_2}^{(2)}=2-0.28=1.72\]&lt;/object&gt;
&lt;p&gt;And we're done! Once again, it's easy to verify - using a calculator and the
analytical derivatives of &lt;object class="valign-m5" data="https://eli.thegreenplace.net/images/math/c644ca35529294cbdae3f76b2dab55120f5cbdbf.svg" style="height: 19px;" type="image/svg+xml"&gt;f(x_1,x_2)&lt;/object&gt; - that these are the right
derivatives at the given points.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="backpropagation-in-ml-reverse-mode-ad-and-vjps"&gt;
&lt;h2&gt;Backpropagation in ML, reverse mode AD and VJPs&lt;/h2&gt;
&lt;p&gt;A quick note on reverse mode AD vs forward mode (please read the ADIMLAS paper
for much more details):&lt;/p&gt;
&lt;p&gt;Reverse mode AD is the approach commonly used for machine learning and neural
networks, because these tend to have a scalar &lt;em&gt;loss&lt;/em&gt; (or &lt;em&gt;error&lt;/em&gt;) output that we
want to minimize. In reverse mode, we have to run AD once per output, while in
forward mode we'd have to run it once per &lt;em&gt;input&lt;/em&gt;. Therefore, when the input
size is much larger than the output size (as is the case in NNs), reverse mode
is preferable.&lt;/p&gt;
&lt;p&gt;There's another advantage, and it relates to the term &lt;em&gt;vector-jacobian product&lt;/em&gt;
(VJP) that you will definitely run into once you start digging deeper in this
domain.&lt;/p&gt;
&lt;p&gt;The VJP is basically a fancy way of saying &amp;quot;using the chain rule in reverse mode
AD&amp;quot;. Recall that in the most general case, the multivariate chain rule is:&lt;/p&gt;
&lt;img alt="\[D(f \circ g)(a)=Df(g(a)) \cdot Dg(a)\]" class="align-center" src="https://eli.thegreenplace.net/images/math/00bdefa904bd34df2dfb50cc385e6497c4e5096e.png" style="height: 18px;" /&gt;
&lt;p&gt;However, in the case of reverse mode AD, we typically have a single output
from the full graph, so &lt;img alt="Df(g(a))" class="valign-m4" src="https://eli.thegreenplace.net/images/math/e567730c48bb2f95c258b630b4d6e997043e09ab.png" style="height: 18px;" /&gt; is a row vector. The chain rule
then means multiplying this row vector by a matrix representing the node's
jacobian. This is the &lt;em&gt;vector-jacobian product&lt;/em&gt;, and its output is another
row vector. Scroll back to the &lt;em&gt;Fan-out&lt;/em&gt; sample to see an example of this.&lt;/p&gt;
&lt;p&gt;This may not seem very profound so far, but it carries an important meaning in
terms of computational efficiency. For each node in the graph, we don't have
to store its complete jacobian; all we need is a function that takes a row
vector and produces the VJP. This is important because jacobians can be very
large and very sparse &lt;a class="footnote-reference" href="#footnote-4" id="footnote-reference-4"&gt;[4]&lt;/a&gt;. In practice, this means that when AD libraries
define the derivative of a computation node, they don't ask you to register
a complete jacobian for each operation, but rather a VJP.&lt;/p&gt;
&lt;p&gt;This also provides an additional way to think about the relative efficiency of
reverse mode AD for ML
applications; since a graph typically has many inputs (all the weights), and a
single output (scalar loss), accumulating from the end going backwards means the
intermediate products are VJPs that are row vectors; accumulating from the front
would mean multiplying full jacobians together, and the intermediate results
would be matrices &lt;a class="footnote-reference" href="#footnote-5" id="footnote-reference-5"&gt;[5]&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-simple-python-implementation-of-reverse-mode-ad"&gt;
&lt;h2&gt;A simple Python implementation of reverse mode AD&lt;/h2&gt;
&lt;p&gt;Enough equations, let's see some code! The whole point of AD is that it's
&lt;em&gt;automatic&lt;/em&gt;, meaning that it's simple to implement in a program. What follows
is the simplest implementation I could think of; it requires one to build
expressions out of a special type, which can then calculate gradients
automatically.&lt;/p&gt;
&lt;p&gt;Let's start with some usage samples; here's the Sigmoid calculation presented
earlier:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sigmoid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;xx = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, sigmoid = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;dsigmoid/dxx = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We begin by building the Sigmoid expression using &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; values (more on this
later). We can then run the &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; method on a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;, with an output
gradient of 1.0 and see that the gradient for &lt;tt class="docutils literal"&gt;xx&lt;/tt&gt; is 0.24, as calculated
before.&lt;/p&gt;
&lt;p&gt;Here's the expression we used for the DAG section:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;x1 = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, x2 = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, f = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;df/dx1 = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, df/dx2 = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once again, we build up the expression, then call &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; on the final value.
It will populate the &lt;tt class="docutils literal"&gt;gv&lt;/tt&gt; attributes of input &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;s with the derivatives
calculated w.r.t. these inputs.&lt;/p&gt;
&lt;p&gt;Let's see how &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; works. The high-level overview is:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; represents a node in the computational graph we've been discussing
in this post.&lt;/li&gt;
&lt;li&gt;Using operator overloading and custom math functions (like the &lt;tt class="docutils literal"&gt;exp&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;sin&lt;/tt&gt;
and &lt;tt class="docutils literal"&gt;log&lt;/tt&gt; seen in the samples above), when an expression is constructed
out of &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; values, we also build the computational graph in the
background. Each &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; has links to its &lt;em&gt;predecessors&lt;/em&gt; in the graph (the
other &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;s that feed into it).&lt;/li&gt;
&lt;li&gt;When the &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; method is called, it runs reverse mode AD through the
computational graph, using the chain rule.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's the &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; class:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;v&lt;/tt&gt; is the value (forward calculation) of this &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;. &lt;tt class="docutils literal"&gt;predecessors&lt;/tt&gt; is
the list of predecessors, each of this type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@dataclass&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;multiplier&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;
    &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Var&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Consider the &lt;tt class="docutils literal"&gt;v5&lt;/tt&gt; node in DAG sample, for example. It represents the
calculation &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;v4-v3&lt;/span&gt;&lt;/tt&gt;. The &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; representing &lt;tt class="docutils literal"&gt;v5&lt;/tt&gt; will have a list of
two predecessors, one for &lt;tt class="docutils literal"&gt;v4&lt;/tt&gt; and one for &lt;tt class="docutils literal"&gt;v3&lt;/tt&gt;. Each of these will
have a &amp;quot;multiplier&amp;quot; associated with it:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;For &lt;tt class="docutils literal"&gt;v3&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;Predecessor.var&lt;/tt&gt; points to the &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; representing &lt;tt class="docutils literal"&gt;v3&lt;/tt&gt;
and &lt;tt class="docutils literal"&gt;Predecessor.multiplier&lt;/tt&gt; is -1, since this is the derivative
of &lt;tt class="docutils literal"&gt;v5&lt;/tt&gt; w.r.t. &lt;tt class="docutils literal"&gt;v3&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Similarly, for &lt;tt class="docutils literal"&gt;v4&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;Predecessor.var&lt;/tt&gt; points to the &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; representing
&lt;tt class="docutils literal"&gt;v4&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;Predecessor.multiplier&lt;/tt&gt; is 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's see some overloaded operators of &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; &lt;a class="footnote-reference" href="#footnote-6" id="footnote-reference-6"&gt;[6]&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__add__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensure_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;

&lt;span class="c1"&gt;# ...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__mul__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensure_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And some of the custom math functions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;log(x) - natural logarithm of x&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensure_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;sin(x)&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ensure_var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Var&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Predecessor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note how the multipliers for each node are exactly the derivatives of its
output w.r.t. corresponding input. Notice also that in some cases we use the
forward calculated value of a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;'s inputs to calculate this derivative
(e.g. in the case of &lt;tt class="docutils literal"&gt;sin(x)&lt;/tt&gt;, the derivative is &lt;tt class="docutils literal"&gt;cos(x)&lt;/tt&gt;, so we need the
actual value of &lt;tt class="docutils literal"&gt;x&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;Finally, this is the &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gv&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;gv&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predecessors&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiplier&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;gv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Some notes about this method:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;It has to be invoked on a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; node that represents the entire computation.&lt;/li&gt;
&lt;li&gt;Since this function walks the graph backwards (from the outputs to the inputs),
this is the direction our graph edges are pointing (we keep track of the
predecessors of each node, not the successors).&lt;/li&gt;
&lt;li&gt;Since we typically want the derivative of some output &amp;quot;loss&amp;quot; w.r.t. each
&lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;, the computation will usually start with &lt;tt class="docutils literal"&gt;grad(1.0)&lt;/tt&gt;, because the
output of the entire computation &lt;em&gt;is&lt;/em&gt; the loss.&lt;/li&gt;
&lt;li&gt;For each node, &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; adds the incoming gradient to its own, and propagates
the incoming gradient to each of its predecessors, using the relevant
multiplier.&lt;/li&gt;
&lt;li&gt;The addition &lt;tt class="docutils literal"&gt;self.gv += gv&lt;/tt&gt; is key to managing nodes with fan-out. Recall
our discussion from the DAG section - according to the multivariate chain rule,
fan-out nodes' derivatives add up for each of their outputs.&lt;/li&gt;
&lt;li&gt;This implementation of &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; is very simplistic and inefficient because it
will process the same &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; multiple times in complex graphs. A more
efficient implementation would sort the graph topologically first and then
would only have to visit each &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; once.&lt;/li&gt;
&lt;li&gt;Since the gradient of each &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; adds up, one shouldn't be reusing &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;s
between different computations. Once &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; was run, the &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; should
not be used for other &lt;tt class="docutils literal"&gt;grad&lt;/tt&gt; calculations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The full code for this sample is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2025/rad/rad.py"&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The goal of this post is to serve as a supplement for the ADIMLAS paper; once
again, if the topic of AD is interesting to you, I strongly encourage you to
read the paper! I hope this post added something on top - please let me know
if you have any questions.&lt;/p&gt;
&lt;p&gt;Industrial strength implementations of AD, like &lt;a class="reference external" href="https://github.com/HIPS/autograd/"&gt;autograd&lt;/a&gt;
and &lt;a class="reference external" href="https://github.com/jax-ml/jax"&gt;JAX&lt;/a&gt;, have much better ergonomics and
performance than the toy implementation shown above. That said, the underlying
principles are similar - reverse mode AD on computational graphs. To explore
how such a system works, see my
&lt;a class="reference external" href="https://github.com/eliben/radgrad"&gt;radgrad&lt;/a&gt; project.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;In this post we're only looking at single-output graphs, however, since
these are typically sufficient in machine learning (the output is some
scalar &amp;quot;loss&amp;quot; or &amp;quot;error&amp;quot; that we're trying to minimize). That said,
for functions with multiple outputs the process is very similar - we just
have to run the reverse mode AD process for each output variable
separately.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Note that the notation here is a bit different from the one used for
the sigmoid function. This notation is adopted from the ADIMLAS paper,
which uses &lt;object class="valign-m3" data="https://eli.thegreenplace.net/images/math/f0cd317158e9b3b19134b2c5db4e0861fcd95222.svg" style="height: 11px;" type="image/svg+xml"&gt;v_i&lt;/object&gt; for all temporary values within the graph.
I'm keeping the notations different to emphasize they have absolutely
no bearing on the math and the AD algorithm. They're just a naming
convention.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;For consistency, I'll be using the partial derivative notation
throughout this example, even for nodes that have a single input and
output.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;For an example of gigantic, sparse jacobians see my older post
on &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/backpropagation-through-a-fully-connected-layer/"&gt;backpropagation through a fully connected layer&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;There are a lot of additional nuances here to explain; I strongly
recommend this &lt;a class="reference external" href="https://videolectures.net/videos/deeplearning2017_johnson_automatic_differentiation"&gt;excellent lecture&lt;/a&gt;
by &lt;a class="reference external" href="https://github.com/mattjj"&gt;Matthew Johnson&lt;/a&gt; (of JAX and autograd
fame) for a deeper overview.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-6"&gt;[6]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;These use the utility function &lt;tt class="docutils literal"&gt;ensure_var&lt;/tt&gt;; all it does is wrap the
its argument in a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt; if it's not already a &lt;tt class="docutils literal"&gt;Var&lt;/tt&gt;. This is needed
to wrap constants in the expression, to ensure that the computational
graph includes everything.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Math"></category><category term="Machine Learning"></category><category term="Python"></category></entry><entry><title>GoMLX: ML in Go without Python</title><link href="https://eli.thegreenplace.net/2024/gomlx-ml-in-go-without-python/" rel="alternate"></link><published>2024-11-22T07:00:00-08:00</published><updated>2024-11-22T15:00:29-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-11-22:/2024/gomlx-ml-in-go-without-python/</id><summary type="html">&lt;p&gt;In the &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/"&gt;previous post&lt;/a&gt;
I talked about running ML inference in Go through a Python sidecar process. In
this post, let's see how we can accomplish the same tasks without using Python
at all.&lt;/p&gt;
&lt;div class="section" id="how-ml-models-are-implemented"&gt;
&lt;h2&gt;How ML models are implemented&lt;/h2&gt;
&lt;p&gt;Let's start with a brief overview of how ML models are â€¦&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In the &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/"&gt;previous post&lt;/a&gt;
I talked about running ML inference in Go through a Python sidecar process. In
this post, let's see how we can accomplish the same tasks without using Python
at all.&lt;/p&gt;
&lt;div class="section" id="how-ml-models-are-implemented"&gt;
&lt;h2&gt;How ML models are implemented&lt;/h2&gt;
&lt;p&gt;Let's start with a brief overview of how ML models are implemented
under the hood &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;. The model is typically written in Python, using one of the
ML frameworks like TensorFlow, JAX or PyTorch. The framework takes care
of at least 2 high-level concerns for developers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Expressive way to describe the model architecture, including
auto-differentiation for training.&lt;/li&gt;
&lt;li&gt;Efficient implementation of computational primitives on common HW: CPUs,
GPUs and TPUs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In-between these two concerns there exists a standardized model definition
format (or several) that helps multiple tools interoperate. While it's by no
means the only solution &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;, let's look at the &lt;a class="reference external" href="https://openxla.org/"&gt;OpenXLA stack&lt;/a&gt; as a way to run models on diverse hardware:&lt;/p&gt;
&lt;img alt="OpenXLA architectural diagram, with a gopher" class="align-center" src="https://eli.thegreenplace.net/images/2024/openxla-diagram-with-gopher.png" /&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The top layer are the frameworks that provide high-level primitives to define
ML models, and translate them to
a common interchange format called &lt;a class="reference external" href="https://github.com/openxla/stablehlo"&gt;StableHLO&lt;/a&gt; (where &amp;quot;HLO&amp;quot; stands for High-Level
Operations). I've added the gopher on the very right - it will soon become
clear why.&lt;/li&gt;
&lt;li&gt;The bottom layer is the HW that executes these models efficiently.&lt;/li&gt;
&lt;li&gt;In the middle is the OpenXLA system, which includes two major components:
the XLA compiler translating HLO to HW machine code, and &lt;a class="reference external" href="https://opensource.googleblog.com/2023/05/pjrt-simplifying-ml-hardware-and-framework-integration.html"&gt;PJRT&lt;/a&gt; -
the &lt;em&gt;runtime&lt;/em&gt; component responsible for managing HW devices, moving data
(tensors) between the host CPU and these devices, executing tasks, sharding and
so on.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There's a huge amount of complexity hidden by the bottom layers of this diagram.
Efficient compilation and code generation for diverse HW - including using fixed
blocks and libraries (like cuDNN), runtime management etc. All of this is really
something one shouldn't try to re-implement unless there's a really, &lt;em&gt;really&lt;/em&gt;
good reason to do so. And the best part? There's no Python there - this is
C and C++; Python only exists on the upper layer - in the high-level ML
frameworks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="gomlx"&gt;
&lt;h2&gt;GoMLX&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/gomlx"&gt;GoMLX&lt;/a&gt; is a relatively new Go package for ML
that deserves some attention. GoMLX slots in as one of the frameworks,
exactly where the Gopher is in the diagram above &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;. This is absolutely the
right approach to the problem. There's no point in re-implementing the
low-level primitives - whatever works for TF and JAX will work for Go as well!
Google, NVIDIA, Intel and several other companies invest huge resources into
these systems, and it's a good idea to benefit from these efforts.&lt;/p&gt;
&lt;p&gt;In this post I will showcase re-implementations of some of the samples from
the &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/"&gt;previous post&lt;/a&gt;,
but with no Python in sight. But first, a few words about what GoMLX does.&lt;/p&gt;
&lt;p&gt;GoMLX should be familiar if you've used one of the popular Python ML frameworks.
You build a computational graph representing your model - the usual operations
are supported and sufficient to implement anything from linear regression to
cutting-edge transformers. Since GoMLX wraps XLA, it has access to all the same
building blocks TF and JAX use (and it adds its own higher-level primitives,
similarly to the Python frameworks).&lt;/p&gt;
&lt;p&gt;GoMLX supports &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation"&gt;automatic differentiation&lt;/a&gt; to
create the &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/backpropagation-through-a-fully-connected-layer/"&gt;backward propagation&lt;/a&gt;
operations required to update weights in training. It also provides many helpers
for training and keeping track of progress, as well as &lt;a class="reference external" href="https://github.com/janpfeifer/gonb"&gt;Jupyter notebook support&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="an-image-model-for-the-cifar-10-dataset-with-gomlx"&gt;
&lt;h2&gt;An image model for the CIFAR-10 dataset with GoMLX&lt;/h2&gt;
&lt;p&gt;In the &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/"&gt;previous post&lt;/a&gt;
we built a CNN (convolutional neural network) model using TF+Keras in Python,
and ran its inference in a sidecar process we could control from Go.&lt;/p&gt;
&lt;p&gt;Here, let's build a similar model in Go, without using Python at all; we'll
be training it on the same &lt;a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html"&gt;CIFAR-10 dataset&lt;/a&gt;
we've used before.&lt;/p&gt;
&lt;img alt="CIFAR-10 dataset sample" class="align-center" src="https://eli.thegreenplace.net/images/2024/cifar10.png" /&gt;
&lt;p&gt;The full code for this sample &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-mlx/cifar-cnn"&gt;is here&lt;/a&gt;;
it is heavily based on GoMLX's
&lt;a class="reference external" href="https://github.com/gomlx/gomlx/tree/main/examples/cifar"&gt;own example&lt;/a&gt;, with
some modifications for simplicity and clarity. Here's the code defining the
model graph:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;C10ConvModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;mlxcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Context&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;spec&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;any&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;inputs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Node&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Node&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;batchedImages&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;g&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;batchedImages&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;dtype&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;batchedImages&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;DType&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;batchedImages&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Shape&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;Dimensions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;batchedImages&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;layerIdx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;mlxcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Context&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;newCtx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Inf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%03d_%s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layerIdx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;layerIdx&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newCtx&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// Convolution / activation layers&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;conv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Filters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;KernelSize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;PadSame&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;AssertDims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;activations&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;conv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Filters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;KernelSize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;PadSame&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;activations&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MaxPool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Window&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;DropoutNormalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dropout&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;AssertDims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;conv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Filters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;KernelSize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;PadSame&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;AssertDims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;activations&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;conv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Filters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;KernelSize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;PadSame&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;AssertDims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;activations&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MaxPool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Window&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;DropoutNormalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dropout&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;AssertDims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;conv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Filters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;KernelSize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;PadSame&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;AssertDims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;activations&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Convolution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;conv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Filters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;KernelSize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;PadSame&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;AssertDims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;activations&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;MaxPool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Window&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;DropoutNormalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dropout&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;AssertDims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// Flatten logits, and apply dense layer&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;batchSize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dense&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;activations&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;DropoutNormalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dropout&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;numClasses&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nextCtx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dense&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;numClasses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Node&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you might expect, the Go code is longer and more explicit (nodes are threaded
explicitly between builder calls, instead of being magically accumulated). It's
not hard to envision a Keras-like high level library on top of this.&lt;/p&gt;
&lt;p&gt;Here's a snippet from &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/go-mlx/cifar-cnn/classify-cnn/classify-cnn.go"&gt;the classifier (inference)&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flagCheckpoint&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;String&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;checkpoint&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Directory to load checkpoint from&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Parse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mlxcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;backend&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;backends&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;checkpoints&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Dir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;flagCheckpoint&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Done&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;panic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Reuse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;// helps sanity check the loaded context&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;exec&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mlxcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;NewExec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;backend&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;In&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;mlxcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Context&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;image&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Node&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Node&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Convert our image to a tensor with batch dimension of size 1, and pass&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// it to the C10ConvModel graph.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;image&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ExpandAxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;// Create a batch dimension of size 1.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;cnnmodel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;C10ConvModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;mlxctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Node&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nx"&gt;image&lt;/span&gt;&lt;span class="p"&gt;})[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Take the class with highest logit value, then remove the batch dimension.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;choice&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ArgMax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// classify takes a 32x32 image and returns a Cifar-10 classification according&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// to the models. Use C10Labels to convert the returned class to a string&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// name. The returned class is from 0 to 9.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;classify&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;img&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;image&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int32&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;images&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;dtypes&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Float32&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;Single&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;outputs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;exec&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;classID&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;tensors&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ToScalar&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="nx"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;classID&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// ...&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now &lt;tt class="docutils literal"&gt;classify&lt;/tt&gt; is a function that takes an &lt;a class="reference external" href="https://pkg.go.dev/image#Image"&gt;image.Image&lt;/a&gt; and runs it through the network, returning
the index of the most likely label out of the list of CIFAR-10 labels.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/go-mlx/cifar-cnn/README.md"&gt;README file&lt;/a&gt;
in the sample explains how to run it locally on a GPU; the
model trains and runs successfully, with similar results to the TF+Keras
model we trained in Python earlier.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="gemma2-with-gomlx"&gt;
&lt;h2&gt;Gemma2 with GoMLX&lt;/h2&gt;
&lt;p&gt;For a (much) more involved example, GoMLX has a full implementation of
&lt;a class="reference external" href="https://github.com/gomlx/gemma"&gt;Gemma2 inference&lt;/a&gt;. The model implementation
itself is in the &lt;a class="reference external" href="https://github.com/gomlx/gemma/tree/main/transformers"&gt;transformers package&lt;/a&gt;.
It should look fairly familiar if you've seen a transformer implementation in
another language.&lt;/p&gt;
&lt;p&gt;The official example in that repository shows how to run it with weights
downloaded from HuggingFace; since I've already downloaded the &lt;a class="reference external" href="https://www.kaggle.com/models/google/gemma-2"&gt;Gemma2 weights
from Kaggle&lt;/a&gt; for the &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/"&gt;previous
post&lt;/a&gt;,
here's a simple adaptation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flagDataDir&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;String&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dir with converted weights&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flagVocabFile&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;String&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;vocab&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tokenizer vocabulary file&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Parse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// Load model weights from the checkpoint downloaded from Kaggle.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;kaggle&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ReadConvertedWeights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;flagDataDir&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// Load tokenizer vocabulary.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;sentencepiece&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;NewFromPath&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;flagVocabFile&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// Create a Gemma sampler and start sampling tokens.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;sampler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;samplers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;backends&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatalf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%+v&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;start&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;sampler&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Sample&lt;/span&gt;&lt;span class="p"&gt;([]&lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Are bees and wasps similar?&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatalf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%+v&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;\tElapsed time: %s\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Since&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;start&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Generated text:\n%s\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;strings&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;\n\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complete code together with installation and setup instructions
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-mlx/gemma"&gt;is here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;gomlx/gemma&lt;/tt&gt; demonstrates that GoMLX has sufficiently advanced capabilities
to run a real production-grade open LLM, without Python in the loop.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/"&gt;previous post&lt;/a&gt;
discussed some options for incorporating ML inference into a Go project via
a minimal Python sidecar process. Here, we take it a step further and
implement ML inference in Go without using Python. We do so by leveraging
GoMLX, which itself relies on XLA and PJRT to do the heavy lifting.&lt;/p&gt;
&lt;p&gt;If we strip down a framework like TensorFlow to its layers, GoMLX reuses the
bottom layers (which is where most of the magic lies), and replaces the
model builder library with a Go variant.&lt;/p&gt;
&lt;p&gt;Since GoMLX is still a relatively new project, it may be a little risky for
production uses at this point. That said, I find this direction very promising
and will be following the project's development with interest.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The full code for the samples in this post is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-mlx"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;This assumes you know the basics of neural network graphs,
their training, etc. If not, check out &lt;a class="reference external" href="https://eli.thegreenplace.net/2018/understanding-how-to-implement-a-character-based-rnn-language-model/"&gt;this post&lt;/a&gt;
and some of my other posts in the &lt;a class="reference external" href="https://eli.thegreenplace.net/tag/machine-learning"&gt;Machine Learning category&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;It's likely the most common production solution, and pretty much the only
way to access Google's TPUs.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;It does so by including Go bindings for both XLA and PJRT; these are
wrapped in higher-level APIs for users.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category><category term="Python"></category></entry><entry><title>ML in Go with a Python sidecar</title><link href="https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/" rel="alternate"></link><published>2024-11-11T06:13:00-08:00</published><updated>2024-11-11T14:29:57-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-11-11:/2024/ml-in-go-with-a-python-sidecar/</id><summary type="html">&lt;p&gt;Machine learning models are rapidly becoming more capable; how can we make
use of these powerful new tools in our Go applications?&lt;/p&gt;
&lt;p&gt;For top-of-the-line commercial LLMs like ChatGPT, Gemini or Claude, the models
are exposed as language agnostic REST APIs. We can hand-craft
HTTP requests or use client libraries (SDKs â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Machine learning models are rapidly becoming more capable; how can we make
use of these powerful new tools in our Go applications?&lt;/p&gt;
&lt;p&gt;For top-of-the-line commercial LLMs like ChatGPT, Gemini or Claude, the models
are exposed as language agnostic REST APIs. We can hand-craft
HTTP requests or use client libraries (SDKs) provided by the LLM vendors.
If we need more customized solutions, however, some challenges arise. Completely
bespoke models are typically trained in Python using tools like TensorFlow,
JAX or PyTorch that don't have real non-Python alternatives.&lt;/p&gt;
&lt;p&gt;In this post, I will present some approaches for Go developers to use ML models
in their applications - with increasing level of customization. The summary up
front is that it's pretty easy, and we only have to deal with Python very
minimally, if at all - depending on the circumstances.&lt;/p&gt;
&lt;img alt="Go gopher with Python logo inside" class="align-center" src="https://eli.thegreenplace.net/images/2024/gopherpythonlogo.png" style="width: 219px;" /&gt;
&lt;div class="section" id="internet-llm-services"&gt;
&lt;h2&gt;Internet LLM services&lt;/h2&gt;
&lt;p&gt;This is the easiest category: multimodal services from Google, OpenAI
and others are available as REST APIs with convenient client libraries for
most leading languages (including Go), as well as third-party packages that
provide abstractions on top (e.g. &lt;a class="reference external" href="https://github.com/tmc/langchaingo"&gt;langchaingo&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Check out the official Go blog titled &lt;a class="reference external" href="https://go.dev/blog/llmpowered"&gt;Building LLM-powered applications in
Go&lt;/a&gt; that was published earlier this year.
I've written about it before on this blog as well:
&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemini-cli-access-gemini-models-from-the-command-line/"&gt;#1&lt;/a&gt;,
&lt;a class="reference external" href="https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/"&gt;#2&lt;/a&gt;,
&lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-gemini-models-from-go/"&gt;#3&lt;/a&gt; etc.&lt;/p&gt;
&lt;p&gt;Go is typically as well supported as other programming languages in this domain;
in fact, it's uniquely powerful for such applications because of its
network-native nature; quoting from the Go blog post:&lt;/p&gt;
&lt;blockquote&gt;
Working with LLM services often means sending REST or RPC requests to a
network service, waiting for the response, sending new requests to other
services based on that and so on. Go excels at all of these, providing great
tools for managing concurrency and the complexity of juggling network
services.&lt;/blockquote&gt;
&lt;p&gt;Since this has been covered extensively, let's move on to the more challenging
scenarios.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="locally-running-llms"&gt;
&lt;h2&gt;Locally-running LLMs&lt;/h2&gt;
&lt;p&gt;There's a plethora of high-quality open models &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt; one can choose from to
run locally: Gemma, Llama, Mistral and many more. While these models aren't
quite as capable as the strongest commercial LLM services, they are often
surprisingly good and have clear benefits w.r.t. cost and privacy.&lt;/p&gt;
&lt;p&gt;The industry has begun standardizing on some common formats for shipping and
sharing these models - e.g. GGUF from &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;,
&lt;a class="reference external" href="https://huggingface.co/docs/safetensors/en/index"&gt;safetensors from Hugging Face&lt;/a&gt;
or the older &lt;a class="reference external" href="https://github.com/onnx/"&gt;ONNX&lt;/a&gt;.
Additionally, there are a number of excellent OSS tools that let us run such
models locally and expose a REST API for an experience that's very similar to
the OpenAI or Gemini APIs, including dedicated client libraries.&lt;/p&gt;
&lt;p&gt;The best known such tool is probably &lt;a class="reference external" href="https://ollama.com/"&gt;Ollama&lt;/a&gt;; I've
written extensively about it in the past:
&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/the-life-of-an-ollama-prompt/"&gt;#1&lt;/a&gt;,
&lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/"&gt;#2&lt;/a&gt;,
&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/"&gt;#3&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="Internals of ollama, showing service connecting to clients and loading GGUF" class="align-center" src="https://eli.thegreenplace.net/images/2024/ollama-internals.png" /&gt;
&lt;p&gt;Ollama lets us customize an LLM through a &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md"&gt;Modelfile&lt;/a&gt;,
which includes things like setting model parameters, system prompts etc. If we
fine-tuned a model &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;, it can also be loaded into Ollama by specifying our
own GGUF file.&lt;/p&gt;
&lt;p&gt;If you're running in a cloud environment, some vendors already have
off-the-shelf solutions like &lt;a class="reference external" href="https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama"&gt;GCP's Cloud Run integration&lt;/a&gt; that
may be useful.&lt;/p&gt;
&lt;p&gt;Ollama isn't the only player in this game, either; recently a new tool
emerged with a slightly different approach. &lt;a class="reference external" href="https://github.com/Mozilla-Ocho/llamafile"&gt;Llamafile&lt;/a&gt;
distributes the entire model as a single binary, which is portable across
several OSes and CPU architectures. Like Ollama, it provides REST APIs for the
model.&lt;/p&gt;
&lt;p&gt;If such a customized LLM is a suitable solution for your project, consider just
running Ollama or Llamafile and using their REST APIs to communicate with the
model. If you need higher degrees of customization, read on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-note-about-the-sidecar-pattern"&gt;
&lt;h2&gt;A note about the sidecar pattern&lt;/h2&gt;
&lt;p&gt;Before we proceed, I want to briefly discuss the &lt;a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/"&gt;sidecar pattern&lt;/a&gt;
of application deployment. That k8s link talks about &lt;em&gt;containers&lt;/em&gt;, but the
pattern isn't limited to these. It applies to any software architecture in which
functionality is isolated across processes.&lt;/p&gt;
&lt;p&gt;Suppose we have an application that requires some library functionality; using
Go as an example, we could find &lt;a class="reference external" href="https://pkg.go.dev/"&gt;an appropriate package&lt;/a&gt;, import it and be on our way. Suppose there's no
suitable Go package, however. If libraries exist with a C interface, we could
alternatively use &lt;a class="reference external" href="https://go.dev/blog/cgo"&gt;cgo&lt;/a&gt; to import it.&lt;/p&gt;
&lt;p&gt;But say there's no C API either, for example if the functionality is only
provided by a language without a convenient exported interface. Maybe it's
in Lisp, or Perl, or... Python.&lt;/p&gt;
&lt;p&gt;A very general solution could be to wrap the code we need in some kind of server
interface and run it as a separate process; this kind of process is called a
&lt;em&gt;sidecar&lt;/em&gt; - it's launched specifically to provide additional functionality for
another process. Whichever inter-process communication (IPC) mechanism we use,
the benefits of this approach are many - isolation, security, language
independence, etc. In today's world of containers and orchestration this
approach is becoming increasingly more common; this is why many of the links
about sidecars lead to k8s and other containerized solutions.&lt;/p&gt;
&lt;img alt="Depiction of a motorcycle with a Gopher, with Python in a sidecar" class="align-center" src="https://eli.thegreenplace.net/images/2024/sidecar-go-py.png" style="width: 256px;" /&gt;
&lt;p&gt;The Ollama approach outlined in the previous section is one example of using
the sidecar pattern. Ollama provides us with LLM functionality but it runs as
a server in its own process.&lt;/p&gt;
&lt;p&gt;The solutions presented in the rest of this post are more explicit and fully
worked-out examples of using the sidecar pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="locally-running-llm-with-python-and-jax"&gt;
&lt;h2&gt;Locally-running LLM with Python and JAX&lt;/h2&gt;
&lt;p&gt;Suppose none of the existing open LLMs will do for our project, even
fine-tuned. At this point we can consider training our own LLM - this
is hugely expensive, but perhaps there's no choice. Training usually involves
one of the large ML frameworks like TensorFlow, JAX or PyTorch. In this section
I'm not going to talk about how to train models; instead, I'll show how to run
local inference of an already trained model - in Python with JAX, and use that
as a sidecar server for a Go application.&lt;/p&gt;
&lt;p&gt;The sample (&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml/jax-gemma-server"&gt;full code is here&lt;/a&gt;)
is based on the
&lt;a class="reference external" href="https://github.com/google-deepmind/gemma/"&gt;official Gemma repository&lt;/a&gt;,
using its &lt;em&gt;sampler&lt;/em&gt; library &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;. It comes with a README that explains how
to set everything up. This is the relevant code instantiating a Gemma
sampler:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Once initialized, this will hold a sampler_lib.Sampler instance that&lt;/span&gt;
&lt;span class="c1"&gt;# can be used to generate text.&lt;/span&gt;
&lt;span class="n"&gt;gemma_sampler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;initialize_gemma&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Initialize Gemma sampler, loading the model into the GPU.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;model_checkpoint&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;MODEL_CHECKPOINT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model_tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;MODEL_TOKENIZER&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;params_lib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_and_format_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_checkpoint&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Parameters loaded&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;vocab&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SentencePieceProcessor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_tokenizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;transformer_config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transformer_lib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TransformerConfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;cache_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;transformer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transformer_lib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Transformer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;transformer_config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;global&lt;/span&gt; &lt;span class="n"&gt;gemma_sampler&lt;/span&gt;
    &lt;span class="n"&gt;gemma_sampler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sampler_lib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sampler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;transformer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transformer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;transformer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Sampler ready&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The model weights and tokenizer vocabulary are files downloaded
&lt;a class="reference external" href="https://www.kaggle.com/models/google/gemma"&gt;from Kaggle&lt;/a&gt;, per the
instructions in the Gemma repository README.&lt;/p&gt;
&lt;p&gt;So we have LLM inference up and running in Python; how do we use it from
Go?&lt;/p&gt;
&lt;p&gt;Using a sidecar, of course. Let's whip up a quick web server around this model
and expose a trivial REST interface on a local port that Go (or any other
tool) can talk to. As an example, I've set up a Flask-based web server around
this inference code. The web server is invoked with &lt;a class="reference external" href="https://gunicorn.org/"&gt;gunicorn&lt;/a&gt; - see the
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/go-py-ml/jax-gemma-server/run-gemma-server.sh"&gt;shell script&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;Excluding the imports, here's the entire application code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;create_app&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# Create an app and perform one-time initialization of Gemma.&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;app_context&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;initialize_gemma&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;app&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_app&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Route for simple echoing / smoke test.&lt;/span&gt;
&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/echo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;methods&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;POST&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;echo&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;prompt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prompt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;echo_prompt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;prompt&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;


&lt;span class="c1"&gt;# The real route for generating text.&lt;/span&gt;
&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/prompt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;methods&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;POST&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;prompt&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;prompt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prompt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# For total_generation_steps, 128 is a default taken from the Gemma&lt;/span&gt;
    &lt;span class="c1"&gt;# sample. It&amp;#39;s a tradeoff between speed and quality (higher values mean&lt;/span&gt;
    &lt;span class="c1"&gt;# better quality but slower generation).&lt;/span&gt;
    &lt;span class="c1"&gt;# The user can override this value by passing a &amp;quot;sampling_steps&amp;quot; key in&lt;/span&gt;
    &lt;span class="c1"&gt;# the request JSON.&lt;/span&gt;
    &lt;span class="n"&gt;sampling_steps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sampling_steps&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;sampled_str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gemma_sampler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;input_strings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prompt&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;total_generation_steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sampling_steps&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;response&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sampled_str&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The server exposes two routes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;prompt&lt;/tt&gt;: a client sends in a textual prompt, the server runs Gemma
inference and returns the generated text in a JSON response&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;echo&lt;/tt&gt;: used for testing and benchmarking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's how it all looks tied together:&lt;/p&gt;
&lt;img alt="Flask server wrapping Gemma sampling and exposing REST" class="align-center" src="https://eli.thegreenplace.net/images/2024/jax-gemma-server.png" style="width: 500px;" /&gt;
&lt;p&gt;The important takeaway is that this is just an example. Literally any part of
this setup can be changed: one could use a different ML library (maybe PyTorch
instead of JAX); one could use a different model (not Gemma, not even an LLM)
and one can use a different setup to build a web server around it. There are
many options, and each developer will choose what fits their project best.&lt;/p&gt;
&lt;p&gt;It's also worth noting that we've written less than 100 lines of Python code
in total - much of it piecing together snippets from tutorials. This tiny amount
of Python code is sufficient to wrap an HTTP server with a simple REST interface
around an LLM running locally through JAX on the GPU. From here on, we're safely
back in our application's actual business logic and Go.&lt;/p&gt;
&lt;p&gt;Now, a word about performance. One of the concerns developers may have with
sidecar-based solutions is the performance overhead of IPC
between Python and Go. I've added a simple &lt;tt class="docutils literal"&gt;echo&lt;/tt&gt; endpoint to measure this
effect; take a look at the &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml/jax-gemma-server/measure-request-latency"&gt;Go client that exercises it&lt;/a&gt;;
on my machine the
latency of sending a JSON request from Go to the Python server and getting back
the echo response is about 0.35 ms on average. Compared to the time it takes
Gemma to process a prompt and return a response (typically measured in seconds,
or maybe hundreds of milliseconds on very powerful GPUs), this is entirely
negligible.&lt;/p&gt;
&lt;p&gt;That said, not every custom model you may need to run is a full-fledged LLM.
What if your model is small and fast, and the overhead of 0.35 ms becomes
significant? Worry not, it can be optimized. This is the topic of the next
section.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="locally-running-fast-image-model-with-python-and-tensorflow"&gt;
&lt;h2&gt;Locally-running fast image model with Python and TensorFlow&lt;/h2&gt;
&lt;p&gt;The final sample of this post mixes things up a bit:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;We'll be using a simple image model (instead of an LLM)&lt;/li&gt;
&lt;li&gt;We're going to train it ourselves using TensorFlow+Keras (instead of JAX)&lt;/li&gt;
&lt;li&gt;We'll use a different IPC method between the Python sidecar
server and clients (instead of HTTP+REST)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model is still implemented in Python, and it's still driven as a sidecar
server process by a Go client &lt;a class="footnote-reference" href="#footnote-4" id="footnote-reference-4"&gt;[4]&lt;/a&gt;. The idea here is to show the versatility of
the sidecar approach, and to demonstrate a lower-latency way to communicate
between the processes.&lt;/p&gt;
&lt;p&gt;The full code of the sample &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml/tf-cifar-server"&gt;is here&lt;/a&gt;.
It trains a simple CNN
(convolutional neural network) to classify images from the
&lt;a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html"&gt;CIFAR-10 dataset&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="CIFAR-10 dataset sample" class="align-center" src="https://eli.thegreenplace.net/images/2024/cifar10.png" /&gt;
&lt;p&gt;The neural net setup with TensorFlow and Keras was taken from an official
tutorial. Here's the entire network definition:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPooling2D&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPooling2D&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;CIFAR-10 images are 32x32 pixels, each pixel being 3 values for red, green
and blue. In the original dataset, these values are bytes in the inclusive
range 0-255 representing color intensity. This should explain the
&lt;tt class="docutils literal"&gt;(32, 32, 3)&lt;/tt&gt; shape appearing in the code. The full code for training the
model is in the &lt;tt class="docutils literal"&gt;train.py&lt;/tt&gt; file in the sample; it runs for a bit and saves the
serialized model along with the trained weights into a local file.&lt;/p&gt;
&lt;p&gt;The next component is an &amp;quot;image server&amp;quot;: it loads the trained model+weights
file from disk and runs inference on images passed into it, returning the
label the model thinks is most likely for each.&lt;/p&gt;
&lt;p&gt;The server doesn't use HTTP and REST, however. It creates a
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Unix_domain_socket"&gt;Unix domain socket&lt;/a&gt;
and uses a simple length-prefix encoding protocol to communicate:&lt;/p&gt;
&lt;img alt="Length-prefix packet format" class="align-center" src="https://eli.thegreenplace.net/images/2024/length-prefix-packet.png" style="width: 663px;" /&gt;
&lt;p&gt;Each packet starts with a 4-byte field that specifies the length of the rest
of the contents. A type is a single byte, and the body can be anything &lt;a class="footnote-reference" href="#footnote-5" id="footnote-reference-5"&gt;[5]&lt;/a&gt;.
In the sample image server two commands are currently supported:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;0 means &amp;quot;echo&amp;quot; - the server will respond with the same packet back to
the client. The contents of the packet body are immaterial.&lt;/li&gt;
&lt;li&gt;1 means &amp;quot;classify&amp;quot; - the packet body is interpreted as a 32x32 RGB image,
encoded as the red channel for each pixel in the first 1024 bytes (32x32,
&lt;a class="reference external" href="https://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays"&gt;row major&lt;/a&gt;),
then green in the next 1024 bytes and finally blue in the last 1024
bytes. Here the server will run the image through the model, and reply with
the label the model thinks describes the image.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The sample also includes &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml/tf-cifar-server/client"&gt;a simple Go client&lt;/a&gt;
that can take a PNG file
from disk, encode it in the required format and send it over the domain socket
to the server, recording the response.&lt;/p&gt;
&lt;p&gt;The client can also be used to benchmark the latency of a roundtrip message
exchange. It's easier to just show the code instead of explaining what it does:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;runBenchmark&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Conn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;numIters&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;// Create a []byte with 3072 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;body&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;make&lt;/span&gt;&lt;span class="p"&gt;([]&lt;/span&gt;&lt;span class="kt"&gt;byte&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3072&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;body&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;body&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;byte&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;t1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;numIters&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;sendPacket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;messageTypeEcho&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;body&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;cmd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;readPacket&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;cmd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;||&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;body&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;bad response&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;elapsed&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Since&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;t1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Num packets: %d, Elapsed time: %s\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;numIters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;elapsed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Average time per request: %d ns\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;elapsed&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Nanoseconds&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;numIters&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In my testing, the average latency of a roundtrip
is about 10 Î¼s (that's &lt;em&gt;micro&lt;/em&gt;-seconds). Considering the size of the message
and it being Python on the other end, this is roughly in-line with my
&lt;a class="reference external" href="https://eli.thegreenplace.net/2019/unix-domain-sockets-in-go/"&gt;earlier benchmarking of Unix domain socket latency in Go&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;How long does a single image inference take with this model? In my measurements,
about 3 ms. Recall that the communication latency for the HTTP+REST approach was
0.35 ms; while this is only 12% of the image inference time, it's close enough
to be potentially worrying. On a beefy server-class GPU the time can be much
shorter &lt;a class="footnote-reference" href="#footnote-6" id="footnote-reference-6"&gt;[6]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the custom protocol over domain sockets, the latency - being 10 Î¼s -
seems quite negligible no matter what you end up running
on your GPU.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="code"&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;The full code for the samples in this post is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;To be pedantic, these models are not entirely open: their inference
architecture is open-source and their weights are available, but the
details of their training remain proprietary.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The details of fine-tuning models are beyond the scope of this post,
but there are plenty resources about this online.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&amp;quot;Sampling&amp;quot; in LLMs means roughly &amp;quot;inference&amp;quot;. A trained model is
fed an input prompt and then &amp;quot;sampled&amp;quot; to produce its output.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;In my samples, the Python server and Go client simply run in different
terminals and talk to each other. How service management is structured
is very project-specific. We could envision an approach wherein the
Go application launches the Python server to run in the background and
communicates with it. Increasingly likely these days, however, would
be a container-based setup, where each program is its own container
and an orchestration solution launches and manages these containers.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-5"&gt;[5]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;You may be wondering why I'm implementing a custom protocol here instead
of using something established. In real life, I'd definitely recommend
using something like gRPC. However, for the sake of this sample I wanted
something that would be (1) simple without additional libraries and
(2) very fast. FWIW, I don't think the latency numbers would be very
much different for gRPC. Check out my
&lt;a class="reference external" href="https://eli.thegreenplace.net/2019/unix-domain-sockets-in-go/"&gt;earlier post about RPC over Unix domain sockets in Go&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-6"&gt;[6]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;On the other hand, the model I'm running here is &lt;em&gt;really&lt;/em&gt; small.
It's fair to say realistic models you'll use in your application
will be much larger and hence slower.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category><category term="Python"></category></entry><entry><title>SentencePiece BPE Tokenizer in Go</title><link href="https://eli.thegreenplace.net/2024/sentencepiece-bpe-tokenizer-in-go/" rel="alternate"></link><published>2024-08-23T10:35:00-07:00</published><updated>2024-08-23T17:39:54-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-08-23:/2024/sentencepiece-bpe-tokenizer-in-go/</id><summary type="html">&lt;p&gt;Earlier this year I wrote a &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/tokens-for-llms-byte-pair-encoding-in-go/"&gt;post about implementing BPE tokenization in Go&lt;/a&gt;,
which made it possible to reproduce OpenAI's tokenizer.&lt;/p&gt;
&lt;p&gt;Today I want to mention a new project I've been hacking on recently:
&lt;a class="reference external" href="https://github.com/eliben/go-sentencepiece"&gt;go-sentencepiece&lt;/a&gt;
- a pure Go implementation of the &lt;a class="reference external" href="https://github.com/google/sentencepiece"&gt;SentencePiece&lt;/a&gt; tokenizer
that's used for Google AI's models like â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Earlier this year I wrote a &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/tokens-for-llms-byte-pair-encoding-in-go/"&gt;post about implementing BPE tokenization in Go&lt;/a&gt;,
which made it possible to reproduce OpenAI's tokenizer.&lt;/p&gt;
&lt;p&gt;Today I want to mention a new project I've been hacking on recently:
&lt;a class="reference external" href="https://github.com/eliben/go-sentencepiece"&gt;go-sentencepiece&lt;/a&gt;
- a pure Go implementation of the &lt;a class="reference external" href="https://github.com/google/sentencepiece"&gt;SentencePiece&lt;/a&gt; tokenizer
that's used for Google AI's models like Gemma and Gemini. SentencePiece has
a canonical C++ implementation and Python bindings (using SWIG).
While it's not too hard to wrap the C++ code with cgo, in some cases a C
compiler dependency isn't desirable, so a pure Go solution may be useful. This
is what &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;go-sentencepiece&lt;/span&gt;&lt;/tt&gt; is for.&lt;/p&gt;
&lt;p&gt;A disclaimer: while SentencePiece contains implementations for both BPE and
Unigram tokenizers, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;go-sentencepiece&lt;/span&gt;&lt;/tt&gt; only implements BPE because this is the
one use in practice by models. Also, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;go-sentencepiece&lt;/span&gt;&lt;/tt&gt; doesn't implement the
training phase of the tokenizer, only encoding &amp;amp; decoding. For training,
feel free to review my &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/tokens-for-llms-byte-pair-encoding-in-go/"&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="Screenshot of tokenizer with a sample text, showing tokens" class="align-center" src="https://eli.thegreenplace.net/images/2024/sp-tok-screenshot.png" /&gt;
&lt;p&gt;There are a couple of ways in which SentencePiece works differently from
OpenAI's variant of BPE:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;The text is not pre-split by whitespace using a regexp; instead, whitespace
is considered just another part of the input and has its own tokens. You
can even see it in the screenshot above - it's marked by the &amp;quot;fat underscore&amp;quot;
character (U+2581). While single-space runes are usually part of the next
non-space token, multi-space tokens exist as distinct tokens.&lt;/li&gt;
&lt;li&gt;Instead of being configured by just a vocabulary and a regexp, SentencePiece
tokenizers have a whole protobuf for configuration, with many options.
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;go-sentencepiece&lt;/span&gt;&lt;/tt&gt; only supports the set of options used for Google AI's
models, but more can be added easily.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The whitespace difference turns out to play a crucial role in performance. My
&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/tokens-for-llms-byte-pair-encoding-in-go/"&gt;original BPE implementation&lt;/a&gt;
was fairly naive, using simple quadratic algorithms for encoding; this was OK,
because these algorithms were working on one word at a time, so the N was very
small.&lt;/p&gt;
&lt;p&gt;This is no longer sufficient for SentencePiece, however, since the length
of the full text is N. Therefore, the implementation adopts some more sophisticated
algorithms from the C++ SentencePiece codebase; in particular:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;To match a prefix of a long string from a set of candidates, we use a trie
data structure. The &lt;a class="reference external" href="https://github.com/eliben/go-sentencepiece/tree/main/internal/prefixmatcher"&gt;prefixmatcher&lt;/a&gt;
package implements this and may be generally interesting.&lt;/li&gt;
&lt;li&gt;To figure out which pair of tokens to try merging next, we use a heap-based
priority queue; this is implemented in the generic &lt;a class="reference external" href="https://github.com/eliben/go-sentencepiece/tree/main/internal/priorityqueue"&gt;priorityqueue&lt;/a&gt;
package.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While I didn't spend much time in micro-optimizing the
implementation, these algorithmic improvements sped up the encoder by about 100x
compared to a naive approach, and it's now so fast that I don't think it will
ever be a bottleneck in reality.&lt;/p&gt;
&lt;div class="section" id="config-and-set-up"&gt;
&lt;h2&gt;Config and set up&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, SentencePiece is configurable with a protobuf file. There
are two parts to this: first is a &lt;tt class="docutils literal"&gt;.proto&lt;/tt&gt; file defining the schema of
the protobuf. This is &lt;a class="reference external" href="https://github.com/eliben/go-sentencepiece/blob/main/internal/model/sentencepiece_model.proto"&gt;vendored into my repository&lt;/a&gt;,
copied from the C++ SentencePiece repository. The &lt;tt class="docutils literal"&gt;.pb.go&lt;/tt&gt; file is also in the
tree so you don't need to run the protobuf compiler unless the &lt;tt class="docutils literal"&gt;.proto&lt;/tt&gt;
changes.&lt;/p&gt;
&lt;p&gt;The second part is the protobuf itself, which contains the tokenizer vocabulary
and a bunch of configuration options. This can be downloaded from the
&lt;a class="reference external" href="https://github.com/google/gemma_pytorch/blob/main/tokenizer/tokenizer.model"&gt;official Gemma repository&lt;/a&gt;.
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;go-sentencepiece&lt;/span&gt;&lt;/tt&gt; should be able to load this file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="online-demo"&gt;
&lt;h2&gt;Online demo&lt;/h2&gt;
&lt;p&gt;As before, I've implemented an online demo of this tokenizer by compiling it
into WebAssembly and adding some HTML+JS scaffolding around it. This is where
the screenshot above is from.&lt;/p&gt;
&lt;p&gt;You can play with it here: &lt;a class="reference external" href="https://eliben.github.io/go-sentencepiece/"&gt;https://eliben.github.io/go-sentencepiece/&lt;/a&gt; (the
model protobuf is quite big though, so this page may take a few seconds to load
if you have a slow connection).&lt;/p&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category><category term="WebAssembly"></category></entry><entry><title>Asking an LLM to build a simple web tool</title><link href="https://eli.thegreenplace.net/2024/asking-an-llm-to-build-a-simple-web-tool/" rel="alternate"></link><published>2024-07-09T20:09:00-07:00</published><updated>2024-09-14T13:15:30-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-07-09:/2024/asking-an-llm-to-build-a-simple-web-tool/</id><summary type="html">&lt;p&gt;I've been really enjoying following &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison's blog posts&lt;/a&gt; recently. Simon shows other
programmers the way LLMs will be used for code assistance in the future, and
posts full interactions with LLMs to build small tools or parts of larger
applications.&lt;/p&gt;
&lt;p&gt;A &lt;a class="reference external" href="https://simonwillison.net/2024/Jul/8/box-shadow-css-generator/"&gt;recent post&lt;/a&gt;
caught my attention; here Simon got â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been really enjoying following &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison's blog posts&lt;/a&gt; recently. Simon shows other
programmers the way LLMs will be used for code assistance in the future, and
posts full interactions with LLMs to build small tools or parts of larger
applications.&lt;/p&gt;
&lt;p&gt;A &lt;a class="reference external" href="https://simonwillison.net/2024/Jul/8/box-shadow-css-generator/"&gt;recent post&lt;/a&gt;
caught my attention; here Simon got an LLM (Claude 3.5 Sonnet in this case) to
build a complete tool that lets one configure/tweak box shadow settings
and copy the resulting CSS code for use in a real application. One thing that
seemed interesting is that the LLM in this case used some heavyweight
dependencies (React + JSX) to implement this; Almost 3 MiB of dependency for
something that clearly needs only a few dozen lines of HTML + JS to implement;
yikes.&lt;/p&gt;
&lt;p&gt;So I've decided to try my own experiment and get an LLM to do this without any
dependencies. It turned out to be very easy, because the LLM I used (in this
case ChatGPT 4o, but it could really have been any of the top-tier LLMs, I
think) opted for the no-dependency approach from the start. I was preparing to
ask it to adjust the code to remove dependencies, but this turned out to be
unnecessary.&lt;/p&gt;
&lt;p&gt;The resulting tool is very similar to Simon's in functionality; it's deployed
at &lt;a class="reference external" href="https://eliben.org/box-shadow-tool/"&gt;https://eliben.org/box-shadow-tool/&lt;/a&gt;; here's a screenshot:&lt;/p&gt;
&lt;img alt="Screenshot of box shadow tool" class="align-center" src="https://eli.thegreenplace.net/images/2024/box-shadow-screenshot.png" /&gt;
&lt;p&gt;Here are my prompts:&lt;/p&gt;
&lt;blockquote&gt;
CSS for a slight box shadow, build me a tool that helps me twiddle settings
and preview them and copy and paste out the CSS&lt;/blockquote&gt;
&lt;p&gt;ChatGPT produced a working tool but it didn't really look good on the page.&lt;/p&gt;
&lt;blockquote&gt;
Yes, make the tool itself look a bit better with some CSS so it's all centered
on the screen and there's enough space for the preview box&lt;/blockquote&gt;
&lt;p&gt;It still wasn't quite what I wanted.&lt;/p&gt;
&lt;blockquote&gt;
the container has to be wider so all the text and sliders fix nicely, and
there's still not enough space for the shadows of the preview box to show
without overlapping with other elements&lt;/blockquote&gt;
&lt;p&gt;Now it was looking better; I wanted a button to copy-paste, like in Simon's
demo:&lt;/p&gt;
&lt;blockquote&gt;
this looks better; now add a nice-looking button at the bottom that copies the
resulting css code to the clipboard&lt;/blockquote&gt;
&lt;p&gt;The code ChatGPT produced for the clipboard copy operation was flagged by
vscode as deprecated, so I asked:&lt;/p&gt;
&lt;blockquote&gt;
it seems like &amp;quot;document.execCommand('copy')&amp;quot; is deprecated; is there a more
accepted way to do this?&lt;/blockquote&gt;
&lt;p&gt;The final version can be seen in the &lt;a class="reference external" href="https://eliben.org/box-shadow-tool/"&gt;online demo&lt;/a&gt; (view-source). The complete ChatGPT
transcript is &lt;a class="reference external" href="https://chatgpt.com/share/a05935cb-3e6d-4f5d-997f-5d8541450d1c"&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="insights"&gt;
&lt;h2&gt;Insights&lt;/h2&gt;
&lt;p&gt;Overall, this was a positive experience. While a tool like this is very simple
to implement manually, doing it with an LLM was even quicker. The results are
still not perfect in terms of alignment and space, but they're good enough. At
this point one would probably just take over and do the final tweaks manually.&lt;/p&gt;
&lt;p&gt;I was pleasantly surprised by how stable the LLM managed to keep its output
throughout the interaction; it only modified the parts I asked it to, and the
rest of the code remained identical. Stability has been an issue with LLMs
(particularly for images), and I'm happy to see it holds well for code (there
could be some special tuning or prompt engineering for ChatGPT to make this work
well).&lt;/p&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Machine Learning"></category><category term="JavaScript"></category></entry><entry><title>Tokens for LLMs: Byte Pair Encoding in Go</title><link href="https://eli.thegreenplace.net/2024/tokens-for-llms-byte-pair-encoding-in-go/" rel="alternate"></link><published>2024-04-25T06:34:00-07:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-04-25:/2024/tokens-for-llms-byte-pair-encoding-in-go/</id><summary type="html">&lt;p&gt;A basic unit of currency in modern LLMs is the &lt;em&gt;token&lt;/em&gt;; &lt;a class="reference external" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/"&gt;exciting new models&lt;/a&gt;
have long context windows of millions of &lt;em&gt;tokens&lt;/em&gt;. API pricing for the large
providers is &lt;em&gt;per-token&lt;/em&gt;. We're even seeing the invention of new, derived units
like TPM (&lt;a class="reference external" href="https://ai.google.dev/pricing"&gt;tokens per minute&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But what are tokens?&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"&gt;This OpenAI â€¦&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;A basic unit of currency in modern LLMs is the &lt;em&gt;token&lt;/em&gt;; &lt;a class="reference external" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/"&gt;exciting new models&lt;/a&gt;
have long context windows of millions of &lt;em&gt;tokens&lt;/em&gt;. API pricing for the large
providers is &lt;em&gt;per-token&lt;/em&gt;. We're even seeing the invention of new, derived units
like TPM (&lt;a class="reference external" href="https://ai.google.dev/pricing"&gt;tokens per minute&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But what are tokens?&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"&gt;This OpenAI help article&lt;/a&gt;
tells us that tokens are &lt;em&gt;pieces of words&lt;/em&gt;, and gives some useful rules of thumb
like a token being equivalent to approximately 4 characters or 3/4 of a word
for the English language.&lt;/p&gt;
&lt;p&gt;In this post I want to review the most commonly used algorithm for splitting
text into tokens, provide a complete implementation in Go, and show a playground
for experimenting with it. While my implementation isn't tuned for speed, it
aims to be complete, readable and compatible with OpenAI's &lt;a class="reference external" href="https://github.com/openai/tiktoken"&gt;tiktoken library&lt;/a&gt;, generating identical results and
working with the same vocabulary files.&lt;/p&gt;
&lt;div class="section" id="byte-pair-encoding-introduction"&gt;
&lt;h2&gt;Byte pair encoding - introduction&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Byte_pair_encoding"&gt;Byte pair encoding&lt;/a&gt; (BPE) is
an algorithm originally designed for data compression. A &lt;a class="reference external" href="https://arxiv.org/abs/1508.07909"&gt;2016 paper&lt;/a&gt; suggested re-purposing it for &amp;quot;word
segmentation&amp;quot; for machine learning tasks. The colloquial term for word
segmentation is &lt;em&gt;tokenization&lt;/em&gt;.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Input: arbitrary text with words, numbers, whitespace and punctuation.&lt;/li&gt;
&lt;li&gt;Output: list of tokens representing the same text. Each token is an integer
identifier which can be looked up in a vocabulary to reproduce the input text
&lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The BPE algorithm has an important pre-processing step: splitting the input text
into words. The splitting is customizable and different models / vocabularies
use different regexps for splitting (more on this later). The main idea is
some sort of whitespace-based splitting (though whitespace itself is preserved)
because we typically don't want inter-word tokens &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We'll be using this line from a &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Blue_(Da_Ba_Dee)"&gt;catchy 1990s song&lt;/a&gt;
as an example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;i&amp;#39;m blue dabadee dabadam
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A word splitter will produce something like the following list, where spaces are
replaced by underscores &lt;tt class="docutils literal"&gt;_&lt;/tt&gt; for the sake of presentation (they remain as
spaces in the actual implementation of the algorithm and its trained
vocabulary):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;i
&amp;#39;m
_blue
_dabadee
_dabadam
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A few things to note:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The contraction &lt;tt class="docutils literal"&gt;'m&lt;/tt&gt; is split from &lt;tt class="docutils literal"&gt;i&lt;/tt&gt; - this is common for English language
splitters, which want things like &lt;tt class="docutils literal"&gt;'m&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;'ll&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;'re&lt;/tt&gt; as separate words.&lt;/li&gt;
&lt;li&gt;Whitespace is preserved and attached at the start of a word. Whitespace is
important because tokens at the beginning of words sometimes have different
semantic meaning from tokens not at the beginning of words.
The choice of where it's attached is arbitrary. From this point on, whitespace
bytes are considered like any other bytes in the BPE algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now is a good time for some terminology we'll be using while talking about BPE:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;em&gt;Word&lt;/em&gt;: produced by the splitter in pre-processing, like the list shown above.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Token&lt;/em&gt;: typically a sub-word sequence of bytes; the output of the tokenizer
is a list of tokens, by ID.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Token ID&lt;/em&gt;: unique numerical identifier for a token.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Vocabulary&lt;/em&gt;: a mapping of token IDs --&amp;gt; token values learned by the tokenizer
during the training process.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Training&lt;/em&gt;: the process in which BPE learns a vocabulary from a corpus of
text.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Splitter regexp&lt;/em&gt;: regular expression used to split text into words during
pre-processing. Given an algorithm (in this case BPE), the pair vocabulary +
splitter regexp unambiguously defines how a given text will be tokenized.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Encoder&lt;/em&gt;: given a vocabulary and a splitter regexp, tokenizes any text into
a list of IDs from the vocabulary.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Decoder&lt;/em&gt;: given a list of IDs and the vocabulary, reconstructs the
original text.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="training"&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;BPE training proceeds by first assuming each byte is its own token, and then
successively merging pairs of tokens into longer tokens and adding these to
the vocabulary, until the desired vocabulary size is achieved.&lt;/p&gt;
&lt;p&gt;Let's reuse our example, starting with these words:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;i
&amp;#39;m
_blue
_dabadee
_dabadam
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The BPE process starts by creating a token for each byte in the inclusive
range [0..255]. So the minimal vocabulary size is 256; this guarantees that
from the very start, there's a valid encoded representation of any text.&lt;/p&gt;
&lt;p&gt;Then, the following process is repeated:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Count how many times each ordered pair of bytes appears in the input.
&lt;em&gt;Ordered pair&lt;/em&gt; here means two bytes right next to each other. In our example,
some such pairs are &amp;quot;bl&amp;quot;, &amp;quot;da&amp;quot;, &amp;quot;de&amp;quot;, &amp;quot;ee&amp;quot; etc.&lt;/li&gt;
&lt;li&gt;Find the pair with the highest count, and create a new token from it (create
a new token ID, mapping it to the concatenation of the most common pair).&lt;/li&gt;
&lt;li&gt;Replace this most common pair with the combined token in the input set.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our example, we start by splitting input words to bytes, so it's a list of
single-byte token lists. This is our working list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[i]
[&amp;#39; m]
[_ b l u e]
[_ d a b a d e e]
[_ d a b a d a m]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, we count the frequency of appearance of each ordered pair:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[d a] --&amp;gt; 3
[a b] --&amp;gt; 2
[b a] --&amp;gt; 2
[&amp;#39; m] --&amp;gt; 1
[_ b] --&amp;gt; 1
[l u] --&amp;gt; 1
[u e] --&amp;gt; 1
[_ d] --&amp;gt; 2
[a d] --&amp;gt; 2
[d e] --&amp;gt; 1
[e e] --&amp;gt; 1
[b l] --&amp;gt; 1
[a m] --&amp;gt; 1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The pair &amp;quot;da&amp;quot; is the most common one, so we're creating a new token for it,
and substituting it everywhere in the working list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[i]
[&amp;#39; m]
[_ b l u e]
[_ da b a d e e]
[_ da b a da m]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can see, in every instance &amp;quot;d&amp;quot; followed by &amp;quot;a&amp;quot; was combined into &amp;quot;da&amp;quot;.
Now repeat the process; finding the most common pairs in this new working list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[e e] --&amp;gt; 1
[a da] --&amp;gt; 1
[l u] --&amp;gt; 1
[_ da] --&amp;gt; 2
[da b] --&amp;gt; 2
[a d] --&amp;gt; 1
[d e] --&amp;gt; 1
[da m] --&amp;gt; 1
[&amp;#39; m] --&amp;gt; 1
[_ b] --&amp;gt; 1
[b l] --&amp;gt; 1
[u e] --&amp;gt; 1
[b a] --&amp;gt; 2
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Several pairs have a count of 2, so we pick one arbitrarily. Let's say it's
&lt;tt class="docutils literal"&gt;_da&lt;/tt&gt; (a space followed by &amp;quot;da&amp;quot;). We add &lt;tt class="docutils literal"&gt;_da&lt;/tt&gt; as a new token and
make replacements in the working list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[i]
[&amp;#39; m]
[_ b l u e]
[_da b a d e e]
[_da b a da m]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And so on. When does this process stop? When we either run out of pairs (every
word consists of a single token) or - more realistically for an actual training
corpus - when we reach our desired vocabulary size. For example the vocabulary
used for GPT-4 has around 100,000 tokens (more on this later).&lt;/p&gt;
&lt;p&gt;The output of the training process is a vocabulary; let's say we've only run
two cycles on our input text as described. The vocabulary will have 258 tokens
in it: 256 for the single bytes, one for &lt;tt class="docutils literal"&gt;da&lt;/tt&gt; and another for &lt;tt class="docutils literal"&gt;_da&lt;/tt&gt;. Each
of these would have a unique integer ID.&lt;/p&gt;
&lt;p&gt;In our Go sample code, the training is implemented &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/bpe/train.go"&gt;in this file&lt;/a&gt;. You
can set the &lt;tt class="docutils literal"&gt;debugTrain&lt;/tt&gt; variable to &lt;tt class="docutils literal"&gt;true&lt;/tt&gt; to follow the process on some
sample text.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="encoding"&gt;
&lt;h2&gt;Encoding&lt;/h2&gt;
&lt;p&gt;Having learned a vocabulary, the process of encoding is what happens every time
we feed text into an LLM and it needs to be tokenized. The input is arbitrary
text, a splitting regexp and a vocabulary. For example, let's take the input
text &amp;quot;yada daba&amp;quot;. Splitting is performed as before, and the input is broken
into individual bytes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[y a d a]
[_ d a b a]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;BPE encoding takes the vocabulary and tries to apply learned tokens to the input
text, word by word. The process is &lt;em&gt;greedy&lt;/em&gt; - tokens are applied in the same
order they've been learned (this is easy to accomplish by assigning
monotonically increasing integer IDs to new tokens in the vocabulary, and then
prioritizing lower-numbered tokens for encoding).&lt;/p&gt;
&lt;p&gt;The first token we learned was &lt;tt class="docutils literal"&gt;da&lt;/tt&gt;, so let's apply that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[y a da]
[_ da b a]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The next token we learned was &lt;tt class="docutils literal"&gt;_da&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[y a da]
[_da b a]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the final stage; there are no more learned tokens to apply. The result
will consist of 6 tokens.&lt;/p&gt;
&lt;p&gt;In our sample code, the encoder is &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/bpe/encode.go"&gt;in this file&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="realistic-vocabulary-and-splitting"&gt;
&lt;h2&gt;Realistic vocabulary and splitting&lt;/h2&gt;
&lt;p&gt;The examples shown so far have been toys, but the algorithms are real and work
with the actual vocabularies and splitters used in modern models. As a case
study, the tokenizer used for OpenAI's GPT-4 uses a vocabulary called
&lt;tt class="docutils literal"&gt;cl100k_base&lt;/tt&gt;, which contains 100k tokens in addition to the 256 byte-sized
ones. This is also the vocabulary (encoding) the
&lt;a class="reference external" href="https://github.com/openai/tiktoken"&gt;tiktoken&lt;/a&gt; library uses. It can be
freely downloaded from OpenAI - a copy is available in my
&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/bpe/data"&gt;sample repository&lt;/a&gt;.
The file is base64 encoded, which is easy to unravel and we'll see
tokens like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;quot; Fritz&amp;quot;  91083
&amp;quot;Initially&amp;quot;  91084
&amp;quot;nodeValue&amp;quot;  91085
&amp;quot;_TRIANGLES&amp;quot;  91086
&amp;quot;-backend&amp;quot;  91087
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The token string value is to the left, and the numerical token ID is to the
right. As you can see, the algorithm is not particularly discerning about what
it learns - names, pieces of code - whatever works!&lt;/p&gt;
&lt;p&gt;The other important data needed to reproduce OpenAI's tokenization is the
splitting regexp, which is this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(?i:&amp;#39;s|&amp;#39;t|&amp;#39;re|&amp;#39;ve|&amp;#39;m|&amp;#39;ll|&amp;#39;d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It's just a combination of several alternatives. You could
use one of the many &amp;quot;regexp explainer&amp;quot; websites out there to study
it, or ask a modern LLM, but the gist of it is: this regexp splits
space-delimited words, leaving spaces in front of the words, with some special
provisions like English contractions (being separate words) and long numbers
being split to groups of 3. For Go programmers, it's important to note that
this pattern uses &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;?!&lt;/span&gt;&lt;/tt&gt; - negative lookahead - which the standard &lt;tt class="docutils literal"&gt;regexp&lt;/tt&gt;
package doesn't support. Therefore, we'll have to reach for the 3rd party
&lt;a class="reference external" href="https://github.com/dlclark/regexp2"&gt;regexp2&lt;/a&gt; to implement this &lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In our sample repository, take a look at &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/bpe/tiktoken_tokenize_test.go"&gt;this test&lt;/a&gt;
that ties everything together - it loads the &lt;tt class="docutils literal"&gt;cl100k_base&lt;/tt&gt; encoding and uses
it alongside the splitting regexp to tokenize some real text.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="full-online-demo-with-a-web-ui-and-webassembly"&gt;
&lt;h2&gt;Full online demo with a web UI and WebAssembly&lt;/h2&gt;
&lt;p&gt;My goal with this project wasn't only to understand the BPE algorithm, but to
also try reproducing the actual tokenizer used by OpenAI for its most modern
models. And this goal was accomplished!&lt;/p&gt;
&lt;p&gt;OpenAI has a nice &lt;a class="reference external" href="https://platform.openai.com/tokenizer"&gt;website here&lt;/a&gt;
that lets you enter text and see how it's tokenized. I've managed to reproduce
this UI - see the &lt;tt class="docutils literal"&gt;cmd/wasm&lt;/tt&gt; directory in the repository. I've also placed it
online - it can ran in your browser &lt;a class="reference external" href="https://eliben.org/bpe/"&gt;from here&lt;/a&gt;.
Here's a screenshot &lt;a class="footnote-reference" href="#footnote-4" id="footnote-reference-4"&gt;[4]&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="Screenshot of tokenizer with a sample text, showing tokens" class="align-center" src="https://eli.thegreenplace.net/images/2024/tokenizer-screenshot.png" /&gt;
&lt;p&gt;How it works: the Go implementation of BPE is compiled to a WebAssembly binary
that's loaded from a bit of glue JavaScript embedded in a simple HTML page.
The JavaScript watches the text box as you type and sends the string to a Go
function exported from the WASM, which tokenizes it on the fly. So we get a nice
effect of &amp;quot;tokens updated as we type&amp;quot;. The selection button at the bottom also
lets us see the numerical IDs for these tokens - they should be equivalent to
what tiktoken is producing.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;For simplicity, this post will focus on English. As you'll see, however,
the BPE algorithm is language-agnostic.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;There's also a performance implication: if we make tokenization
word-oriented, we can easily implement streaming tokenization without
depending on previous words.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I think it would be possible - with a bit of effort - to work around this
limitation and stick to the standard library, but just using &lt;tt class="docutils literal"&gt;regexp2&lt;/tt&gt;
is simpler, and it's also what &lt;a class="reference external" href="https://github.com/pkoukk/tiktoken-go"&gt;tiktoken-go&lt;/a&gt;
is doing.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-4"&gt;[4]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;You'll notice that in this example every word (except contractions) is
a separate token; this shouldn't be surprising, since these are all very
common words and the vocabulary is large! Try playing with it a bit
though, giving it longer words (like &amp;quot;discombobulated&amp;quot;) or non-trivial
variable names from a programming language.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category><category term="WebAssembly"></category></entry><entry><title>The life of an Ollama prompt</title><link href="https://eli.thegreenplace.net/2024/the-life-of-an-ollama-prompt/" rel="alternate"></link><published>2024-03-06T05:28:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-03-06:/2024/the-life-of-an-ollama-prompt/</id><summary type="html">&lt;p&gt;In &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/"&gt;a previous post&lt;/a&gt;
I've described how - thanks to standardized tooling - we
could use a locally-running &lt;a class="reference external" href="https://blog.google/technology/developers/gemma-open-models/"&gt;Gemma model&lt;/a&gt;
from a Go program within hours from its public release.&lt;/p&gt;
&lt;p&gt;This post dives into the internals of &lt;a class="reference external" href="https://ollama.com/"&gt;Ollama&lt;/a&gt; -
a popular and extremely convenient open-source Go project that makes such
workflows possible.&lt;/p&gt;
&lt;img alt="Mechanical llama being taken apart" class="align-center" src="https://eli.thegreenplace.net/images/2024/mllama.png" /&gt;
&lt;div class="section" id="http-request-to-ollama"&gt;
&lt;h2&gt;HTTP â€¦&lt;/h2&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/"&gt;a previous post&lt;/a&gt;
I've described how - thanks to standardized tooling - we
could use a locally-running &lt;a class="reference external" href="https://blog.google/technology/developers/gemma-open-models/"&gt;Gemma model&lt;/a&gt;
from a Go program within hours from its public release.&lt;/p&gt;
&lt;p&gt;This post dives into the internals of &lt;a class="reference external" href="https://ollama.com/"&gt;Ollama&lt;/a&gt; -
a popular and extremely convenient open-source Go project that makes such
workflows possible.&lt;/p&gt;
&lt;img alt="Mechanical llama being taken apart" class="align-center" src="https://eli.thegreenplace.net/images/2024/mllama.png" /&gt;
&lt;div class="section" id="http-request-to-ollama"&gt;
&lt;h2&gt;HTTP request to Ollama&lt;/h2&gt;
&lt;p&gt;Having &lt;a class="reference external" href="https://ollama.com/download"&gt;installed Ollama&lt;/a&gt; and
run &lt;tt class="docutils literal"&gt;ollama run gemma&lt;/tt&gt;, we're ready to send HTTP requests to it. There are
several ways to do so:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Sending a raw HTTP request with a tool like &lt;tt class="docutils literal"&gt;curl&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Using Ollama's own client libraries (currently available in Go, Python and
JS)&lt;/li&gt;
&lt;li&gt;Using a provider-agnostic client like LangChainGo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For options (2) and (3) see the Appendix; here we'll focus on (1) for
simplicity and to remove layers from the explanation.&lt;/p&gt;
&lt;p&gt;Let's send an HTTP request to the &lt;tt class="docutils literal"&gt;api/generate&lt;/tt&gt; endpoint of Ollama with
&lt;tt class="docutils literal"&gt;curl&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl http://localhost:11434/api/generate -d &amp;#39;{
  &amp;quot;model&amp;quot;: &amp;quot;gemma&amp;quot;,
  &amp;quot;prompt&amp;quot;: &amp;quot;very briefly, tell me the difference between a comet and a meteor&amp;quot;,
  &amp;quot;stream&amp;quot;: false
}&amp;#39; | jq .

[...]

{
  &amp;quot;model&amp;quot;: &amp;quot;gemma&amp;quot;,
  &amp;quot;created_at&amp;quot;: &amp;quot;2024-03-04T14:43:51.665311735Z&amp;quot;,
  &amp;quot;response&amp;quot;: &amp;quot;Sure, here is the difference between a comet and a meteor:

  **Comet:**
  - A celestial object that orbits the Sun in a highly elliptical path.
  - Can be seen as a streak of light in the sky, often with a tail.
  - Comets typically have a visible nucleus, meaning a solid core that
    can be seen from Earth.

  **Meteor:**
  - A streak of hot gas or plasma that appears to move rapidly across the sky.
  - Can be caused by small pieces of rock or dust from space that burn up
    in the atmosphere.
  - Meteors do not have a visible nucleus.&amp;quot;,
  &amp;quot;done&amp;quot;: true,
  &amp;quot;context&amp;quot;:
[...]
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;(The response is JSON and I've reformatted the text for clarity)&lt;/p&gt;
&lt;p&gt;Ollama's HTTP API is &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/api.md"&gt;documented here&lt;/a&gt;.
For each endpoint, it lists a description of parameters and the data returned.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ollama-service"&gt;
&lt;h2&gt;Ollama service&lt;/h2&gt;
&lt;img alt="Internals of ollama, showing service connecting to clients and loading GGUF" class="align-center" src="https://eli.thegreenplace.net/images/2024/ollama-internals.png" /&gt;
&lt;p&gt;Ollama itself is a client-server application; when the installation script is
run, it does several things:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Download Ollama binary&lt;/li&gt;
&lt;li&gt;Place it in &lt;tt class="docutils literal"&gt;$PATH&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Run &lt;tt class="docutils literal"&gt;ollama serve&lt;/tt&gt; as a background service&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The service checks the value of the &lt;tt class="docutils literal"&gt;OLLAMA_HOST&lt;/tt&gt; env var to figure out which
host and port to use. The default is port 11434 on &lt;tt class="docutils literal"&gt;localhost&lt;/tt&gt; (hence you can
see our &lt;tt class="docutils literal"&gt;curl&lt;/tt&gt; request is made to &lt;tt class="docutils literal"&gt;localhost:11434&lt;/tt&gt;). It then listens on
the port, presenting the API discussed above.&lt;/p&gt;
&lt;p&gt;What's interesting to note is that when we run &lt;tt class="docutils literal"&gt;ollama run &amp;lt;model&amp;gt;&lt;/tt&gt; from
the command-line, this invokes the Ollama binary in client mode; in this mode,
it sends requests to the service using the same API. For example, here are
two ways to invoke it - interactive:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ollama run gemma
&amp;gt;&amp;gt;&amp;gt; translate naranjo to english
Naranjo translates to Orange in English.

Naranjo is the Spanish word for Orange.

&amp;gt;&amp;gt;&amp;gt; &amp;lt;Ctrl+D&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And piping to stdin:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ echo &amp;quot;translate naranjo to english&amp;quot; | ollama run gemma
Naranjo translates to Orange in English. Orange is the English word equivalent of the word Naranjo.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In both these cases, the Ollama binary sends an HTTP request to
&lt;a class="reference external" href="http://localhost:11434/api/generate"&gt;http://localhost:11434/api/generate&lt;/a&gt;, just like the one we've made manually
with &lt;tt class="docutils literal"&gt;curl&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-generate-api-endpoint"&gt;
&lt;h2&gt;The &lt;tt class="docutils literal"&gt;generate&lt;/tt&gt; API endpoint&lt;/h2&gt;
&lt;p&gt;Now that we know where our prompt to Ollama ends up (whether we issue it using
an HTTP request or the Ollama command-line tool), let's see what the
&lt;tt class="docutils literal"&gt;generate&lt;/tt&gt; API endpoint actually does.&lt;/p&gt;
&lt;p&gt;Ollama uses the Gin web framework, and the API route is fairly standard:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nx"&gt;r&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;POST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/api/generate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GenerateHandler&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This routes HTTP POST requests for &lt;tt class="docutils literal"&gt;/api/generate&lt;/tt&gt; to a handler function called
&lt;tt class="docutils literal"&gt;GenerateHandler&lt;/tt&gt;, which is defined &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/server/routes.go"&gt;in the same source file&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;GenerateHandler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;gin&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Context&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After parsing and validating the request, &lt;tt class="docutils literal"&gt;GenerateHandler&lt;/tt&gt; starts by
fetching the model the request asked for with the &lt;tt class="docutils literal"&gt;&amp;quot;model&amp;quot;&lt;/tt&gt; field. It then
loads the right model and runs it, feeding it with the prompt provided
in the request. The next sections describe these two steps.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fetching-and-loading-the-model"&gt;
&lt;h2&gt;Fetching and loading the model&lt;/h2&gt;
&lt;p&gt;When Ollama is looking for a model (by name), it first checks if it already
has it downloaded and stored locally. On my Linux machine, Ollama stores its
local cache of models at &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/usr/share/ollama/.ollama/models/blobs&lt;/span&gt;&lt;/tt&gt;. If the
model is already available locally, there's not much to do for this step.&lt;/p&gt;
&lt;p&gt;Otherwise, Ollama looks in its &lt;a class="reference external" href="https://ollama.com/library"&gt;online library&lt;/a&gt;
of models. Specifically, the service makes a request to
&lt;a class="reference external" href="https://registry.ollama.ai/v2/library/"&gt;https://registry.ollama.ai/v2/library/&lt;/a&gt; to check if a model exists. At the time
of writing, it's not clear if anyone except the Ollama maintainers can upload
new models to the library - but it seems like they're working on this option.&lt;/p&gt;
&lt;p&gt;But where do these models come from? As &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/import.md"&gt;this doc explains&lt;/a&gt;, models are
imported from other sources in formats like GGUF or Safetensors. The topic
of these formats is very interesting, but I won't be covering it in this
post; if you're interested, &lt;a class="reference external" href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/"&gt;a recent blog post by Vicki Boykis&lt;/a&gt;
provides useful historic background.&lt;/p&gt;
&lt;p&gt;While models can be imported from a variety of formats, Ollama's library
stores them as GGUF and that's what the service expects to find.&lt;/p&gt;
&lt;p&gt;For the purpose of this explanation, it's sufficient to know that GGUF
stores some metadata about the model (e.g. its architecture and parameters,
like numbers of layers in different parts, etc) as well as its actual weights.
The weights can be stored in different formats - some more suitable for
GPUs, some for CPUs. Quantization is common, especially for CPU-oriented models.
The model file is usually a giant multi-GiB binary blob that needs to be
downloaded and cached locally.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="running-the-underlying-model-with-a-prompt"&gt;
&lt;h2&gt;Running the underlying model with a prompt&lt;/h2&gt;
&lt;p&gt;To run the model, Ollama turns to another project - &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;. &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; arose as a local
inference engine for the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/LLaMA"&gt;Llama model&lt;/a&gt;
when it was originally released. Since the model architecture and weights
were published, it became possible to implement inference for the model
without relying on full-blown Python ML frameworks like TensorFlow, PyTorch or
JAX. It uses its author's separate project - &lt;a class="reference external" href="https://github.com/ggerganov/ggml"&gt;ggml&lt;/a&gt;, for an efficient C++ library of ML
primitives that can run on CPUs and GPUs.&lt;/p&gt;
&lt;p&gt;Originally &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; just hard-coded Llama's architecture and loaded the
weights, but in time it grew to incorporate additional open-sourced models
and its implementation became a kind of a &lt;tt class="docutils literal"&gt;switch&lt;/tt&gt; based on
the model's architecture.&lt;/p&gt;
&lt;p&gt;For example, &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp/commit/580111d42b3b6ad0a390bfb267d6e3077506eb31"&gt;this commit&lt;/a&gt;
added Gemma support to &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;. Once this is in place, all it needs is to
load the weights and some parameterization of the model from its GGUF file and
it's ready to go.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; is a C++ project that was originally designed as a command-line
utility you can use to load models and chat with them. C++ is not known for
having a pleasant or stable ABI to work with, so many projects wrapped
&lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; with a lightweight C ABI in order to create bindings into other
languages.&lt;/p&gt;
&lt;p&gt;Ollama, as a Go project, did the same. It went a step further though, and
cleverly leverages &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt;'s &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp/tree/master/examples/server"&gt;server sample&lt;/a&gt;,
which encapsulates all operations in functions that take JSON inputs and
return JSON outputs. Ollama added some glue in &lt;a class="reference external" href="https://github.com/ollama/ollama/tree/main/llm/ext_server"&gt;ext_server&lt;/a&gt;,
and wrapped it with &lt;tt class="docutils literal"&gt;cgo&lt;/tt&gt; to be able to invoke &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; inference
in-process.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;generate&lt;/tt&gt; endpoint calls &lt;a class="reference external" href="https://pkg.go.dev/github.com/jmorganca/ollama&amp;#64;v0.1.28/llm#LLM.Predict"&gt;llm.Predict&lt;/a&gt;,
which after some hops ends &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt;'s &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/server.cpp#L1260"&gt;request_completion&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="afterword-standard-interfaces"&gt;
&lt;h2&gt;Afterword: standard interfaces&lt;/h2&gt;
&lt;p&gt;In my &lt;a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/"&gt;previous post&lt;/a&gt;,
I've mentioned that the flow works and is easy to set up due to standardized
interfaces that have been implemented in OSS projects.&lt;/p&gt;
&lt;p&gt;After reading this post with Ollama internals, I hope it's clear what
standardized interfaces come into play here.&lt;/p&gt;
&lt;p&gt;First and foremost is &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; and its associated GGUF format. While the
internals of &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; are somewhat clunky, this project is unapologetically
pragmatic and a true boon for
the ecosystem because of the way it standardizes LLM inference (and embeddings).
Given a model architecture implemented in C++ in the innards of &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt;,
variations can be easily explored and run on compatible CPUs and GPUs. Slight
model modifications? Tuning? Trying some new kind of quantizations? Just create
a GGUF file and &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; will run it for you.&lt;/p&gt;
&lt;p&gt;The other half of the solution is Ollama, which wraps &lt;tt class="docutils literal"&gt;llama.cpp&lt;/tt&gt; in a
conveniently packaged tool, API and ecosystem &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;. As a Go project, it's easily
distributable and makes it trivial to hack on a powerful API server. The REST
API it presents can then be leveraged by any tool capable of issuing HTTP
requests.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="appendix-go-client-libraries-for-the-ollama-api"&gt;
&lt;h2&gt;Appendix: Go client libraries for the Ollama API&lt;/h2&gt;
&lt;p&gt;If you want to use LLMs programmatically from Go through Ollama, the most
convenient options are either using Ollama's own Go client library or through
LangChainGo. Another option - as discussed above - is to send raw HTTP requests.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://pkg.go.dev/github.com/jmorganca/ollama/api"&gt;Ollama Go client library&lt;/a&gt;
is a great option because it's what the Ollama client itself uses to talk to the
service; it's as battle-tested and functional as you can hope for. On the other
hand, LangChainGo is convenient if you use multiple providers and want code
that's consistent and provider-agnostic.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/ollama-go-clients"&gt;This sample&lt;/a&gt;
lists Go code to ask Ollama a question using (1) the Ollama Go library or (2)
LangChainGo.&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;The Gemma announcement points to this official documentation and
implementation - &lt;a class="reference external" href="https://github.com/google-deepmind/gemma"&gt;https://github.com/google-deepmind/gemma&lt;/a&gt; - it can be
used to re-implement Gemma inference, along with the pre-trained model
weights Google released.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Ollama has additional capabilities I haven't mentioned here, like &lt;a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md"&gt;Modelfiles&lt;/a&gt; for creating
and sharing models.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category></entry><entry><title>Gemma, Ollama and LangChainGo</title><link href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/" rel="alternate"></link><published>2024-02-22T16:24:00-08:00</published><updated>2024-05-04T19:46:23-07:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-02-22:/2024/gemma-ollama-and-langchaingo/</id><summary type="html">&lt;p&gt;Yesterday Google released Gemma - an open LLM that folks can run locally on
their machines (similarly to &lt;tt class="docutils literal"&gt;llama2&lt;/tt&gt;). I was wondering how easy it would be
to run Gemma on my computer, chat with it and interact with it from a Go
program.&lt;/p&gt;
&lt;p&gt;Turns it - thanks to &lt;a class="reference external" href="https://ollama.com/download"&gt;Ollama&lt;/a&gt; - it's extremely â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday Google released Gemma - an open LLM that folks can run locally on
their machines (similarly to &lt;tt class="docutils literal"&gt;llama2&lt;/tt&gt;). I was wondering how easy it would be
to run Gemma on my computer, chat with it and interact with it from a Go
program.&lt;/p&gt;
&lt;p&gt;Turns it - thanks to &lt;a class="reference external" href="https://ollama.com/download"&gt;Ollama&lt;/a&gt; - it's extremely
easy! Gemma was already &lt;a class="reference external" href="https://ollama.com/library/gemma"&gt;added to Ollama&lt;/a&gt;,
so all one has to do is run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ollama run gemma
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And wait for a few minutes while the model downloads. From this point on, my
previous post about &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/"&gt;using Ollama locally in Go&lt;/a&gt; applies
with pretty much no changes. Gemma becomes available through a REST API locally,
and can be accessed from ollama-aware libraries like &lt;a class="reference external" href="https://github.com/tmc/langchaingo"&gt;LangChainGo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I went ahead and added a &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--model&lt;/span&gt;&lt;/tt&gt; flag to all my &lt;a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/ollama-go-langchain"&gt;code samples from that post&lt;/a&gt;,
and they can all run with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--model&lt;/span&gt; gemma&lt;/tt&gt; now. It all just works, due to the
magic of standard interfaces:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Gemma is packaged in a standard interface for inclusion in Ollama&lt;/li&gt;
&lt;li&gt;Ollama then presents a standardized REST API for this model, just like it
does for other compatible models&lt;/li&gt;
&lt;li&gt;LangChainGo has an Ollama provider that lets us write code to interact with
any model running through Ollama&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we can write code like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;flag&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/tmc/langchaingo/llms/ollama&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;modelName&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;String&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ollama model name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Parse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;New&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ollama&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WithModel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;modelName&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Args&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;completion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llms&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateFromSinglePrompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;llm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Response:\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;completion&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then run it as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ go run ollama-completion-arg.go --model gemma &amp;quot;what should be added to 91 to make -20?&amp;quot;
Response:
 The answer is -111.

91 + (-111) = -20
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Gemma seems relatively fast for a model running on a CPU.
I find that the default 7B model, while much more capable than the default 7B
llama2 based on published benchmarks - also runs about 30% faster on my machine.&lt;/p&gt;
&lt;div class="section" id="without-langchaingo"&gt;
&lt;h2&gt;Without LangChainGo&lt;/h2&gt;
&lt;p&gt;While LangChainGo offers a conveneint API that's standardized across LLM
providers, its use is by no means required for this sample. Ollama itself has
a &lt;a class="reference external" href="https://pkg.go.dev/github.com/jmorganca/ollama/api"&gt;Go API&lt;/a&gt; as part of
its structure and it can be used externally as well. Here's an equivalent
sample that doesn't require LangChainGo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;context&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;flag&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fmt&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;log&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;github.com/jmorganca/ollama/api&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;modelName&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;String&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ollama model name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Parse&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;client&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;api&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ClientFromEnvironment&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nx"&gt;api&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateRequest&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;modelName&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;Prompt&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;flag&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Args&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;Stream&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;// disable streaming&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Background&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;respFunc&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;api&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;GenerateResponse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;error&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;resp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Response&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;client&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;req&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;respFunc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;log&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Fatal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Response:\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category></entry><entry><title>gemini-cli: Access Gemini models from the command-line</title><link href="https://eli.thegreenplace.net/2024/gemini-cli-access-gemini-models-from-the-command-line/" rel="alternate"></link><published>2024-02-21T06:04:00-08:00</published><updated>2024-02-21T14:06:19-08:00</updated><author><name>Eli Bendersky</name></author><id>tag:eli.thegreenplace.net,2024-02-21:/2024/gemini-cli-access-gemini-models-from-the-command-line/</id><summary type="html">&lt;p&gt;This post is about a new command-line tool I've recently built in Go -
&lt;a class="reference external" href="https://github.com/eliben/gemini-cli"&gt;gemini-cli&lt;/a&gt;, and how to use it for
LLM-based data analysis with Google's Gemini models.&lt;/p&gt;
&lt;p&gt;Background: I've been reading &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison's&lt;/a&gt; posts about LLMs with interest, especially his
work on tools that leverage LLMs and SQLite to create â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is about a new command-line tool I've recently built in Go -
&lt;a class="reference external" href="https://github.com/eliben/gemini-cli"&gt;gemini-cli&lt;/a&gt;, and how to use it for
LLM-based data analysis with Google's Gemini models.&lt;/p&gt;
&lt;p&gt;Background: I've been reading &lt;a class="reference external" href="https://simonwillison.net/"&gt;Simon Willison's&lt;/a&gt; posts about LLMs with interest, especially his
work on tools that leverage LLMs and SQLite to create &lt;a class="reference external" href="https://til.simonwillison.net/llms/openai-embeddings-related-content"&gt;fun little
analysis pipelines for local documents&lt;/a&gt;.
Since I've recently done some Go work on &lt;a class="reference external" href="https://github.com/google/generative-ai-go"&gt;Google's Gemini SDKs&lt;/a&gt; (also in &lt;a class="reference external" href="https://github.com/tmc/langchaingo"&gt;langchaingo&lt;/a&gt;)
and wrote a &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/using-gemini-models-from-go/"&gt;couple of&lt;/a&gt;
&lt;a class="reference external" href="https://eli.thegreenplace.net/2024/using-gemini-models-in-go-with-langchaingo/"&gt;blog posts&lt;/a&gt;
about it, I was interested in creating a similar pipeline for myself using
Go and Gemini models. This is how the idea for &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; was born.&lt;/p&gt;
&lt;div class="section" id="the-tool"&gt;
&lt;h2&gt;The tool&lt;/h2&gt;
&lt;p&gt;Like any Go command-line tool, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; is very easy to install:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ go install github.com/eliben/gemini-cli@latest
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And you're good to go! It will want a Gemini API key set in the
&lt;tt class="docutils literal"&gt;GEMINI_API_KEY&lt;/tt&gt; env var or passed with the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--key&lt;/span&gt;&lt;/tt&gt; flag. If you don't have
an API key yet, you can get one quickly and for free from &lt;a class="reference external" href="https://ai.google.dev/"&gt;https://ai.google.dev/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-motivating-task"&gt;
&lt;h2&gt;The motivating task&lt;/h2&gt;
&lt;p&gt;For a while I've been interested in adding a &amp;quot;related posts&amp;quot; feature to my blog.
It was clear that I'll want to use &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sentence_embedding"&gt;embeddings&lt;/a&gt; to convert my posts to
vector space and then use vector similarity to find related posts. Check
out my earlier &lt;a class="reference external" href="https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/"&gt;post on RAG&lt;/a&gt;
for additional information on these techniques.&lt;/p&gt;
&lt;p&gt;Before starting to write the code, however, I wanted to experiment with
a command-line tool so I could rapidly prototype. Think of it as crafting
some text processing pipeline from classical Unix command-line tools before
trying to implement it in a programming language. &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; excels for
precisely such prototyping.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="finding-related-posts"&gt;
&lt;h2&gt;Finding related posts&lt;/h2&gt;
&lt;p&gt;Let's see how to use &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; for my task. I have access to the contents
of my blog posts on the file system as a large bunch of
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/ReStructuredText"&gt;reStructuredText&lt;/a&gt; and HTML
files. These are private, but you're free to replicate this experiment for
any collection of textual documents you have handy. It will even work on
programming language source code!&lt;/p&gt;
&lt;p&gt;Let's first get the lay of the land - how many files are there &lt;a class="footnote-reference" href="#footnote-1" id="footnote-reference-1"&gt;[1]&lt;/a&gt;?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pss -f --rst content/|wc -l
279
$ pss -f --html content/|wc -l
1064
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;OK, so a bit over 1300 overall. Let's start by computing the embeddings for
the reST files. We'll ask &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; to write it into a new SQLite
DB called &lt;tt class="docutils literal"&gt;blogemb.db&lt;/tt&gt;, using its &lt;tt class="docutils literal"&gt;embed db&lt;/tt&gt; subcommand:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ export GEMINI_API_KEY=...
$ gemini-cli embed db blogemb.db --files content/,&amp;quot;*.rst&amp;quot;
Found 279 values to embed
Splitting to 9 batches
Embedding batch #1 / 9, size=32
Embedding batch #2 / 9, size=32
Embedding batch #3 / 9, size=32
Embedding batch #4 / 9, size=32
Embedding batch #5 / 9, size=32
Embedding batch #6 / 9, size=32
Embedding batch #7 / 9, size=32
Embedding batch #8 / 9, size=32
Embedding batch #9 / 9, size=23
Collected 279 embeddings; inserting into table embeddings
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let's look at the DB file using the &lt;tt class="docutils literal"&gt;sqlite3&lt;/tt&gt; command-line tool:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sqlite3 blogemb.db
SQLite version 3.37.2 2022-01-06 13:25:41
Enter &amp;quot;.help&amp;quot; for usage hints.

sqlite&amp;gt; .tables
embeddings

sqlite&amp;gt; .schema
CREATE TABLE embeddings (
id TEXT PRIMARY KEY,
embedding BLOB
);

sqlite&amp;gt; select count(*) from embeddings;
279

sqlite&amp;gt; select id, length(embedding) from embeddings limit 10;
content/2014/blogging-setup-with-pelican.rst|3072
content/2014/c++-perfect-forwarding-and-universal-references.rst|3072
content/2014/derivation-normal-equation-linear-regression.rst|3072
content/2014/goodbye-wordpress.rst|3072
content/2014/highlight-tab-gnome-terminal.rst|3072
content/2014/meshgrids-and-disambiguating-rows-and-columns-from-cartesian-coordinates.rst|3072
content/2014/samples-for-llvm-clang-library.rst|3072
content/2014/sfinae-and-enable-if.rst|3072
content/2014/summary-of-reading-july-september-2014.rst|3072
content/2014/summary-of-reading-october-december-2014.rst|3072
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As expected, we see 279 entries in the table; for each row the &lt;tt class="docutils literal"&gt;id&lt;/tt&gt; column
value is the path of the file and &lt;tt class="docutils literal"&gt;embedding&lt;/tt&gt; contains the embedding as a
blob. Embeddings are returned by the model as arrays of 32-bit floats, and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; encodes them into a blob as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;// encodeEmbedding encodes an embedding into a byte buffer, e.g. for DB&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// storage as a blob.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;encodeEmbedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt;&lt;span class="kt"&gt;byte&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nx"&gt;buf&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;bytes&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Buffer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;f&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;range&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;emb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;binary&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;buf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;binary&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;LittleEndian&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nb"&gt;panic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;buf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Bytes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Each &lt;tt class="docutils literal"&gt;float32&lt;/tt&gt; thus occupies 4 bytes; since our DB blobs are 3072 bytes long,
we can infer that each embedding vector has 768 elements; the embedding model
projects our text into 768-dimensional space &lt;a class="footnote-reference" href="#footnote-2" id="footnote-reference-2"&gt;[2]&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Back to our task, though. Note that &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; uses the batch-embedding
API of Gemini under the hood, so it's efficient for large input corpora. We
can control the batch size with a flag; just for fun, let's do this when
embedding the HTML files since there are so many of them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gemini-cli embed db blogemb.db --batch-size=64 --files content/,&amp;quot;*.html&amp;quot;
Found 1064 values to embed
Splitting to 17 batches
Embedding batch #1 / 17, size=64
Embedding batch #2 / 17, size=64
Embedding batch #3 / 17, size=64
Embedding batch #4 / 17, size=64
Embedding batch #5 / 17, size=64
Embedding batch #6 / 17, size=64
Embedding batch #7 / 17, size=64
Embedding batch #8 / 17, size=64
Embedding batch #9 / 17, size=64
Embedding batch #10 / 17, size=64
Embedding batch #11 / 17, size=64
Embedding batch #12 / 17, size=64
Embedding batch #13 / 17, size=64
Embedding batch #14 / 17, size=64
Embedding batch #15 / 17, size=64
Embedding batch #16 / 17, size=64
Embedding batch #17 / 17, size=40
Collected 1064 embeddings; inserting into table embeddings
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A brief note on performance: with a batch size of 64, this process took only
17 seconds - not bad for over a thousand documents. In the future I plan to
improve this time further with more concurrency and smarter batch size selection
&lt;a class="footnote-reference" href="#footnote-3" id="footnote-reference-3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's examine the resulting SQLite DB with all the embeddings:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ stat -c %s blogemb.db
5627904
$ echo &amp;quot;select count(*) from embeddings&amp;quot; | sqlite3 blogemb.db
1343
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All 1343 entries have made it into the &lt;tt class="docutils literal"&gt;embeddings&lt;/tt&gt; table, and the total
size of the DB is just over 5 MiB.&lt;/p&gt;
&lt;p&gt;Now we're ready to look for related posts. The &lt;tt class="docutils literal"&gt;embed similar&lt;/tt&gt; subcommand
takes the name of a SQLite DB that holds all embeddings (like the one we've just
created) and a string of content to compare; it also accepts &lt;tt class="docutils literal"&gt;-&lt;/tt&gt; as an
indication that the input content will be piped through standard input, so
let's use that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gemini-cli embed similar blogemb.db - &amp;lt; content/2023/better-http-server-routing-in-go-122.rst
{&amp;quot;id&amp;quot;:&amp;quot;content/2023/better-http-server-routing-in-go-122.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;1.0000001&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2021/rest-servers-in-go-part-2-using-a-router-package.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.8904768&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2021/life-of-an-http-request-in-a-go-server.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.83037585&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2021/rest-servers-in-go-part-5-middleware.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.8136583&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2022/serving-static-files-and-web-apps-in-go.rst&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7732284&amp;quot;}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The output is in &lt;a class="reference external" href="https://jsonlines.org/examples/"&gt;the JSON Lines format&lt;/a&gt;,
and by default prints the ID and the similarity score (using cosine
similarity), sorted by decreasing similarity. Unsurprisingly, the most similar
post is... itself, with a perfect similarity score of 1.0&lt;/p&gt;
&lt;p&gt;The results look pretty good! The most similar posts found indeed are very
relevant to the one we were asking about. For fun, let's try a book review and
now with a larger list of output candidates (by using the &lt;tt class="docutils literal"&gt;topk&lt;/tt&gt; flag):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ gemini-cli embed similar blogemb.db --topk=10 - &amp;lt; content/2011/book-review-the-voyage-of-the-beagle-by-charles-darwin.html
{&amp;quot;id&amp;quot;:&amp;quot;content/2011/book-review-the-voyage-of-the-beagle-by-charles-darwin.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;1&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2008/book-review-the-origin-of-species-by-charles-darwin.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.80570847&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2006/book-review-the-selfish-gene-by-richard-dawkins.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7845073&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2011/summary-of-reading-april-june-2011.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7939675&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2004/book-review-a-short-history-of-nearly-by-bill-bryson.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7784306&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2005/book-review-around-the-world-in-80-days-by-jules-verne.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7792236&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2005/book-review-the-double-helix-by-james-watson.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7658307&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2008/book-review-after-tamerlane-by-john-darwin.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7641713&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2005/book-review-mysterious-island-by-jules-verne.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.7605505&amp;quot;}
{&amp;quot;id&amp;quot;:&amp;quot;content/2008/book-review-the-adventures-of-tom-sawyer-by-mark-twain.html&amp;quot;,&amp;quot;score&amp;quot;:&amp;quot;0.75610566&amp;quot;}
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="what-s-next"&gt;
&lt;h2&gt;What's next&lt;/h2&gt;
&lt;p&gt;For my task, I now have the basic information available to implement it, and
all the infrastructure for running experiments; with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; in hand,
this took less than 5 minutes. All I needed to do is
&lt;a class="reference external" href="https://xkcd.com/1205/"&gt;write the tool&lt;/a&gt; :-)&lt;/p&gt;
&lt;p&gt;I really enjoyed building &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt;; it's true to the spirit of simple,
textual Unix CLIs that can be easily combined together through pipes. Using
SQLite as the storage and retrieval format is also quite pleasant, and provides
interoperability for free.&lt;/p&gt;
&lt;p&gt;For you - if you're a Go developer interested in building stuff with LLMs and
getting started for free - I hope you find &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; useful. I've only
shown its &lt;tt class="docutils literal"&gt;embed *&lt;/tt&gt; subcommands, but the CLI also lets you chat with an LLM
through the terminal, query the API for various model details, and everything
is configurable with extra flags.&lt;/p&gt;
&lt;p&gt;It's &lt;a class="reference external" href="https://github.com/eliben/gemini-cli"&gt;open-source&lt;/a&gt;, of course; the
README file rendered on GitHub has extensive documentation, and more is
available by running &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt; help&lt;/tt&gt;. Try it, ask questions, open issues!&lt;/p&gt;
&lt;hr class="docutils" /&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-1" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;I like using &lt;a class="reference external" href="https://github.com/eliben/pss/"&gt;pss&lt;/a&gt;, but feel free
to use your favorite tools - &lt;tt class="docutils literal"&gt;git grep&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;ag&lt;/tt&gt; or just a concoction
of &lt;tt class="docutils literal"&gt;find&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;grep&lt;/tt&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;p class="first"&gt;A word of caution: LLMs have limited context window sizes; for embeddings,
if the input is larger than the model's context window it may get
truncated - so it's the user's responsibility to ensure that input
documents are properly sized.&lt;/p&gt;
&lt;p class="last"&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt;&lt;/tt&gt; will report the maximal number of input tokens for
supported models when you invoke the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gemini-cli&lt;/span&gt; models&lt;/tt&gt; command.&lt;/p&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="footnote-3" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#footnote-reference-3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;We have to be careful with too much parallelism, because at the free
tier the Gemini SDK may be rate-limited.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</content><category term="misc"></category><category term="Go"></category><category term="Machine Learning"></category><category term="Software &amp; Tools"></category></entry></feed>