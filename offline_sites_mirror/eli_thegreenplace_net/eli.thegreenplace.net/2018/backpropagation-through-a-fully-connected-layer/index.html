<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2018/backpropagation-through-a-fully-connected-layer/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:46:11 GMT -->
<head>
    <title>Backpropagation through a fully-connected layer - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../theme/css/style.css" type="text/css"/>

        <link href="../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../index.html" class="navbar-brand">
                <img src="../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="index.html"
                       rel="bookmark"
                       title="Permalink to Backpropagation through a fully-connected layer">
                        Backpropagation through a fully-connected layer
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> May 22, 2018 at 05:47</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../tag/math.html">Math</a>
        ,
    <a href="../../tag/machine-learning.html">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>The goal of this post is to show the math of backpropagating a derivative for a
fully-connected (FC) neural network layer consisting of matrix multiplication
and bias addition. I have briefly mentioned this in an <a class="reference external" href="../../2016/the-softmax-function-and-its-derivative.html">earlier post dedicated
to Softmax</a>,
but here I want to give some more attention to FC layers specifically.</p>
<p>Here is a fully-connected layer for input vectors with <em>N</em> elements, producing
output vectors with <em>T</em> elements:</p>
<img alt="Diagram of a fully connected layer" class="align-center" src="../../images/2018/fclayer.png" />
<p>As a formula, we can write:</p>
<object class="align-center" data="../../images/math/0f980ab4c97ad86b4d0a15ede6e9c05901323702.svg" style="height: 17px;" type="image/svg+xml">\[y=Wx+b\]</object>
<p>Presumably, this layer is part of a network that ends up computing some loss
<em>L</em>. We'll assume we already have the derivative of the loss w.r.t. the output
of the layer <object class="valign-m9" data="../../images/math/9a5154f5e8d64cc77db745d8d3baa723bc6df829.svg" style="height: 26px;" type="image/svg+xml">\frac{\partial{L}}{\partial{y}}</object>.</p>
<p>We'll be interested in two other derivatives:
<object class="valign-m7" data="../../images/math/33d2709b664fdd69317758b433b61b13c1cdc62f.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{W}}</object> and
<object class="valign-m7" data="../../images/math/5f12a50803653cf2ee02135944343ec70506d31c.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{b}}</object>.</p>
<div class="section" id="jacobians-and-the-chain-rule">
<h2>Jacobians and the chain rule</h2>
<p>As a reminder from <a class="reference external" href="../../2016/the-chain-rule-of-calculus.html">The Chain Rule of Calculus</a>,
we're dealing with functions that map from <em>n</em> dimensions to <em>m</em> dimensions:
<img alt="f:\mathbb{R}^{n} \to \mathbb{R}^{m}" class="valign-m4" src="../../images/math/13f219789047343729036279bb11630db317d98d.png" style="height: 16px;" />. We'll consider the outputs of <em>f</em>
to be numbered from 1 to <em>m</em> as <img alt="f_1,f_2 \dots f_m" class="valign-m4" src="../../images/math/93b446c5209263534d09d617bbede21101d6536e.png" style="height: 16px;" />. For each such
<img alt="f_i" class="valign-m4" src="../../images/math/68bd0dc647944d362ec8df628a22967b91d82c80.png" style="height: 16px;" /> we can compute its partial derivative by any of the <em>n</em> inputs as:</p>
<img alt="\[D_j f_i(a)=\frac{\partial f_i}{\partial a_j}(a)\]" class="align-center" src="../../images/math/30881b5a92e45259714ba01c7a12fbf8f6c56109.png" style="height: 42px;" />
<p>Where <em>j</em> goes from 1 to <em>n</em> and <em>a</em> is a vector with <em>n</em> components. If <em>f</em>
is differentiable at <em>a</em> then the derivative of <em>f</em> at <em>a</em> is the <em>Jacobian
matrix</em>:</p>
<img alt="\[Df(a)=\begin{bmatrix} D_1 f_1(a) &amp;amp; \cdots &amp;amp; D_n f_1(a) \\ \vdots &amp;amp;  &amp;amp; \vdots \\ D_1 f_m(a) &amp;amp; \cdots &amp;amp; D_n f_m(a) \\ \end{bmatrix}\]" class="align-center" src="../../images/math/ab09367d48e9ef4d8bc2314a60313dec700193af.png" style="height: 76px;" />
<p>The multivariate chain rule states: given <img alt="g:\mathbb{R}^n \to \mathbb{R}^m" class="valign-m4" src="../../images/math/b4b7d25491897b053abf7e48688fada4a85368bd.png" style="height: 16px;" />
and <img alt="f:\mathbb{R}^m \to \mathbb{R}^p" class="valign-m4" src="../../images/math/ac8a6cea4e02e885538fc3ef969c5733e84712f9.png" style="height: 16px;" /> and a point <img alt="a \in \mathbb{R}^n" class="valign-m1" src="../../images/math/43a85f2c59f396fe5c4e2c403a0453c463fcfb0d.png" style="height: 13px;" />,
if <em>g</em> is differentiable at <em>a</em> and <em>f</em> is differentiable at <img alt="g(a)" class="valign-m4" src="../../images/math/e7373233d49e18a0882e0dce41d9d6aa26964d6b.png" style="height: 18px;" /> then
the composition <img alt="f \circ g" class="valign-m4" src="../../images/math/1247a6ac0bc07bfdbd790831aa70b0b000bad2e4.png" style="height: 16px;" /> is differentiable at <em>a</em> and its derivative
is:</p>
<img alt="\[D(f \circ g)(a)=Df(g(a)) \cdot Dg(a)\]" class="align-center" src="../../images/math/00bdefa904bd34df2dfb50cc385e6497c4e5096e.png" style="height: 18px;" />
<p>Which is the matrix multiplication of <img alt="Df(g(a))" class="valign-m4" src="../../images/math/e567730c48bb2f95c258b630b4d6e997043e09ab.png" style="height: 18px;" /> and <img alt="Dg(a)" class="valign-m4" src="../../images/math/2575fc98e794a733a7aa6237fe67246a41e6c8c5.png" style="height: 18px;" />.</p>
</div>
<div class="section" id="back-to-the-fully-connected-layer">
<h2>Back to the fully-connected layer</h2>
<p>Circling back to our fully-connected layer, we have the loss <object class="valign-m4" data="../../images/math/abf7408ae6d9fb4683480735dc1ebc8555b8fef8.svg" style="height: 18px;" type="image/svg+xml">L(y)</object> - a
scalar function <object class="valign-m1" data="../../images/math/ddef8b9ca23fb246b2a984c719d812f37a41a406.svg" style="height: 16px;" type="image/svg+xml">L:\mathbb{R}^{T} \to \mathbb{R}</object>. We also have the
function <object class="valign-m4" data="../../images/math/f09c295439296549a068b64ffe69a48dd77d1078.svg" style="height: 17px;" type="image/svg+xml">y=Wx+b</object>. If we're interested in the derivative w.r.t the
weights, what are the dimensions of this function? Our &quot;variable part&quot; is then
<em>W</em>, which has <em>NT</em> elements overall, and the output has <em>T</em> elements, so
<object class="valign-m4" data="../../images/math/06178f1d07375b8286afcd48f02bcd34d71537f0.svg" style="height: 19px;" type="image/svg+xml">y:\mathbb{R}^{NT} \to \mathbb{R}^{T}</object> <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a>.</p>
<p>The chain rule tells us how to compute the derivative of <em>L</em> w.r.t. <em>W</em>:</p>
<object class="align-center" data="../../images/math/ee6bc25a34980031f93f0c7eefccc40663b05c76.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial{L}}{\partial{W}}=D(L \circ y)(W)=DL(y(W)) \cdot Dy(W)\]</object>
<p>Since we're backpropagating, we already know <object class="valign-m4" data="../../images/math/dcb2eda345045dac22c425a1ee19113e047126cf.svg" style="height: 18px;" type="image/svg+xml">DL(y(W))</object>; because of the
dimensionality of the <em>L</em> function, the dimensions of <object class="valign-m4" data="../../images/math/dcb2eda345045dac22c425a1ee19113e047126cf.svg" style="height: 18px;" type="image/svg+xml">DL(y(W))</object> are
[1,T] (one row, <em>T</em> columns). <object class="valign-m4" data="../../images/math/d0064a180ddb231bb6868ce25c68ef3ec1c2a464.svg" style="height: 18px;" type="image/svg+xml">y(W)</object> has <em>NT</em> inputs and <em>T</em> outputs,
so the dimensions of <object class="valign-m4" data="../../images/math/b22fe7345e02ae50c68605696f3a447435cd1f9d.svg" style="height: 18px;" type="image/svg+xml">Dy(W)</object> are [T,NT]. Overall, the dimensions of
<object class="valign-m4" data="../../images/math/2f6b6eded4ba20b3eeb59b2b687f84de1e91c04c.svg" style="height: 18px;" type="image/svg+xml">D(L \circ y)(W)</object> are then [1,NT]. This makes sense if you think about it,
because as a function of <em>W</em>, the loss has <em>NT</em> inputs and a single scalar
output.</p>
<p>What remains is to compute <object class="valign-m4" data="../../images/math/b22fe7345e02ae50c68605696f3a447435cd1f9d.svg" style="height: 18px;" type="image/svg+xml">Dy(W)</object>, the Jacobian of <em>y</em> w.r.t. <em>W</em>. As
mentioned above, it has <em>T</em> rows - one for each output element of <em>y</em>, and <em>NT</em>
columns - one for each element in the weight matrix <em>W</em>. Computing such a large
Jacobian may seem daunting, but we'll soon see that it's very easy to generalize
from a simple example. Let's start with <object class="valign-m4" data="../../images/math/6a53741f2a8810da3cae4efadde63c8e7ee2662f.svg" style="height: 12px;" type="image/svg+xml">y_1</object>:</p>
<object class="align-center" data="../../images/math/7190e002ac69968b674aecacfd5a8531ad9cd208.svg" style="height: 55px;" type="image/svg+xml">\[y_1=\sum_{j=1}^{N}W_{1,j}x_{j}+b_1\]</object>
<p>What's the derivative of this result element w.r.t. each element in <em>W</em>? When
the element is in row 1, the derivative is <object class="valign-m6" data="../../images/math/73058e43db0f4edc791b10f27f913cbc5d361ab6.svg" style="height: 14px;" type="image/svg+xml">x_j</object> (<em>j</em> being the column
of <em>W</em>); when the element is in any other row, the derivative is 0.</p>
<p>Similarly for <object class="valign-m4" data="../../images/math/b9f59182e34baa532fa4e27471acc714f3105d16.svg" style="height: 12px;" type="image/svg+xml">y_2</object>, we'll have non-zero derivatives only for the second
row of <em>W</em> (with the same result of <object class="valign-m6" data="../../images/math/73058e43db0f4edc791b10f27f913cbc5d361ab6.svg" style="height: 14px;" type="image/svg+xml">x_j</object> being the derivative for the
<em>j</em>-th column), and so on.</p>
<p>Generalizing from the example, if we split the index of <em>W</em> to <em>i</em> and <em>j</em>, we
get:</p>
<object class="align-center" data="../../images/math/e28e0d3b44645eb299cceae8dde2319244e86373.svg" style="height: 50px;" type="image/svg+xml">\[\begin{align}
D_{ij}y_t&amp;=\frac{\partial(\sum_{j=1}^{N}W_{t,j}x_{j}+b_t)}{\partial W_{ij}}
         &amp;= \left\{\begin{matrix}
x_j &amp; i = t\\
0 &amp; i \ne t
\end{matrix}\right.
\end{align*}\]</object>
<p>This goes into row <em>t</em>, column <object class="valign-m4" data="../../images/math/ef7b2d987af3c0ceb75381d096c35e8c19085642.svg" style="height: 18px;" type="image/svg+xml">(i-1)N+j</object> in the Jacobian matrix. Overall,
we get the following Jacobian matrix with shape [T,NT]:</p>
<object class="align-center" data="../../images/math/8a59a6251d12196f12eaadb6537289e3a6368d53.svg" style="height: 76px;" type="image/svg+xml">\[Dy=\begin{bmatrix}
x_1 &amp; x_2 &amp; \cdots &amp; x_N &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; x_1 &amp; x_2 &amp; \cdots &amp; x_N
\end{bmatrix}\]</object>
<p>Now we're ready to finally multiply the Jacobians together to complete the
chain rule:</p>
<object class="align-center" data="../../images/math/2e9823350972d4874d201a0f8232d89fea710c6f.svg" style="height: 18px;" type="image/svg+xml">\[D(L \circ y)(W)=DL(y(W)) \cdot Dy(W)\]</object>
<p>The left-hand side is this row vector:</p>
<object class="align-center" data="../../images/math/af6d7af820f16b7493d378e8a40daa87031591f4.svg" style="height: 41px;" type="image/svg+xml">\[DL(y)=(\frac{\partial L}{\partial y_1}, \frac{\partial L}{\partial y_2},\cdots,\frac{\partial L}{\partial y_T})\]</object>
<p>And we're multiplying it by the matrix <object class="valign-m4" data="../../images/math/baf7d8b700759b28ece347bd62793400ef52a8e0.svg" style="height: 16px;" type="image/svg+xml">Dy</object> shown above. Each item in the
result vector will be a dot product between <object class="valign-m4" data="../../images/math/992673c682a388ea8231ebbd8ea28c9cecae874d.svg" style="height: 18px;" type="image/svg+xml">DL(y)</object> and the corresponding
column in the matrix <object class="valign-m4" data="../../images/math/baf7d8b700759b28ece347bd62793400ef52a8e0.svg" style="height: 16px;" type="image/svg+xml">Dy</object>. Since <object class="valign-m4" data="../../images/math/baf7d8b700759b28ece347bd62793400ef52a8e0.svg" style="height: 16px;" type="image/svg+xml">Dy</object> has a single non-zero element
in each column, the result is fairly trivial. The first <em>N</em> entries are:</p>
<object class="align-center" data="../../images/math/d0e87e4d5cd9feb9d93d153733a182426d175d7e.svg" style="height: 41px;" type="image/svg+xml">\[\frac{\partial L}{\partial y_1}x_1,
\frac{\partial L}{\partial y_1}x_2,
\cdots,
\frac{\partial L}{\partial y_1}x_N\]</object>
<p>The next <em>N</em> entries are:</p>
<object class="align-center" data="../../images/math/5bd4d8b6b4eb817b071dbf4ddda71680f4bf0392.svg" style="height: 41px;" type="image/svg+xml">\[\frac{\partial L}{\partial y_2}x_1,
\frac{\partial L}{\partial y_2}x_2,
\cdots,
\frac{\partial L}{\partial y_2}x_N\]</object>
<p>And so on, until the last (<em>T</em>-th) set of <em>N</em> entries is all <em>x</em>-es multiplied
by <object class="valign-m9" data="../../images/math/b44681f2ca721dae2b24a49d88f01463e3a88e50.svg" style="height: 26px;" type="image/svg+xml">\frac{\partial L}{\partial y_T}</object>.</p>
<p>To better see how to apply each derivative to a corresponding element in <em>W</em>, we
can &quot;re-roll&quot; this result back into a matrix of shape [T,N]:</p>
<object class="align-center" data="../../images/math/7cfccbaaa844f8ae994f8e012f12557919927e31.svg" style="height: 129px;" type="image/svg+xml">\[\frac{\partial{L}}{\partial{W}}=D(L\circ y)(W)=\begin{bmatrix}
\frac{\partial L}{\partial y_1}x_1 &amp; \frac{\partial L}{\partial y_1}x_2 &amp; \cdots &amp; \frac{\partial L}{\partial y_1}x_N \\ \\
\frac{\partial L}{\partial y_2}x_1 &amp; \frac{\partial L}{\partial y_2}x_2 &amp; \cdots &amp; \frac{\partial L}{\partial y_2}x_N \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial L}{\partial y_T}x_1 &amp; \frac{\partial L}{\partial y_T}x_2 &amp; \cdots &amp; \frac{\partial L}{\partial y_T}x_N
\end{bmatrix}\]</object>
</div>
<div class="section" id="computational-cost-and-shortcut">
<h2>Computational cost and shortcut</h2>
<p>While the derivation shown above is complete and mathematically correct, it can
also be computationally intensive; in realistic scenarios, the full Jacobian
matrix can be <em>really</em> large. For example, let's say our input is a (modestly
sized) 128x128 image, so <em>N=16,384</em>. Let's also say that <em>T=100</em>. The weight
matrix then has <em>NT=1,638,400</em> elements; respectably big, but nothing out of the
ordinary.</p>
<p>Now consider the size of the full Jacobian matrix: it's <em>T</em> by <em>NT</em>, or over
160 million elements. At 4 bytes per element that's more than half a GiB!</p>
<p>Moreover, to compute every backpropagation we'd be forced to multiply this full
Jacobian matrix by a 100-dimensional vector, performing 160 million
multiply-and-add operations for the dot products. That's a lot of compute.</p>
<p>But the final result <object class="valign-m4" data="../../images/math/89da23923a43fcd95b185bebb6fd362b6d1ac695.svg" style="height: 18px;" type="image/svg+xml">D(L\circ y)(W)</object> is the size of <em>W</em> - 1.6 million
elements. Do we really need 160 million computations to get to it? No, because
the Jacobian is very <em>sparse</em> - most of it is zeros. And in fact, when we look
at the <object class="valign-m4" data="../../images/math/89da23923a43fcd95b185bebb6fd362b6d1ac695.svg" style="height: 18px;" type="image/svg+xml">D(L\circ y)(W)</object> found above - it's fairly straightforward to
compute using a single multiplication per element.</p>
<p>Moreover, if we stare at the <object class="valign-m7" data="../../images/math/33d2709b664fdd69317758b433b61b13c1cdc62f.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{W}}</object> matrix a
bit, we'll notice it has a familiar pattern: this is just the <a class="reference external" href="https://en.wikipedia.org/wiki/Outer_product">outer product</a> between the vectors
<object class="valign-m9" data="../../images/math/9a5154f5e8d64cc77db745d8d3baa723bc6df829.svg" style="height: 26px;" type="image/svg+xml">\frac{\partial{L}}{\partial{y}}</object> and <em>x</em>:</p>
<object class="align-center" data="../../images/math/89bdcdf27feb489a5e3cb1bb8adc7faffcf0207d.svg" style="height: 41px;" type="image/svg+xml">\[\frac{\partial L}{\partial W}=\frac{\partial L}{\partial y}\otimes x\]</object>
<p>If we have to compute this backpropagation in Python/Numpy, we'll likely write
code similar to:</p>
<div class="highlight"><pre><span></span><span class="c1"># Assuming dy (gradient of loss w.r.t. y) and x are column vectors, by</span>
<span class="c1"># performing a dot product between dy (column) and x.T (row) we get the</span>
<span class="c1"># outer product.</span>
<span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="bias-gradient">
<h2>Bias gradient</h2>
<p>We've just seen how to compute weight gradients for a fully-connected layer.
Computing the gradients for the bias vector is very similar, and a bit simpler.</p>
<p>This is the chain rule equation applied to the bias vector:</p>
<object class="align-center" data="../../images/math/3aee48692aeafe9ffab07037ad374f4c803787a7.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial{L}}{\partial{b}}=D(L \circ y)(b)=DL(y(b)) \cdot Dy(b)\]</object>
<p>The shapes involved here are: <object class="valign-m4" data="../../images/math/ef9baa4141fed9b40c4f1b0ebf189e4d8d28badc.svg" style="height: 18px;" type="image/svg+xml">DL(y(b))</object> is still [1,T], because the
number of elements in <em>y</em> remains <em>T</em>. <object class="valign-m4" data="../../images/math/966d0c5b07f027b02b0ca9eb418ed9ac12f63386.svg" style="height: 18px;" type="image/svg+xml">Dy(b)</object> has <em>T</em> inputs (bias
elements) and <em>T</em> outputs (<em>y</em> elements), so its shape is [T,T]. Therefore, the
shape of the gradient <object class="valign-m4" data="../../images/math/7640378fb78362268ffe48bf5d68a266211673e4.svg" style="height: 18px;" type="image/svg+xml">D(L \circ y)(b)</object> is [1,T].</p>
<p>To see how we'd fill the Jacobian matrix <object class="valign-m4" data="../../images/math/966d0c5b07f027b02b0ca9eb418ed9ac12f63386.svg" style="height: 18px;" type="image/svg+xml">Dy(b)</object>, let's go back to the
formula for <em>y</em>:</p>
<object class="align-center" data="../../images/math/7190e002ac69968b674aecacfd5a8531ad9cd208.svg" style="height: 55px;" type="image/svg+xml">\[y_1=\sum_{j=1}^{N}W_{1,j}x_{j}+b_1\]</object>
<p>When derived by anything other than <object class="valign-m4" data="../../images/math/c7cd24d955e66b8fe5ce45ded69fd98da5c68ba8.svg" style="height: 17px;" type="image/svg+xml">b_1</object>, this would be 0; when derived
by <object class="valign-m4" data="../../images/math/c7cd24d955e66b8fe5ce45ded69fd98da5c68ba8.svg" style="height: 17px;" type="image/svg+xml">b_1</object> the result is 1. The same applies to every other element of <em>y</em>:</p>
<object class="align-center" data="../../images/math/0511363d44324e04aee704c9cc3094a4e8c8c108.svg" style="height: 44px;" type="image/svg+xml">\[\frac{\partial y_i}{\partial b_j}=\left\{\begin{matrix}
1 &amp; i=j \\
0 &amp; i\neq j
\end{matrix}\right\]</object>
<p>In matrix form, this is just an identity matrix with dimensions [T,T].
Therefore:</p>
<object class="align-center" data="../../images/math/a64bd506727621a3e78444f7e158769dae30f93b.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial{L}}{\partial{b}}=D(L \circ y)(b)=DL(y(b)) \cdot I =DL(y(b))\]</object>
<p>For a given element of <em>b</em>, its gradient is just the corresponding element in
<object class="valign-m9" data="../../images/math/f004c6bbe71887354e0aad67dd7cbe6650eb58e9.svg" style="height: 26px;" type="image/svg+xml">\frac{\partial L}{\partial y}</object>.</p>
</div>
<div class="section" id="fully-connected-layer-for-a-batch-of-inputs">
<h2>Fully-connected layer for a batch of inputs</h2>
<p>The derivation shown above applies to a FC layer with a single input vector <em>x</em>
and a single output vector <em>y</em>. When we train models, we almost always try to
do so in <em>batches</em> (or <em>mini-batches</em>) to better leverage the parallelism of
modern hardware. So a more typical layer computation would be:</p>
<object class="align-center" data="../../images/math/c26a36b850dd7b2e4288e475a590f343ec3a18a3.svg" style="height: 15px;" type="image/svg+xml">\[Y=WX+b\]</object>
<p>Where the shape of <em>X</em> is [N,B]; <em>B</em> is the batch size, typically a
not-too-large power of 2, like 32. <em>W</em> and <em>b</em> still have the same shapes, so
the shape of <em>Y</em> is [T,B]. Each column in <em>X</em> is a new input vector (for a
total of <em>B</em> vectors in a batch); a corresponding column in <em>Y</em> is the output.</p>
<p>As before, given <object class="valign-m7" data="../../images/math/d5de3c7d9e0e1bcb4f6c00ea06b4ad808d2ea998.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{Y}}</object>, our goal is to find
<object class="valign-m7" data="../../images/math/33d2709b664fdd69317758b433b61b13c1cdc62f.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{W}}</object> and
<object class="valign-m7" data="../../images/math/5f12a50803653cf2ee02135944343ec70506d31c.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{b}}</object>. While the end results are fairly simple
and pretty much what you'd expect, I still want to go through the full Jacobian
computation to show how to find the gradiends in a rigorous way.</p>
<p>Starting with the weigths, the chain rule is:</p>
<object class="align-center" data="../../images/math/0ffed6ae9645ea6bd0d02932e2f0ca20fb8e7bc6.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial{L}}{\partial{W}}=D(L \circ Y)(W)=DL(Y(W)) \cdot DY(W)\]</object>
<p>The dimensions are:</p>
<ul class="simple">
<li><object class="valign-m4" data="../../images/math/86485acc2c4461f7817626204bf6c9148dad9d87.svg" style="height: 18px;" type="image/svg+xml">DL(Y(W))</object>: [1,TB] because <em>Y</em> has <em>T</em> outputs for each input vector in
the batch.</li>
<li><object class="valign-m4" data="../../images/math/573b889d69d85759886840570c6970345209b332.svg" style="height: 18px;" type="image/svg+xml">DY(W)</object>: [TB,TN] since <object class="valign-m4" data="../../images/math/fe5551953a6c071c738578f2ebc316864078cc81.svg" style="height: 18px;" type="image/svg+xml">Y(W)</object> has <em>TB</em> outputs and <em>TN</em>
inputs overall.</li>
<li><object class="valign-m4" data="../../images/math/b214435777e236879c609900ba7a118e9f0da022.svg" style="height: 18px;" type="image/svg+xml">D(L\circ Y)(W)</object>: [1,TN] same as in the batch-1 case, because the same
weight matrix is used for all inputs in the batch.</li>
</ul>
<p>Also, we'll use the notation <object class="valign-m5" data="../../images/math/5f40e2ad50a0eb5c2f5019c48563f9c6605f84b6.svg" style="height: 24px;" type="image/svg+xml">x_{i}^{[b]}</object> to talk about the <em>i</em>-th
element in the <em>b</em>-th input vector <em>x</em> (out of a total of <em>B</em> such input
vectors).</p>
<p>With this in hand, let's see how the Jacobians look; starting with
<object class="valign-m4" data="../../images/math/86485acc2c4461f7817626204bf6c9148dad9d87.svg" style="height: 18px;" type="image/svg+xml">DL(Y(W))</object>, it's the same as before except that we have to take the batch
into account. Each batch element is independent of the others in loss
computations, so we'll have:</p>
<object class="align-center" data="../../images/math/943d68b7dbda5009cbfd597b4e0fcc46748204a5.svg" style="height: 47px;" type="image/svg+xml">\[\frac{\partial L}{\partial y_{i}^{[b]}}\]</object>
<p>As the Jacobian element; how do we arrange them in a 1-dimensional vector with
shape [1,TB]? We'll just have to agree on a linearization here - same as we did
with <em>W</em> before. We'll go for row-major again, so in 1-D the array <em>Y</em> would be:</p>
<object class="align-center" data="../../images/math/13122f18953df14ebb5ff74f59441194d3adb445.svg" style="height: 26px;" type="image/svg+xml">\[Y=(y_{1}^{[1]},y_{1}^{[2]},\cdots,y_{1}^{[B]},
   y_{2}^{[1]},y_{2}^{[2]},\cdots,y_{2}^{[B]},\cdots)\]</object>
<p>And so on for <em>T</em> elements. Therefore, the Jacobian of <em>L</em> w.r.t <em>Y</em> is:</p>
<object class="align-center" data="../../images/math/4e79e866dbfbe0d6b6de5f6617762bce00d5f61f.svg" style="height: 48px;" type="image/svg+xml">\[\frac{\partial L}{\partial Y}=(
  \frac{\partial L}{\partial y_{1}^{[1]}},
  \frac{\partial L}{\partial y_{1}^{[2]}},\cdots,
  \frac{\partial L}{\partial y_{1}^{[B]}},
  \frac{\partial L}{\partial y_{2}^{[1]}},
  \frac{\partial L}{\partial y_{2}^{[2]}},\cdots,
  \frac{\partial L}{\partial y_{2}^{[B]}},\cdots)\]</object>
<p>To find <object class="valign-m4" data="../../images/math/573b889d69d85759886840570c6970345209b332.svg" style="height: 18px;" type="image/svg+xml">DY(W)</object>, let's first see how to compute <em>Y</em>. The <em>i</em>-th element
of <em>Y</em> for batch <em>b</em> is:</p>
<object class="align-center" data="../../images/math/2b9c88f44b9cbec2343ce49418ca3e17dd2e0946.svg" style="height: 55px;" type="image/svg+xml">\[y_{i}^{[b]}=\sum_{j=1}^{N}W_{i,j}x_{j}^{[b]}+b_i\]</object>
<p>Recall that the Jacobian <object class="valign-m4" data="../../images/math/573b889d69d85759886840570c6970345209b332.svg" style="height: 18px;" type="image/svg+xml">DY(W)</object> now has shape [TB,TN]. Previously we had
to unroll the [T,N] of the weight matrix into the rows. Now we'll also have to
unrill the [T,B] of the output into the columns. As before, first all <em>b</em>-s for
<em>t=1</em>, then all <em>b</em>-s for <em>t=2</em>, etc. If we carefully compute the derivative,
we'll see that the Jacobian matrix has similar structure to the single-batch
case, just with each line repeated <em>B</em> times for each of the batch elements:</p>
<object class="align-center" data="../../images/math/4fe86285c11962226758ecfad2839b2ce6520d2d.svg" style="height: 291px;" type="image/svg+xml">\[DY(W)=\begin{bmatrix}
x_{1}^{[1]} &amp; x_{2}^{[1]} &amp; \cdots &amp; x_{N}^{[1]} &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \\
x_{1}^{[2]} &amp; x_{2}^{[2]} &amp; \cdots &amp; x_{N}^{[2]} &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\
x_{1}^{[B]} &amp; x_{2}^{[B]} &amp; \cdots &amp; x_{N}^{[B]} &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; x_{1}^{[1]} &amp; x_{2}^{[1]} &amp; \cdots &amp; x_{N}^{[1]} \\ \\
0 &amp; 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; x_{1}^{[2]} &amp; x_{2}^{[2]} &amp; \cdots &amp; x_{N}^{[2]} \\
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; x_{1}^{[B]} &amp; x_{2}^{[B]} &amp; \cdots &amp; x_{N}^{[B]} \\
\end{bmatrix}\]</object>
<p>Multiplying the two Jacobians together we get the full gradient of <em>L</em> w.r.t.
each element in the weight matrix. Where previously (in the non-batch case) we
had:</p>
<object class="align-center" data="../../images/math/1c762a65e0003e82f7dc5108f23126989b64112b.svg" style="height: 42px;" type="image/svg+xml">\[\frac{\partial L}{\partial W_{ij}}=\frac{\partial L}{\partial y_i}x_j\]</object>
<p>Now, instead, we'll have:</p>
<object class="align-center" data="../../images/math/14fb5d857d1e43a9f692ad436c448a43c0ea041f.svg" style="height: 54px;" type="image/svg+xml">\[\frac{\partial L}{\partial W_{ij}}=\sum_{b=1}^{B}\frac{\partial L}{\partial y_{i}^{[b]}}x_{j}^{[b]}\]</object>
<p>Which makes total sense, since it's simply taking the loss gradient computed
from each batch separately and adds them up. This aligns with our intuition of
how gradient for a whole batch is computed - compute the gradient for each batch
element separately and add up all the gradients <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a>.</p>
<p>As before, there's a clever way to express the final gradient using matrix
operations. Note the sum across all batch elements when computing
<object class="valign-m10" data="../../images/math/2d41c4c820515c93e916d32532b9bdc7012e8121.svg" style="height: 27px;" type="image/svg+xml">\frac{\partial L}{\partial W_{ij}}</object>. We can express this as the matrix
multiplication:</p>
<object class="align-center" data="../../images/math/7f6c176f28de451b2d67fcf7ebf238122de9a970.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial L}{\partial W}=\frac{\partial L}{\partial Y}\cdot X^T\]</object>
<p>This is a good place to recall the computation cost again. Previously we've seen
that for a single-input case, the Jacobian can be extremely large ([T,NT] having
about 160 million elements). In the batch case, the Jacobian would be even
larger since its shape is [TB,NT]; with a reasonable batch of 32, it's something
like 5-billion elements strong. It's good that we don't actually have to hold
the full Jacobian in memory and have a shortcut way of computing the gradient.</p>
</div>
<div class="section" id="bias-gradient-for-a-batch">
<h2>Bias gradient for a batch</h2>
<p>For the bias, we have:</p>
<object class="align-center" data="../../images/math/1228909398e9618223e551b8b1d394ac20d697f1.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial{L}}{\partial{b}}=D(L \circ Y)(b)=DL(Y(b)) \cdot DY(b)\]</object>
<p><object class="valign-m4" data="../../images/math/40699cf4e67bde5205359e04102f7b0011dac800.svg" style="height: 18px;" type="image/svg+xml">DL(Y(b))</object> here has the shape [1,TB]; <object class="valign-m4" data="../../images/math/f260ada7edc55af13f145e5786803198a3452f1e.svg" style="height: 18px;" type="image/svg+xml">DY(b)</object> has the shape [TB,T].
Therefore, the shape of <object class="valign-m7" data="../../images/math/5f12a50803653cf2ee02135944343ec70506d31c.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{b}}</object> is [1,T], as
before.</p>
<p>From the formula for computing <em>Y</em>:</p>
<object class="align-center" data="../../images/math/7190e002ac69968b674aecacfd5a8531ad9cd208.svg" style="height: 55px;" type="image/svg+xml">\[y_1=\sum_{j=1}^{N}W_{1,j}x_{j}+b_1\]</object>
<p>We get, for any batch <em>b</em>:</p>
<object class="align-center" data="../../images/math/7ad81febc17ec33d21c8fba2a2e6956a8b43e1ad.svg" style="height: 49px;" type="image/svg+xml">\[\frac{\partial y_{i}^{[b]}}{\partial b_j}=\left\{\begin{matrix}
1 &amp; i=j \\
0 &amp; i\neq j
\end{matrix}\right\]</object>
<p>So, whereas <object class="valign-m4" data="../../images/math/f260ada7edc55af13f145e5786803198a3452f1e.svg" style="height: 18px;" type="image/svg+xml">DY(b)</object> was an identity matrix in the no-batch case, here it
looks like this:</p>
<object class="align-center" data="../../images/math/4b8148fb1343ab283fa0f8b0cdb6f3723201df15.svg" style="height: 267px;" type="image/svg+xml">\[DY(b)=\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\
\end{bmatrix}\]</object>
<p>With <em>B</em> identical rows at a time, for a total of <em>TB</em> rows. Since
<object class="valign-m7" data="../../images/math/c7d9499ae5d7e1fc81bc540909deac668210911d.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial L}{\partial Y}</object> is the same as before, their matrix
multiplication result has this in column <em>j</em>:</p>
<object class="align-center" data="../../images/math/910c6d77b9356303c0a01896a09d62fe2963d8ac.svg" style="height: 57px;" type="image/svg+xml">\[\frac{\partial{L}}{\partial{b_j}}=\sum_{b=1}^{B}\frac{\partial L}{\partial y_{j}^{[b]}}\]</object>
<p>Which just means adding up the gradient effects from every batch element
independently.</p>
</div>
<div class="section" id="addendum-gradient-w-r-t-x">
<h2>Addendum - gradient w.r.t. x</h2>
<p>This post started by explaining that the parameters of a fully-connected layer
we're usually looking to optimize are the weight matrix and bias. In most cases
this is true; however, in some other cases we're actually interested in
propagating a gradient through <em>x</em> - often when there are more layers before
the fully-connected layer in question.</p>
<p>Let's find the derivative <object class="valign-m7" data="../../images/math/54869ab2743febebc22269d12572c77e057c816e.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{x}}</object>. The chain
rule here is:</p>
<object class="align-center" data="../../images/math/a52a21efbf7f0b992db127d495abddd618677709.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial{L}}{\partial{x}}=D(L \circ y)(x)=DL(y(x)) \cdot Dy(x)\]</object>
<p>Dimensions: <object class="valign-m4" data="../../images/math/2e9597ce1ffd09d94be733216c3f1c1b2ab5f33c.svg" style="height: 18px;" type="image/svg+xml">DL(y(x))</object> is [1, T] as before; <object class="valign-m4" data="../../images/math/a2bef37f23427154c47e53945043549039e36bcf.svg" style="height: 18px;" type="image/svg+xml">Dy(x)</object> has T outputs
(elements of <em>y</em>) and N inputs (elements of <em>x</em>), so its dimensions are [T, N].
Therefore, the dimensions of <object class="valign-m7" data="../../images/math/54869ab2743febebc22269d12572c77e057c816e.svg" style="height: 24px;" type="image/svg+xml">\frac{\partial{L}}{\partial{x}}</object> are [1, N].</p>
<p>From:</p>
<object class="align-center" data="../../images/math/7190e002ac69968b674aecacfd5a8531ad9cd208.svg" style="height: 55px;" type="image/svg+xml">\[y_1=\sum_{j=1}^{N}W_{1,j}x_{j}+b_1\]</object>
<p>We know that <object class="valign-m10" data="../../images/math/c209b0f19299fee08359f73898212bb0d0df8c30.svg" style="height: 28px;" type="image/svg+xml">\frac{\partial y_1}{\partial x_j}=W_{1,j}</object>. Generalizing
this, we get <object class="valign-m10" data="../../images/math/6abadcb3365f09141a9cac088fdbb17418e75171.svg" style="height: 28px;" type="image/svg+xml">\frac{\partial y_i}{\partial x_j}=W_{i,j}</object>; in other words,
<object class="valign-m4" data="../../images/math/a2bef37f23427154c47e53945043549039e36bcf.svg" style="height: 18px;" type="image/svg+xml">Dy(x)</object> is just the weight matrix <em>W</em>. So
<object class="valign-m8" data="../../images/math/e6c2d66d989f1abdb9e8b492e45f00be1ab2a21b.svg" style="height: 25px;" type="image/svg+xml">\frac{\partial{L}}{\partial{x_i}}</object> is the dot product of <object class="valign-m4" data="../../images/math/2e9597ce1ffd09d94be733216c3f1c1b2ab5f33c.svg" style="height: 18px;" type="image/svg+xml">DL(y(x))</object>
with the <em>i</em>-th column of <em>W</em>.</p>
<p>Computationally, we can express this as follows:</p>
<div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dy</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
<p>Again, recall that our vectors are <em>column</em> vectors. Therefore, to multiply <em>dy</em>
from the left by <em>W</em> we have to transpose it to a row vector first. The result
of this matrix multiplication is a [1, N] row-vector, so we transpose it again
to get a column.</p>
<p>An alternative method to compute this would transpose <em>W</em> rather than <em>dy</em> and
then swap the order:</p>
<div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
</pre></div>
<p>These two methods produce exactly the same <em>dx</em>; it's important to be familiar
with these tricks, because otherwise it may be confusing to see a transposed <em>W</em>
when we expect the actual <em>W</em> from gradient computations.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td><p class="first">As explained in the
<a class="reference external" href="../../2016/the-softmax-function-and-its-derivative.html">softmax post</a>,
we <em>linearize</em> the 2D matrix <em>W</em> into a single vector with <em>NT</em> elements
using some approach like row-major, where the <em>N</em> elements of the first
row go first, then the <em>N</em> elements of the second row, and so on until we
have <em>NT</em> elements for all the rows.</p>
<p class="last">This is a fully general approach as we can linearize any-dimensional
arrays. To work with Jacobians, we're interested in <em>K</em> inputs, no matter
where they came from - they could be a linearization of a 4D array. As
long as we remember which element out of the <em>K</em> corresponds to which
original element, we'll be fine.</p>
</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td>In some cases you may hear about <em>averaging</em> the gradients across the
batch. Averaging just means dividing the sum by <em>B</em>; it's a constant
factor that can be consolidated into the learning rate.</td></tr>
</tbody>
</table>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2018/backpropagation-through-a-fully-connected-layer/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:46:11 GMT -->
</html>