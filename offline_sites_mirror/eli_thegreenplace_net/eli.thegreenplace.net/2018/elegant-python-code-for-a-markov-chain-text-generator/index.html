<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:58:23 GMT -->
<head>
    <title>Elegant Python code for a Markov chain text generator - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../theme/css/style.css" type="text/css"/>

        <link href="../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../index.html" class="navbar-brand">
                <img src="../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="index.html"
                       rel="bookmark"
                       title="Permalink to Elegant Python code for a Markov chain text generator">
                        Elegant Python code for a Markov chain text generator
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> July 05, 2018 at 05:40</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../tag/python.html">Python</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>While preparing <a class="reference external" href="../understanding-how-to-implement-a-character-based-rnn-language-model/index.html">the post on minimal char-based RNNs</a>,
I coded a simple Markov chain text generator to serve as a comparison for the
quality of the RNN model. That code turned out to be concise and quite elegant
(IMHO!), so it seemed like I should write a few words about it.</p>
<p>It's so short I'm just going to paste it here in its entirety, but
<a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2018/markov-simple">this link</a>
should have it in a Python file with some extra debugging information for
tinkering, along with a sample input file.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">Counter</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="c1"># This is the length of the &quot;state&quot; the current character is predicted from.</span>
<span class="c1"># For Markov chains with memory, this is the &quot;order&quot; of the chain. For n-grams,</span>
<span class="c1"># n is STATE_LEN+1 since it includes the predicted character as well.</span>
<span class="n">STATE_LEN</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">Counter</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Learning model...&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">STATE_LEN</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">STATE_LEN</span><span class="p">]</span>
    <span class="nb">next</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">STATE_LEN</span><span class="p">]</span>
    <span class="n">model</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="nb">next</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sampling...&#39;</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">400</span><span class="p">):</span>
    <span class="n">out</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="n">state</span><span class="p">]),</span> <span class="n">model</span><span class="p">[</span><span class="n">state</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
<p>Without going into too much details, a <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_chain">Markov Chain</a> is a model describing the
probabilities of events based on the current state only (without having to
recall all past states). It's very easy to implement and &quot;train&quot;.</p>
<p>In the code shown above, the most important part to grok is the data structure
<tt class="docutils literal">model</tt>. It's a dictionary mapping a string <em>state</em> to the probabilities of
characters following this state. The size of that string is configurable, but
let's just assume it's 4 for the rest of the discussion. This is the <em>order</em> of
the Markov chain. For every string seen in the input, we look at the character
following it and increment a counter for that character; the end result is a
dictionary mapping the alphabet to integers. For example, we may find that for
the state &quot;foob&quot;, 'a' appeared 75 times right after it, 'b' appeared 25 times,
'e' 44 times and so on.</p>
<p>The learning process is simply sliding a &quot;window&quot; of 4 characters over the
input, recording these appearances:</p>
<img alt="Markov chain sliding window diagram" class="align-center" src="../../images/2018/markov-chain-window.png" />
<p>The learning loop is extremely concise; this is made possible by the right
choice of Python data structures. First, we use a <tt class="docutils literal">defaultdict</tt> for the model
itself; this lets us avoid existence checks or <tt class="docutils literal">try</tt> for states that don't
appear in the model at all.</p>
<p>Second, the objects contained inside <tt class="docutils literal">model</tt> are of type <tt class="docutils literal">Counter</tt>, which is
a subclass of <tt class="docutils literal">dict</tt> with some special sauce. In its most basic usage, a
counter is meant to store an integer count for its keys - exactly what we need
here. So a lot of power is packed into this simple statement:</p>
<div class="highlight"><pre><span></span><span class="n">model</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="nb">next</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
<p>If you try to rewrite it with <tt class="docutils literal">model</tt> being a dict of dicts, it will become
much more complicated to keep track of the corner cases.</p>
<p>With the learning loop completed, we have in <tt class="docutils literal">model</tt> every 4-letter string
encountered in the text, mapped to its <tt class="docutils literal">Counter</tt> of occurrences for the
character immediately following it. We're ready to generate text, or &quot;sample
from the model&quot;.</p>
<p>We start by picking a random state that was seen in the training text. Then, we
loop for an arbitrary bound and at every step we randomly select the following
character, and update the current state. The following character is selected
using <a class="reference external" href="../../2010/01/22/weighted-random-generation-in-python.html">weighted random selection</a>
- precisely the right idiom here, as we already have in each counter the
&quot;weights&quot; - the more often some char was observed after a given state, the
higher the chance to select it for sampling will be.</p>
<p>Starting with Python 3.6, the standard library has <tt class="docutils literal">random.choices</tt> to
implement weighted random selection. Before Python 3.6 we'd have to write that
function on our own (<tt class="docutils literal">Counter</tt> has the <tt class="docutils literal">most_common()</tt> method that would
make it easier to write an efficient version).</p>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2018/elegant-python-code-for-a-markov-chain-text-generator/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:58:23 GMT -->
</html>