<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2018/minimal-character-based-lstm-implementation/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:58:28 GMT -->
<head>
    <title>Minimal character-based LSTM implementation - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../theme/css/style.css" type="text/css"/>

        <link href="../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../index.html" class="navbar-brand">
                <img src="../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="index.html"
                       rel="bookmark"
                       title="Permalink to Minimal character-based LSTM implementation">
                        Minimal character-based LSTM implementation
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> June 07, 2018 at 05:34</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../tag/math.html">Math</a>
        ,
    <a href="../../tag/machine-learning.html">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>Following up on <a class="reference external" href="../understanding-how-to-implement-a-character-based-rnn-language-model/index.html">the earlier post</a>
deciphering a minimal vanilla RNN implementation, here I'd like to extend the
example to a simple LSTM model.</p>
<p>Once again, the idea is to combine a well-commented code sample
(<a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/min-char-rnn/min-char-lstm.py">available here</a>)
with some high-level diagrams and math to enable someone to
fully understand the code. The LSTM architecture presented herein is the
standard one originating from Hochreiter's and Schmidthuber's <a class="reference external" href="https://www.google.com/search?q=lstm+hochreiter">1997 paper</a>. It's described pretty much
everywhere; <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Chris Olah's post</a> has particularly
nice diagrams and is worth reading.</p>
<div class="section" id="lstm-cell-structure">
<h2>LSTM cell structure</h2>
<p>From 30,000 feet, LSTMs look just like regular RNNs; there's a &quot;cell&quot; that has a
recurrent connection (output tied to input), and when trained this cell is
usually unrolled to some fixed length.</p>
<p>So we can take the basic RNN structure from the <a class="reference external" href="../understanding-how-to-implement-a-character-based-rnn-language-model.html">previous post</a>:</p>
<img alt="Basic RNN diagram" class="align-center" src="../../images/2018/rnnbasic.png" />
<p>LSTMs are a bit trickier because there are two recurrent connections; these
can be &quot;packed&quot; into a single vector <em>h</em>, so the above diagram still applies.
Here's how an LSTM cell looks inside:</p>
<img alt="LSTM cell" class="align-center" src="../../images/2018/lstm-cell.png" />
<p><em>x</em> is the input; <em>p</em> is the probabilities computed from the output <em>y</em> (these
symbols are named consistently with my earlier RNN post) and exit the cell at
the bottom purely due to topological convenience. The two memory vectors are <em>h</em>
and <em>c</em> - as mentioned earlier, they could be combined into a single vector, but
are shown here separately for clarity.</p>
<p>The main idea of LSTMs is to enable training of longer sequences by providing
a &quot;fast-path&quot; to back-propagate information farther down in memory. Hence the
<em>c</em> vector is not multiplied by any matrices on its path. The circle-in-circle
block means element-wise multiplication of two vectors; plus-in-square is
element-wise addition. The funny greek letter is the Sigmoid non-linearity:</p>
<object class="align-center" data="../../images/math/8b0db8368e8a617143fa6566f42c1e47cd833c9c.svg" style="height: 38px;" type="image/svg+xml">\[\sigma(x) =\frac{1}{1+e^{-x}}\]</object>
<p>The only other block we haven't seen in the vanilla RNN diagram is the
colon-in-square in the bottom-left corner; this is simply the concatenation of
<em>h</em> and <em>x</em> into a single column vector. In addition, I've combined the
&quot;multiply by matrix <em>W</em>, then add bias <em>b</em>&quot; operation into a single rectantular
box to save on precious diagram space.</p>
<p>Here are the equations computed by a cell:</p>
<object class="align-center" data="../../images/math/c2cc966ba7ce8075317b87885bc9c432aafe2dba.svg" style="height: 249px;" type="image/svg+xml">\[\begin{align*}
xh&amp;=x^{[t]}:h^{[t-1]}\\
f&amp;=\sigma(W_f\cdot xh+b_f)\\
i&amp;=\sigma(W_i\cdot xh+b_i)\\
o&amp;=\sigma(W_o\cdot xh+b_o)\\
cc&amp;=tanh(W_{cc}\cdot xh+b_{cc})\\
c^{[t]}&amp;=c^{[t-1]}\odot f +cc\odot i\\
h^{[t]}&amp;=tanh(c^{[t]})\odot o\\
y^{[t]}&amp;=W_{y}\cdot h^{[t]}+b_y\\
p^{[t]}&amp;=softmax(y^{[t]})\\
\end{align*}\]</object>
</div>
<div class="section" id="backpropagating-through-an-lstm-cell">
<h2>Backpropagating through an LSTM cell</h2>
<p>This works <em>exactly</em> like backprop through a vanilla RNN; we have to carefully
compute how the gradient flows through every node and make sure we properly
combine gradients at fork points. Most of the elements in the LSTM diagram are
familiar from the <a class="reference external" href="../understanding-how-to-implement-a-character-based-rnn-language-model.html">previous post</a>.
Let's briefly work through the new ones.</p>
<p>First, the Sigmoid function; it's an elementwise function, and computing its
derivative is very similar to the <em>tanh</em> function discussed in the previous
post. As usual, given <object class="valign-m4" data="../../images/math/e9ef6bd037537d5fe08743736acadccc09e70b06.svg" style="height: 18px;" type="image/svg+xml">f=\sigma(k)</object>, from the chain rule we have the
following derivative w.r.t. some weight <em>w</em>:</p>
<object class="align-center" data="../../images/math/57e3f2cab3c9b46a03d763a2f73b83963a1cd500.svg" style="height: 39px;" type="image/svg+xml">\[\frac{\partial f}{\partial w}=\frac{\partial \sigma(k)}{\partial k}\frac{\partial k}{\partial w}\]</object>
<p>To compute the derivative <object class="valign-m6" data="../../images/math/8aa59f2f536b727cf97239b345ddcc98e41c2c91.svg" style="height: 26px;" type="image/svg+xml">\frac{\partial \sigma(k)}{\partiak k}</object>, we'll
use the ratio-derivative formula:</p>
<object class="align-center" data="../../images/math/9e006cf5e9f1f8ccac82ba1f2bcdabd710731756.svg" style="height: 42px;" type="image/svg+xml">\[(\frac{f}{g})&#x27;=\frac{f&#x27;g-g&#x27;f}{g^2}\]</object>
<p>So:</p>
<object class="align-center" data="../../images/math/e3f7af782f52215e8326b389271709a440993984.svg" style="height: 44px;" type="image/svg+xml">\[\sigma &#x27;(k)=\frac{e^{-k}}{(1+e^{-k})^2}\]</object>
<p>A clever way to express this is:</p>
<object class="align-center" data="../../images/math/eb1953be928287ff01ae23dfb4ff1cb2290854c9.svg" style="height: 20px;" type="image/svg+xml">\[\sigma &#x27;(k)=\sigma(k)(1-\sigma(k))\]</object>
<p>Going back to the chain rule with <object class="valign-m4" data="../../images/math/e9ef6bd037537d5fe08743736acadccc09e70b06.svg" style="height: 18px;" type="image/svg+xml">f=\sigma(k)</object>, we get:</p>
<object class="align-center" data="../../images/math/885829ecab969c96daed7f0df6e5864339ad9d8b.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial f}{\partial w}=f(1-f)\frac{\partial k}{\partial w}\]</object>
<p>The other new operation we'll have to find the derivative of is element-wise
multiplication. Let's say we have the column vectors <em>x</em>, <em>y</em> and <em>z</em>, each with
<em>m</em> rows, and we have <object class="valign-m4" data="../../images/math/660b1e0dacc15aa3737b8170c3ecfdcbc6e77db4.svg" style="height: 18px;" type="image/svg+xml">z(x)=x\odot y</object>. Since <em>z</em> as a function of <em>x</em>
has <em>m</em> inputs and <em>m</em> outputs, its Jacobian has dimensions [m,m].</p>
<p><object class="valign-m6" data="../../images/math/0ab96cb4e5d8c6ba3ac8038fda07d518bbe1f388.svg" style="height: 18px;" type="image/svg+xml">D_{j}z_{i}</object> is the derivative of the i-th element of <em>z</em> w.r.t. the j-th
element of <em>x</em>. For <object class="valign-m4" data="../../images/math/660b1e0dacc15aa3737b8170c3ecfdcbc6e77db4.svg" style="height: 18px;" type="image/svg+xml">z(x)=x\odot y</object> this is non-zero only
when <em>i</em> and <em>j</em> are equal, and in that case the derivative is <img alt="y_i" class="valign-m4" src="../../images/math/35c2ac2f82d0ff8f9011b596ed7e54bfcc55f471.png" style="height: 12px;" />.</p>
<p>Therefore, <object class="valign-m4" data="../../images/math/e6631f3b13f877a8bb7b3b6a0c0d2ca110ecce23.svg" style="height: 18px;" type="image/svg+xml">Dz(x)</object> is a square matrix with the elements of <em>y</em> on the
diagonal and zeros elsewhere:</p>
<object class="align-center" data="../../images/math/2450b2e2a827054f5d292822ff292eaa63c77d1b.svg" style="height: 97px;" type="image/svg+xml">\[Dz=\begin{bmatrix}
y_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; y_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; y_m \\
\end{bmatrix}\]</object>
<p>If we want to backprop some loss <em>L</em> through this function, we get:</p>
<object class="align-center" data="../../images/math/48b17da284ae52bc4b9fdeb7b98b73f398bd4458.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial L}{\partial x}=\frac{\partial L}{\partial z}Dz\]</object>
<p>As <em>x</em> has <em>m</em> elements, the right-hand side of this equation multiplies a [1,m]
vector by a [m,m] matrix which is diagonal, resulting in element-wise multiplication
with the matrix's diagonal elements. In other words:</p>
<object class="align-center" data="../../images/math/e2a6c0742fb006e35e3001d3b3d33f78316fb1e8.svg" style="height: 38px;" type="image/svg+xml">\[\frac{\partial L}{\partial x}=\frac{\partial L}{\partial z}\odot y\]</object>
<p>In code, it looks like this:</p>
<div class="highlight"><pre><span></span><span class="c1"># Assuming dz is the gradient of loss w.r.t. z; dz, y and dx are all</span>
<span class="c1"># column vectors.</span>
<span class="n">dx</span> <span class="o">=</span> <span class="n">dz</span> <span class="o">*</span> <span class="n">y</span>
</pre></div>
</div>
<div class="section" id="model-quality">
<h2>Model quality</h2>
<p>In the <a class="reference external" href="../understanding-how-to-implement-a-character-based-rnn-language-model/index.html">post about min-char-rnn</a>,
we've seen that the vanilla RNN generates fairly low quality text:</p>
<blockquote>
one, my dred, roriny. qued bamp gond hilves non froange saws, to mold
his a work, you shirs larcs anverver strepule thunboler
muste, thum and cormed sightourd
so was rewa her besee pilman</blockquote>
<p>The LSTM's generated text quality is somewhat better when trained with roughtly
the same hyper-parameters:</p>
<blockquote>
the she, over is was besiving the fact to seramed for i said over he
will round, such when a where, &quot;i went of where stood it at eye heardul rrawed
only coside the showed had off with the refaurtoned</blockquote>
<p>I'm fairly sure that it can be made to perform even better with larger memory
vectors and more training data. That said, an even more advanced architecture
can be helpful here. Moreover, since this is a <em>character</em>-based model, to
really capture effects between words a few words apart we'll need a much deeper
LSTM (I'm unrolling to 16 characters we can only capture 2-3 words), and hence
much more training data and time.</p>
<p>Once again, the goal here is not to develop a state-of-the-art language model,
but to show a simple, comprehensible example of how and LSTM is implemented
end-to-end in Python code. <a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/min-char-rnn/min-char-lstm.py">The full code is here</a>
- please let me know if you find any issues with it or something still remains
unclear.</p>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2018/minimal-character-based-lstm-implementation/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:58:28 GMT -->
</html>