<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2016/logistic-regression/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:51:39 GMT -->
<head>
    <title>Logistic regression - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../theme/css/style.css" type="text/css"/>

        <link href="../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../index.html" class="navbar-brand">
                <img src="../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="index.html"
                       rel="bookmark"
                       title="Permalink to Logistic regression">
                        Logistic regression
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> November 02, 2016 at 05:45</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../tag/math.html">Math</a>
        ,
    <a href="../../tag/machine-learning.html">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>This article covers logistic regression - arguably the simplest classification
model in machine learning; it starts with basic binary classification, and ends
up with some techniques for multinomial classification (selecting between
multiple possibilities). The final examples using the softmax function can also
be viewed as an example of a single-layer fully connected neural network.</p>
<p>This article is the theoretical part; in addition, there's quite a bit of
accompanying code <a class="reference external" href="https://github.com/eliben/deep-learning-samples/tree/main/logistic-regression">here</a>.
All the models discussed in the article are implemented from scratch in Python
using only Numpy.</p>
<div class="section" id="linear-model-for-binary-classification">
<h2>Linear model for binary classification</h2>
<p>Using a linear model for binary classification is very similar to <a class="reference external" href="../linear-regression/index.html">linear
regression</a>, except that
we expect a binary (yes/no) answer rather than a numeric answer.</p>
<p>We want to come up with a parameter vector <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" />, such that for every
data vector <strong>x</strong> we can compute <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a>:</p>
<img alt="\[\hat{y}(x) = \theta_0 x_0 + \theta_1 x_1 + \cdots + \theta_n x_n\]" class="align-center" src="../../images/math/ae682f9fda97c28c8e100c87aecad635c7c1d96c.png" style="height: 18px;" />
<p>And then make a binary decision based on the value of <img alt="\hat{y}(x)" class="valign-m4" src="../../images/math/11533fb1b0218620907f5859e6e22aeb65c12cd8.png" style="height: 18px;" />. A
simple way to make a decision is to say &quot;yes&quot; if <img alt="\hat{y}(x)\geq 0" class="valign-m4" src="../../images/math/c30aad52f5af131a89f1a8805e25aa8e354795dc.png" style="height: 18px;" /> and
&quot;no&quot; otherwise. Note that this is arbitrary, as we could flip the condition for
&quot;yes&quot; and for &quot;no&quot;. We could also compare <img alt="\hat{y}(x)" class="valign-m4" src="../../images/math/11533fb1b0218620907f5859e6e22aeb65c12cd8.png" style="height: 18px;" /> to some value other
than zero, and the model would learn equally well <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a>.</p>
<p>Let's make this more concrete, also assigning numeric values to &quot;yes&quot; and &quot;no&quot;,
which will make some computations simpler later on. For &quot;yes&quot; we'll (again,
arbitrarily) select +1, and for &quot;no&quot; we'll go with -1. So, a linear model for
binary classification is parameterized by some <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" />, such that:</p>
<img alt="\[\hat{y}(x) = \theta_0 x_0 + \theta_1 x_1 + \cdots + \theta_n x_n\]" class="align-center" src="../../images/math/ae682f9fda97c28c8e100c87aecad635c7c1d96c.png" style="height: 18px;" />
<p>And:</p>
<img alt="\[class(x)=\left\{\begin{matrix} +1 &amp;amp; \operatorname{if}\ \hat{y}(x)\geq 0\\ -1 &amp;amp; \operatorname{if}\ \hat{y}(x)&amp;lt; 0 \end{matrix}\]" class="align-center" src="../../images/math/092debeba72a26bd76603bd3ce140fc798e5f692.png" style="height: 43px;" />
<p>It helps seeing a graphical example of how this looks in practice. As usual,
we'll have to stick to low dimensionality if we want to visualize things, so
let's use 2D data points.</p>
<p>Since our data is in 2D, we need a 3D <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> (<img alt="\theta_0" class="valign-m3" src="../../images/math/ba6201ddbe2fd0bb66e0704ad8b3c6bdb36f37aa.png" style="height: 15px;" /> for the
bias). Let's pick <img alt="\theta=(4,-0.5, -1)" class="valign-m4" src="../../images/math/6cb259a86870d3bd0a5ad2f839d0515bfc70f0d7.png" style="height: 18px;" />. Plotting <img alt="\hat{y}(x)=\theta \cdot x" class="valign-m4" src="../../images/math/e0a45fd444b0526e19a0f22fb3c264b026fb3bcf.png" style="height: 18px;" /> will give us a plane in 3D, but what we're really interested in is just
to know whether <img alt="\hat{y}(x) \geq 0" class="valign-m4" src="../../images/math/39ad82f3252b80454caa343952948440827f2961.png" style="height: 18px;" />. So we can draw this plane's
intersection with the x/y axis:</p>
<img alt="Line for binary classification" class="align-center" src="../../images/2016/binary-classification-line.png" />
<p>We can play with some sample points to see that everything &quot;to the right&quot; of
the line gives us <img alt="\hat{y}(x) &amp;gt; 0" class="valign-m4" src="../../images/math/d686dc49d4c08e21f67c22cbb42aab2a1f3d3875.png" style="height: 18px;" />, and everything &quot;to the left&quot; of
it gives us <img alt="\hat{y}(x) &amp;lt; 0" class="valign-m4" src="../../images/math/d8a7e77c45cecd8e4ba7c8f7d1f02944e9b55ecf.png" style="height: 18px;" /> <a class="footnote-reference" href="#footnote-3" id="footnote-reference-3">[3]</a>.</p>
</div>
<div class="section" id="loss-functions-for-binary-classification">
<h2>Loss functions for binary classification</h2>
<p>How do we find the right <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> for a classification problem? Similarly
to linear regression, we're going to define a &quot;loss function&quot; and then train a
classifier by minimizing this loss with gradient descent. However, here picking
a good loss function is not as simple - it turns out square loss doesn't work
very well, as we'll see soon.</p>
<p>Let's start by considering the most logical loss function to use for
classification - the number of misclassified data samples. This is called the
0/1 loss, and it's the true measure of how well a classifier works. Say we have
1000 samples, our classifier placed 960 of them in the right category, and
got the wrong answer for the other 40 samples. So the loss would be 40. A better
classifier may get it wrong only 35 times, so its loss would be smaller.</p>
<p>It will be helpful to plot loss functions, so let's add another definition we're
going to be using a lot here: the <em>margin</em>. For a given sample <strong>x</strong>, and its
correct classification <em>y</em>, the margin of classification is
<img alt="m=\hat{y}(x)y" class="valign-m4" src="../../images/math/fc8c312b137c8aafaaebd881836e4332cc14e61f.png" style="height: 18px;" />. Recall that <em>y</em> is either +1 or -1, so the margin
is either <img alt="\hat{y}(x)" class="valign-m4" src="../../images/math/11533fb1b0218620907f5859e6e22aeb65c12cd8.png" style="height: 18px;" /> or its negation, depending on the correct answer.
Note that the margin is positive when our guess is correct (both
<img alt="\hat{y}(x)" class="valign-m4" src="../../images/math/11533fb1b0218620907f5859e6e22aeb65c12cd8.png" style="height: 18px;" /> and y have the same sign) and negative when our guess is
wrong. With this in hand, we define 0/1 loss as:</p>
<img alt="\[L_{01}(m) = \mathbb{I}(m \leq 0)\]" class="align-center" src="../../images/math/e9731883ade0db9b166741b2ff53a8167a8e3ffd.png" style="height: 18px;" />
<p>Where <img alt="\mathbb{I}" class="valign-0" src="../../images/math/3dcdffb11a6b55b62a0c9e29d85dd9120f5945f4.png" style="height: 12px;" /> is an <em>indicator function</em> taking the value 1 when its
condition is true and the value 0 otherwise. Here is the plot of <img alt="L_{01}" class="valign-m4" src="../../images/math/3ed6799c7063de4663bdeab8fa126196f41bcd0f.png" style="height: 16px;" />
as a function of margin:</p>
<img alt="0/1 loss for binary classification" class="align-center" src="../../images/2016/binary-01-loss.png" />
<p>Unfortunately, the 0/1 loss is fairly hostile to gradient descent optimization,
since it's not convex. This is easy to see intuitively. Suppose we have some
<img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> that gives us a margin of -1.5. The 0/1 loss for this margin is
1, but how can we improve it? Small nudges to <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> will still give us
a margin very close to -1.5, which results in exactly the same loss. We don't
know which way to nudge <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> since either way we get the same outcome.
In other words, there's no slope to follow here.</p>
<p>That's not to say all is lost. Some work is being done with optimizing 0/1
losses for classification, but this is a bit outside the mainstream of machine
learning. Here's an <a class="reference external" href="http://jmlr.org/proceedings/papers/v28/nguyen13a.pdf">interesting paper</a> that discusses some
approaches. It's fascinating for computer science geeks since it uses
combinatorial search techniques. The rest of this post, however, will use 0/1
loss only as an idealized limit, trying other kinds of loss we can actually
run gradient descent with.</p>
<p>The first such loss that comes to mind is square loss, the same one we use in
linear regression. We'll define the square loss as a function of margin:</p>
<img alt="\[L_2(m) = (m - 1)^2\]" class="align-center" src="../../images/math/ea06356db44999485977e3a7e6ff5e97e617b1bb.png" style="height: 21px;" />
<p>The reason we do this is to get two desired outcomes at important points: at
<img alt="m=1" class="valign-m1" src="../../images/math/002d212eace214d48ccf82c7bc33021b1d9cdb91.png" style="height: 13px;" /> we want the loss to be 0, since this is actually the correct
classification: we only get <img alt="m=1" class="valign-m1" src="../../images/math/002d212eace214d48ccf82c7bc33021b1d9cdb91.png" style="height: 13px;" /> when either both <object class="valign-m4" data="../../images/math/45f0241f56d9823eb2d24a228d7ffe62c5fdcdc2.svg" style="height: 16px;" type="image/svg+xml">y=1</object> and
<object class="valign-m4" data="../../images/math/c5f34fb4e66b84bde15d596cf76efd468983c4d5.svg" style="height: 17px;" type="image/svg+xml">\hat{y}=1</object> or when both <object class="valign-m4" data="../../images/math/ad8ddd3de86ba8af8476af79d20b151a251ec117.svg" style="height: 16px;" type="image/svg+xml">y=-1</object> and <object class="valign-m4" data="../../images/math/4ae2d248963bac1702c9e5e1f1d0769126f0c479.svg" style="height: 17px;" type="image/svg+xml">\hat{y}=-1</object>.</p>
<p>Furthermore, to approximate the 0/1 loss, we want our loss at <img alt="m=0" class="valign-0" src="../../images/math/5e49227d625a223efeaa8d7bc48bb0b87f878bff.png" style="height: 12px;" /> to be
1. Here's a plot of the square loss together with 0/1 loss:</p>
<img alt="0/1 loss and square loss for binary classification" class="align-center" src="../../images/2016/binary-01-with-square-loss.png" />
<p>A couple of problems are immediately apparent with the square loss:</p>
<ol class="arabic simple">
<li>It penalizes correct classification as well, in case the margin is very
positive. This is not something we want! Ideally, we want the loss to be
0 starting with <img alt="m=1" class="valign-m1" src="../../images/math/002d212eace214d48ccf82c7bc33021b1d9cdb91.png" style="height: 13px;" /> and for all subsequent values of <em>m</em>.</li>
<li>It very strongly penalizes outliers. One sample that we misclassified badly
can shift the training too much.</li>
</ol>
<p>We could try to fix these problems by using clamping of some sort, but there is
another loss function which serves as a much better approximation to 0/1 loss.
It's called &quot;hinge loss&quot;:</p>
<img alt="\[L_h(m) = max(0, 1-m)\]" class="align-center" src="../../images/math/dd883f12c7f609fe9256e0e6bb4cfdf319d07844.png" style="height: 18px;" />
<p>And its plot, along with the previously shown losses:</p>
<img alt="0/1 loss, square loss and hinge loss for binary classification" class="align-center" src="../../images/2016/binary-01-with-square-and-hinge-loss.png" />
<p>Note that the hinge loss also matches 0/1 loss on the two important points:
<img alt="m=0" class="valign-0" src="../../images/math/5e49227d625a223efeaa8d7bc48bb0b87f878bff.png" style="height: 12px;" /> and <img alt="m=1" class="valign-m1" src="../../images/math/002d212eace214d48ccf82c7bc33021b1d9cdb91.png" style="height: 13px;" />. It also has some nice properties:</p>
<ol class="arabic simple">
<li>It doesn't penalize correct classification after <img alt="m=1" class="valign-m1" src="../../images/math/002d212eace214d48ccf82c7bc33021b1d9cdb91.png" style="height: 13px;" />.</li>
<li>It penalizes incorrect classifications, but not as much as square loss.</li>
<li>It's convex (at least where it matters - where the loss is nonzero)! If we
get <object class="valign-m1" data="../../images/math/5abbd129a48c53a04b0caa6eef4d760329f02149.svg" style="height: 14px;" type="image/svg+xml">m=-1.5</object> we can actually examine the loss in its very close
vicinity and find a slope we can use to improve the loss. So, unlike 0/1
loss, it's amenable to gradient descent optimization.</li>
</ol>
<p>There are other loss functions used to train binary classifiers, such as log
loss, but I will leave them out of this post.</p>
<p>This is a good place to mention that hinge loss leads naturally to <a class="reference external" href="https://en.wikipedia.org/wiki/Support_vector_machine#SVM_and_the_hinge_loss">SVMs</a>
(support vector machines), an interesting technique I'll leave for some other
time.</p>
</div>
<div class="section" id="finding-a-classifier-with-gradient-descent">
<h2>Finding a classifier with gradient descent</h2>
<p>With a loss function in hand, we can use <a class="reference external" href="../understanding-gradient-descent/index.html">gradient descent</a> to find a
good classifier for some data. The procedure is very similar to what we've been
doing for linear regression:</p>
<p>Given a loss function, we compute the loss gradient with respect to each
<img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> and update <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> for the next step:</p>
<img alt="\[\theta_{j}=\theta_{j}-\eta\frac{\partial L}{\partial \theta_{j}}\]" class="align-center" src="../../images/math/561a940034503fe1bb00e86c90ac130cb351d73b.png" style="height: 42px;" />
<p>Where <img alt="\eta" class="valign-m4" src="../../images/math/2899aeb886ad0fa72652bffd5511e452aaf084ab.png" style="height: 12px;" /> is the learning rate.</p>
</div>
<div class="section" id="computing-gradients-for-our-loss-functions-with-regularization">
<h2>Computing gradients for our loss functions, with regularization</h2>
<p>The only missing part remaining is computing the gradients for the square and
loss hinge functions we've defined. In addition, I'm going to add &quot;<img alt="L_2" class="valign-m3" src="../../images/math/0d2398f5890edff3f40f1686fc3b51528209bf9b.png" style="height: 15px;" />
regularization&quot; to the loss as a means to prevent overfitting for the training
data. <a class="reference external" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">Regularization</a> is an important
component of the learning algorithm. <img alt="L_2" class="valign-m3" src="../../images/math/0d2398f5890edff3f40f1686fc3b51528209bf9b.png" style="height: 15px;" /> regularization adds the sum of
the squares of all parameters to the loss, and thus &quot;tries&quot; to keep parameters
low. This way, we don't end up over-emphasizing one or a group of parameters
over the others.</p>
<p>Here is square loss with regularization <a class="footnote-reference" href="#footnote-4" id="footnote-reference-4">[4]</a>:</p>
<img alt="\[L_2=\frac{1}{k}\sum_{i=1}^{k}(m^{(i)}-1)^2+\frac{\beta}{2}\sum_{j=0}^{n}\theta_{j}^2\]" class="align-center" src="../../images/math/a9735ff6606b3ad3454c3dfefc541c21b926d541.png" style="height: 56px;" />
<p>This is assuming we have <em>k</em> data points (<em>n+1</em> dimensional) and <em>n+1</em>
parameters (including the special 0th parameter representing the bias). The
total loss is the square loss averaged over all data points, plus the
regularization loss. <img alt="\beta" class="valign-m4" src="../../images/math/6499d503bfc00cadae1440b191c52a8632e2f8c4.png" style="height: 16px;" /> is the regularization &quot;strength&quot; (another
hyper-parameter in the learning algorithm).</p>
<p>Let's start by computing the derivative of the margin. Using
superscripts for indexing data items, recall that:</p>
<img alt="\[m^{(i)}=\hat{y}^{(i)}y^{(i)}=(\theta_0 x_0^{(i)}+\cdots + \theta_n x_n^{(i)})y^{(i)}\]" class="align-center" src="../../images/math/bce48f26ac61cbfd37c8bfbaad0004e5c30ccbbc.png" style="height: 26px;" />
<p>Therefore:</p>
<img alt="\[\frac{\partial m^{(i)}}{\partial \theta_j}=x_j^{(i)}y^{(i)}\]" class="align-center" src="../../images/math/fd79e2321a3ee607dbf3840535d1a8a2327e2117.png" style="height: 47px;" />
<p>With this in hand, it's easy to compute the gradient of <img alt="L_2" class="valign-m3" src="../../images/math/0d2398f5890edff3f40f1686fc3b51528209bf9b.png" style="height: 15px;" /> loss.</p>
<img alt="\[\frac{\partial L_2}{\partial \theta_j}=\frac{2}{k}\sum_{i=1}^{k}(m^{(i)}-1)x_{j}^{(i)}y^{(i)}+\beta\theta_j\]" class="align-center" src="../../images/math/2340ff828a85ab17aa5067b4985cf9da4fd5fae7.png" style="height: 54px;" />
<p>Now let's turn to hinge loss. The total loss for the data set with regularization
is:</p>
<img alt="\[L_h=\frac{1}{k}\sum_{i=1}^{k}max(0, 1-m^{(i)})+\frac{\beta}{2}\sum_{j=0}^{n}\theta_{j}^2\]" class="align-center" src="../../images/math/2ce4a6debf2650ea4c8a1ff24ce8e42f3d370a6e.png" style="height: 56px;" />
<p>The tricky part here is finding the derivative of the <img alt="max" class="valign-0" src="../../images/math/0706025b2bbcec1ed8d64822f4eccd96314938d0.png" style="height: 8px;" /> function
with respect to <img alt="\theta_j" class="valign-m6" src="../../images/math/56adcea6f10a3cd4a439536412c7fb690f803bc9.png" style="height: 18px;" />. I find it easier to reason about functions
like <img alt="max" class="valign-0" src="../../images/math/0706025b2bbcec1ed8d64822f4eccd96314938d0.png" style="height: 8px;" /> when the different cases are cleanly separated:</p>
<img alt="\[max(0,1-m^{(i)})=\left\{\begin{matrix} 1-m^{(i)} &amp;amp; \operatorname{if}\ m^{(i)}&amp;lt; 1\\ 0 &amp;amp; \operatorname{if}\ m^{(i)}\geq 1 \end{matrix}\right.\]" class="align-center" src="../../images/math/884d533e1ff8dd51ae43a229bc2f86bc72e82c2a.png" style="height: 46px;" />
<p>We already know the derivative of <img alt="m^{(i)}" class="valign-0" src="../../images/math/0971cbdfca7ab3d5c094d8a8e75c77ccf66e4715.png" style="height: 17px;" /> with respect to
<img alt="\theta_j" class="valign-m6" src="../../images/math/56adcea6f10a3cd4a439536412c7fb690f803bc9.png" style="height: 18px;" />. So it's easy to derive this expression case-by-case:</p>
<img alt="\[\frac{\partial max(0,1-m^{(i)})}{\partial \theta_j}=\left\{\begin{matrix} -x_j^{(i)}y^{(i)} &amp;amp; \operatorname{if}\ m^{(i)}&amp;lt; 1\\ 0 &amp;amp; \operatorname{if}\ m^{(i)}\geq 1 \end{matrix}\right.\]" class="align-center" src="../../images/math/4feb3f18ab008352c513de8508c4e8f877510167.png" style="height: 54px;" />
<p>And the overall gradient of the hinge loss is:</p>
<img alt="\[\frac{\partial L_h}{\partial \theta_j}=\frac{1}{k}\sum_{i=1}^{k}\frac{\partial max(0,1-m^{(i)})}{\partial \theta_j}+\beta\theta_j\]" class="align-center" src="../../images/math/d3113e543be93630457f9501379fe0b6956d9342.png" style="height: 54px;" />
</div>
<div class="section" id="experiments-with-synthetic-data">
<h2>Experiments with synthetic data</h2>
<p>Let's see an example of learning binary classifier in action. <a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/logistic-regression/simple_binary_classifier.py">This code sample</a>
generates some synthetic data in two dimensions and then uses the approach
described so far in the post to train a binary classifier. Here's a sample data
set:</p>
<img alt="Synthetic data for binary classification" class="align-center" src="../../images/2016/synthetic-data.png" />
<p>The data points for which the correct answer is positive (<em>y=1</em>) are the green
crosses; the ones for which the correct answer is negative (<em>y=-1</em>) are the red
dots. Note that I include a small number of negative outliers (red dots where
we'd expect only green crosses to be) to test the classifier on realistic,
imperfect data.</p>
<p>The sample code can use combinatorial search to find a &quot;best&quot; set of parameters
that results in the lowest 0/1 loss - the lowest number of misclassified data
items. Note that misclassifying some items in this data set is inevitable (with
a linear classifier), because of the outliers. Here is the contour line showing
how the classification decision is made with parameters found by doing the
combinatorial search:</p>
<img alt="Synthetic data for binary classification with only 0/1 loss" class="align-center" src="../../images/2016/synthetic-data-only-01-loss.png" />
<p>The 0/1 loss - number of misclassified data items - for this set of parameters
is 20 out of 400 data items (95% correct prediction rate).</p>
<p>Next, the code trains a classifier using square loss, and another using hinge
loss. I'm not using regularization for this data set, since with only 3
parameters there can't be too much selective bias between them; in other words,
<img alt="\beta=0" class="valign-m4" src="../../images/math/3bb1ac87ba8d8d0c95fd43b91640c0b96f8e72d9.png" style="height: 16px;" />.</p>
<p>A classifier trained with square loss misclassifies 32 items (92% success rate).
A classifier trained with hinge loss misclassifies 26 items (93.5% success rate,
much closer to the &quot;perfect&quot; rate). This is to be expected from the earlier
discussion - square loss very strongly penalizes outliers, which makes it more
skewed on this data <a class="footnote-reference" href="#footnote-5" id="footnote-reference-5">[5]</a>. Here are the contour plots for all losses that
demonstrate this graphically:</p>
<img alt="Synthetic data for binary classification with all losses" class="align-center" src="../../images/2016/synthetic-data-all-losses.png" />
</div>
<div class="section" id="binary-classification-of-mnist-digits">
<h2>Binary classification of MNIST digits</h2>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a> is the
&quot;hello world&quot; of machine learning these days. It's a database of grayscale
images representing handwritten digits, with a correct label for each of these
images.</p>
<p>MNIST is usually employed for the more general multinomial classification
problem - classifying a given data item into one of multiple classes (0 to 9 in
the case of MNIST). We'll address this in a later section.</p>
<p>Here, however, we can experiment with training a binary classifier on MNIST. The
idea is to train a classifier that recognizes some single label. For example, a
classifier answering the question &quot;is this an image of the digit 4&quot;. This is a
binary classification problem, since there are only two answers - &quot;yes&quot; and
&quot;no&quot;.</p>
<p><a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/logistic-regression/mnist_binary_classifier.py">Here's a code sample</a>
that trains such a classifier, using the hinge loss function (since we've already
determined it gives better results than square loss for classification
problems).</p>
<p>It starts by converting the correct labels of MNIST from the numeric range 0-9
to +1 or -1 based on whether the label is 4:</p>
<div class="highlight"><pre><span></span>         0           -1
         1           -1
         4            1
         9           -1
y =      3     ==&gt;   -1
         8           -1
         5           -1
        ...
         4            1
</pre></div>
<p>Then all we have is a binary classification problem, albeit one that is
785-dimensional (784 dimensions for each of the 28x28 pixels in the input
images, plus one for bias). Visualizing the separating contours would be quite
challenging here, but we can now trust the math to know what's going on. Other
than this, the code for gradient descent is <em>exactly the same</em> as for the simple
2D synthetic data shown earlier.</p>
<p>My goal here is not to design a state-of-the-art machine learning architecture,
but to explain how the main parts work. So I didn't tune the model too much, but
it's possible to get 98% accuracy on this binary formulation of MNIST by tuning
the code a bit. While 98% sounds great, recall that we could get 90% just by
saying &quot;no&quot; to every digit :-) Feel free to play with the code to see if you
can get even higher numbers; I don't really expect record-beating numbers from
this model, though, since it's so simple.</p>
</div>
<div class="section" id="logistic-regression-predicting-probabilities">
<h2>Logistic regression - predicting probabilities</h2>
<p>So far the predictors we've been looking at were trained to return a binary
yes/no response; a more useful model would also tell us how sure it is. For
example &quot;what is the chance of rain tomorrow&quot;, rather than &quot;will there be rain,
yes or no&quot;? The probability gives additional information. &quot;90% chance of rain&quot;
vs. &quot;56% chance of rain&quot; gives us additional information over the binary &quot;yes&quot;
for both cases (assuming a 50% cutoff).</p>
<p>Moreover, note that the linear model we've trained actually provides more
information already, giving a numerical answer. We choose to cut it off at 0,
saying yes for positive and no for negative numbers. But some numbers are more
positive (or negative) than others!</p>
<p>Quick thought experiment: can we somehow interpret the response before cutoff as
probability? The main problem here is that probabilities must be in the range
[0, 1], while the linear model gives us an arbitrary real number. We may end up
with negative probabilities or probabilities over 1, neither of which makes much
sense. So we'll want to find some mathematical way to &quot;squish&quot; the result into
the valid [0, 1] range. A common way to do this is to use the logistic function:</p>
<img alt="\[S(z) = \frac{1}{1 + e^{-z}}\]" class="align-center" src="../../images/math/62429be191903e2433ba80f92aaf1044568b831d.png" style="height: 38px;" />
<p>It's also known as the &quot;sigmoid&quot; function because of its S-like shape:</p>
<img alt="Sigmoid function" class="align-center" src="../../images/2016/sigmoid.png" />
<p>We're going to assign <img alt="\hat{y}(x)" class="valign-m4" src="../../images/math/11533fb1b0218620907f5859e6e22aeb65c12cd8.png" style="height: 18px;" /> into the <em>z</em> variable of the sigmoid,
to get the function:</p>
<img alt="\[S(x) = \frac{1}{1 + e^{-(\theta_0 x_0 + \theta_1 x_1 + \cdots + \theta_n x_n)}}\]" class="align-center" src="../../images/math/2b9f6770ff23ed08c38a9ab5c3b5972f5d002ddb.png" style="height: 39px;" />
<p>And now, the answer we get can be interpreted as a probability between 0 and 1
(without actually touching either asymptote) <a class="footnote-reference" href="#footnote-6" id="footnote-reference-6">[6]</a>. We can train a model to get
as close to 1 as possible for training samples where the true answer is &quot;yes&quot;
and as close to 0 as possible for training samples where the true answer is
&quot;no&quot;. This is called &quot;logistic regression&quot; due to the use of the logistic
function.</p>
</div>
<div class="section" id="training-logistic-regression-with-the-cross-entropy-loss">
<h2>Training logistic regression with the cross-entropy loss</h2>
<p>Earlier in this post, we've seen how a number of loss functions fare for the
binary classifier problem. It turns out that for logistic regression, a very
natural loss function exists that's called <a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">cross-entropy</a>
(also sometimes &quot;logistic loss&quot; or &quot;log loss&quot;). This loss function is derived
from probability and information theory, and its derivation is outside the scope
of this post (check out <a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap3.html">Chapter 3 of Michael Nielsen's online book</a> for a nice intuitive
explanation for why this loss function makes sense).</p>
<p>The formulation of cross-entropy we're going to use here starts from the most
general:</p>
<img alt="\[C(x^{(i)})=-\sum_{t} p^{(i)}_t log(p(y^{(i)}=t|\theta))\]" class="align-center" src="../../images/math/a689c6537836933fae93c80a71cd52ff88703a78.png" style="height: 41px;" />
<p>Let's unravel this definition, step by step. The parenthesized superscript
<img alt="x^{(i)}" class="valign-0" src="../../images/math/233014006c0adbee71ec71ba3a70f22ad1b906a1.png" style="height: 17px;" /> denotes, as usual, the <em>ith</em> input sample. <em>t</em> runs over all the
possible outcomes; <img alt="p_t" class="valign-m4" src="../../images/math/aaf082725869f54161f39f7d9c39fff25c52ac94.png" style="height: 12px;" /> is the actual probability of outcome <em>t</em> and
inside the <em>log</em> we have the conditional probability of this outcome given the
regression parameters - in other words, this is the model's prediction <a class="footnote-reference" href="#footnote-7" id="footnote-reference-7">[7]</a>.</p>
<p>To make this more concrete, in our case we have two possible outcomes in the
training data: either <img alt="y^{(i)}=+1" class="valign-m4" src="../../images/math/3e3495884df85359610f062a6a6428fba7891bb8.png" style="height: 21px;" /> or <img alt="y^{(i)}=-1" class="valign-m4" src="../../images/math/8465f16030efd8eab0982f1e60b8ff292317cdbe.png" style="height: 21px;" />. Given any such
outcome, its &quot;actual&quot; probability is either 1 (when we get this outcome in the
training data) or 0 (when we don't). So for any given sample, one of the
two possible values of <em>t</em> has <img alt="p^{(i)}_t=0" class="valign-m5" src="../../images/math/eedcbf364060646a9b6abfccb8e9dda67a645ff0.png" style="height: 25px;" /> and the other has
<img alt="p^{(i)}=1" class="valign-m4" src="../../images/math/e44b2858aeb1c845d09a851cbea5fdc9c465199e.png" style="height: 21px;" />. Therefore, we get <a class="footnote-reference" href="#footnote-8" id="footnote-reference-8">[8]</a>:</p>
<img alt="\[C(x^{(i)})=\left\{ \begin{matrix} -log(S(x^{(i)}) &amp;amp; \operatorname{if}\ y^{(i)}=+1 \\ -log(1-S(x^{(i)})) &amp;amp; \operatorname{if}\ y^{(i)}=-1 \end{matrix}\]" class="align-center" src="../../images/math/97e3fd44d870673c7a74047b82e30c993a9bec59.png" style="height: 46px;" />
<p>The second possibility has <img alt="-log(1-S(x^{(i)}))" class="valign-m4" src="../../images/math/0b34c17378147a8a82db655998c07649ca71ed39.png" style="height: 21px;" /> because we define
<img alt="S(z)" class="valign-m4" src="../../images/math/61bc9efb9d2c99669df519617ee7daee7670e156.png" style="height: 18px;" /> to predict the probability of the answer being +1; therefore, the
probability of the answer being -1 is <img alt="1-S(z)" class="valign-m4" src="../../images/math/d006e787dd01f802c9c5cb570e39a44cb133b2ce.png" style="height: 18px;" />.</p>
<p>This is the cross-entropy loss for a single sample <img alt="x^{(i)}" class="valign-0" src="../../images/math/233014006c0adbee71ec71ba3a70f22ad1b906a1.png" style="height: 17px;" />. To get the
total loss over a data set, we take the average sample loss, as usual:</p>
<img alt="\[C = \frac{1}{k}\sum_{i=1}^{k} C(x^{(i)})\]" class="align-center" src="../../images/math/642351dc03ee1f11eca503f558971282d5c700e7.png" style="height: 54px;" />
<p>Now let's compute the gradient of this loss function, so we can use it to
train a model. Starting with the +1 case, we have:</p>
<img alt="\[C_{+1} = -log(S(x^{(i)}))\]" class="align-center" src="../../images/math/986418ed0bf4c05742c9a412a0918ed00108d93d.png" style="height: 23px;" />
<p>Then:</p>
<img alt="\[\frac{\partial C_{+1}}{\partial \theta_j} = \frac{-1}{S(x^{(i)})}\frac{\partial S(x^{(i)})}{\partial \theta_j}\]" class="align-center" src="../../images/math/c91fbcf0bb4630112d1efa5adbb8756c25512c68.png" style="height: 47px;" />
<p>Here it will be helpful to use the following identity, which can be easily
verified by going through the math <a class="footnote-reference" href="#footnote-9" id="footnote-reference-9">[9]</a>:</p>
<img alt="\[S&amp;#x27;(z)=S(z)(1-S(z))\]" class="align-center" src="../../images/math/3d880e07d60096518b916e877cd6a8496c39bc37.png" style="height: 20px;" />
<p>Since in our case <img alt="S(x^{(i)})" class="valign-m4" src="../../images/math/8a85ab5b49ac41fe751ac8b29e2f2e76f34650bb.png" style="height: 21px;" /> is actually <img alt="S(\hat{y}(x^{(i})))" class="valign-m4" src="../../images/math/915144b3b0a3b41ff5d71f88e798c702386cfea8.png" style="height: 21px;" />
where <img alt="\hat{y}(x) = \theta_0 x_0 + \theta_1 x_1 + \cdots + \theta_n x_n" class="valign-m4" src="../../images/math/7ad144258d3d91e1ada8fd7f94a7d0b0538faa2d.png" style="height: 18px;" />,
we can apply the chain rule:</p>
<img alt="\[\frac{\partial S(x^{(i)})}{\partial \theta_j}=S(x^{(i)})(1-S(x^{(i)}))x^{(i)}_j\]" class="align-center" src="../../images/math/6bb3f809b570699a74428c137ea715d97b08b58d.png" style="height: 47px;" />
<p>Substituting back into <img alt="\frac{\partial C_{+1}}{\partial \theta_j}" class="valign-m10" src="../../images/math/cc23ed0ff22b532e2ab3fec04117c8c968318629.png" style="height: 29px;" />, we get:</p>
<img alt="\[\begin{align*} \frac{\partial C_{+1}}{\partial \theta_j} &amp;amp;= \frac{-1}{S(x^{(i)})}S(x^{(i)})(1-S(x^{(i)}))x^{(i)}_j \\                                        &amp;amp;= (S(x^{(i)})-1)x^{(i)}_j \end{align*}\]" class="align-center" src="../../images/math/dc2a24be9f61f7066fbaeb48805bb59c51e445c0.png" style="height: 76px;" />
<p>Similarly, for <img alt="C_{-1}=-log(1-S(x^{(i)}))" class="valign-m4" src="../../images/math/91460ff9e118d56fbbce2f0557bb9208a4d438a4.png" style="height: 21px;" /> we can compute:</p>
<img alt="\[\frac{\partial C_{-1}}{\partial \theta_j} = S(x^{(i)})x^{(i)}_j\]" class="align-center" src="../../images/math/5e5fd85290c89289aacb1486d9f706bd9fca8fdc.png" style="height: 42px;" />
<p>Putting it all together, we find that the contribution of <img alt="x^{(i)}" class="valign-0" src="../../images/math/233014006c0adbee71ec71ba3a70f22ad1b906a1.png" style="height: 17px;" /> to the
gradient of <img alt="\theta_j" class="valign-m6" src="../../images/math/56adcea6f10a3cd4a439536412c7fb690f803bc9.png" style="height: 18px;" /> is:</p>
<img alt="\[\frac{\partial C(x^{(i)})}{\partial \theta_j}=\left\{ \begin{matrix} (S(x^{(i)})-1)x^{(i)}_j &amp;amp; \operatorname{if}\ y^{(i)}=+1 \\ S(x^{(i)})x^{(i)}_j &amp;amp; \operatorname{if}\ y^{(i)}=-1 \end{matrix}\]" class="align-center" src="../../images/math/21cf7ba3c128242b99272a3e47b5ab5c09cb24bf.png" style="height: 56px;" />
<p>Using these formulae, we can train a binary logistic classifier for MNIST that
gives us a probability of some input image being a 4, rather than a yes/no
answer. The <a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/logistic-regression/mnist_binary_classifier.py">binary MNIST code sample</a>
trains either a binary or a logistic classifier using a lot of shared
infrastructure.</p>
<p>The probability gives us more information than just a yes/no answer. Consider,
for example the following image from the MNIST database. When I trained a binary
classifier with hinge loss to recognize the image 4 for 1200 steps, it wrongly
predicted it is a 4:</p>
<img alt="Image of a 9 from MNIST" class="align-center" src="../../images/2016/mnist-test-9740.png" />
<p>The model clearly made a mistake here, but can we know <em>how</em> wrong it was? It
would be hard to know with a binary classifier that only gives us a yes/no
answer. However, when I run a logistic regression model on the same image, it
tells me it is 53% confident this is a 4. Since our cutoff for yes/no is 50%,
this is quite close to the threshold and thus I'd say the model didn't make
a huge mistake here.</p>
</div>
<div class="section" id="multiclass-logistic-regression">
<h2>Multiclass logistic regression</h2>
<p>The previous example is a great transition into the topic of multiclass logistic
regression. Most real-life problems have more than one possible answer and it
would be nice to train models to select the most suitable answer for any given
input.</p>
<p>Our input is still a vector <strong>x</strong>, but now instead of assigning +1 or -1 as the
answer, we'll be assigning one of a fixed set of classes. If there are T
classes, the answer will be a number in the closed range [0..T-1].</p>
<p>The good news is that we can use the building blocks developed in this post to
put together a multiclass classifier. There are many ways to do this; here I'll
focus on two: one-vs-all classification and softmax.</p>
</div>
<div class="section" id="one-vs-all-classification">
<h2>One-vs-all classification</h2>
<p>The One-vs-all (OvA), also known as one-vs-rest (OvR) approach is a natural
extension of binary classification:</p>
<ol class="arabic simple">
<li>For each class <img alt="t\in[0..T-1]" class="valign-m5" src="../../images/math/6eda0dcb5f9805e0e0e4c3d0af82aacdf1295efd.png" style="height: 18px;" /> we train a logistic classifier where we
set <em>t</em> as the &quot;correct&quot; answer, and the other classes as the &quot;incorrect&quot;
answers (+1 and -1 respectively).</li>
<li>The result of each such classifier is the probability that an input sample
belongs to class <em>t</em>.</li>
<li>Given a new input, we run all <em>T</em> classifiers on it and the one that gives
us the highest probability is chosen as the true class of the input.</li>
</ol>
<p>As a completely synthetic example to make this clearer, suppose that <em>T=3</em>. We
take the training data and train 3 logistic regressions. In the first -
<img alt="C_0" class="valign-m3" src="../../images/math/33e4cbb170d6026eb67de894c0d01e8702fb065d.png" style="height: 15px;" />, we set 0 as the right answer, 1 and 2 as the wrong answers. In the
second - <img alt="C_1" class="valign-m4" src="../../images/math/c538a6221da718dd38230dcbb6e1a8fb40561f7a.png" style="height: 16px;" /> we set 1 as the right answer, 0 and 2 as the wrong answers.
Finally in the third - <img alt="C_2" class="valign-m3" src="../../images/math/e65b6ebf7cbd7ef19069cc4837331af9d119cfe6.png" style="height: 15px;" /> we set 2 as the right answer, 0 and 1 the
wrong answers.</p>
<p>Now, given a new input vector <strong>x</strong> we run <img alt="C_0(x)" class="valign-m4" src="../../images/math/5ed83eb3961cbf4855ce46814719658cdc79e5f2.png" style="height: 18px;" />, <img alt="C_1(x)" class="valign-m4" src="../../images/math/47b77ff17810fb0a0d4f6b86f50d403e8a59a7a7.png" style="height: 18px;" /> and
<img alt="C_2(x)" class="valign-m4" src="../../images/math/c063fddc4bcdd77e1131dc70ec5b578b5ec887ef.png" style="height: 18px;" />. Each of these gives us the probability of <strong>x</strong> belonging to
the respective class. If we put all the classifiers in a vector, we get:</p>
<img alt="\[C(x)=[C_0(x), C_1(x), C_2(x)]\]" class="align-center" src="../../images/math/9e4c42a11867dda976b1f7b1ac6aaa46b6625ee9.png" style="height: 19px;" />
<p>We pick the class where the probability is highest. Mathematically, we can use
the <a class="reference external" href="https://en.wikipedia.org/wiki/Arg_max">argmax function</a> for this purpose.
<em>argmax</em> returns the index of the maximal element in the given vector. For
example, given:</p>
<img alt="\[C(x)=[0.45, 0.42, 0.09]\]" class="align-center" src="../../images/math/983ab6a6770f41c06b3eb32f811678aab7f6fb5b.png" style="height: 19px;" />
<p>We get:</p>
<img alt="\[\underset{t \in [0..2]}{argmax}(C(x))=0\]" class="align-center" src="../../images/math/1fa779c771c2d0abcaca9a759ab2e99608842f82.png" style="height: 34px;" />
<p>Therefore, the chosen class is 0. These class/index numbers are just labels of
course. They can stand for anything depending on the problem domain: medical
condition names, digits and so on.</p>
<p>This approach doesn't require any additional math over what we've already
covered in this post.
<a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/logistic-regression/mnist_multinomial_classifier.py">This multinomial MNIST classifier code sample</a>
implements it. The error rate it achieves is ~11%, similar to what
<a class="reference external" href="http://yann.lecun.com/exdb/publis/index.html#lecun-98">LeCun's 1998 paper</a>
achieved with a simple linear classifier. Much better than 11% can be done for
MNIST, even with a single-layer linear model. However, my model is very far from
the state of art - there's no preprocessing, no artificially-enlarged training
set, no adaptive learning rate; I didn't even spend time tuning the
hyperparameters (regularization type and constants, learning rate, batch size
etc.) The goal here was just to demonstrate the basics of logistic regression,
not to compete for the state of the art in MNIST.</p>
</div>
<div class="section" id="softmax">
<h2>Softmax</h2>
<p>An alternative to OvA is to use the softmax function. I covered softmax <a class="reference external" href="../the-softmax-function-and-its-derivative/index.html">in some
detail</a>
previously; just briefly, softmax is a function
<object class="valign-m4" data="../../images/math/1dd52a52398e38c9549b289449de49ba5fbb98b7.svg" style="height: 19px;" type="image/svg+xml">S(\mathbf{a}):\mathbb{R}^{N}\rightarrow \mathbb{R}^{N}</object> such that:</p>
<object class="align-center" data="../../images/math/5470218612381816a8c9a897d43201757560e646.svg" style="height: 46px;" type="image/svg+xml">\[S_j=\frac{e^{a_j}}{\sum_{k=1}^{N}e^{a_k}}
\qquad \forall j \in 1..N\]</object>
<p>It is very useful for multiclass classification, since it lets us generate
probabilities of the input belonging to one of <em>N</em> classes. Similarly to the
OvA case, here we have to train 10 different parameter vectors, one for each
digit. However, unlike OvA, this training doesn't happen separately but occurs
at the same time. Instead of training a model to find a single parameter vector
each time, we train a parameter <em>matrix</em> once.</p>
<p>The model structure is as follows:</p>
<img alt="Model of softmax logistic regression" class="align-center" src="../../images/2016/softmax-logistic-model.png" />
<p>I've chosen the number of classes to be 10 to reflect MNIST where we have 10
possible digits to assign to every input. In MNIST <em>N</em> is 785 (784 for each of
28x28 pixels in the image, plus one for bias). &quot;Logits&quot; is a common name to
assign to the output of a fully connected layer (which is what we have with the
matrix-vector multiplication in the first stage); the logits are arbitrary real
numbers. The softmax function is responsibe for squeezing them into the range of
probabilities (0, 1) and making sure they all add up to 1.</p>
<p>This diagram shows what happens to a single input as it goes through the model.
In a realistic program, there will be another dimension - the batch dimension,
used to vectorize the computation over a whole batch of inputs.</p>
<p>For training this model, we need a loss function. It turns out cross-entropy is
a very popular loss function to use for softmax. In the
<a class="reference external" href="../the-softmax-function-and-its-derivative/index.html">softmax post</a>
I also covered how to compute the gradient of cross-entropy on a softmax, so
we're all set to write some code: the <a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/logistic-regression/mnist_softmax_classifier.py">full sample is here</a>.
Running it on MNIST for a couple of minutes produces a 9.5% error rate -
slightly better than the OvA approach, but very close. This is to be expected,
since OvA and softmax compute very similar results (finding the maximal
probability from a set of probabilities), just in a different way. Softmax
regression is much faster, however, since we can vectorize the training for all
10 digits in the same run.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td>In this post I'm following many of the conventions established in
my post on <a class="reference external" href="../linear-regression/index.html">linear regression</a>.
In particular, by construction <img alt="x_0=1" class="valign-m3" src="../../images/math/0c1d7f319728a07a57d000f2379b5215e4130147.png" style="height: 15px;" /> so that <img alt="\theta_0" class="valign-m3" src="../../images/math/ba6201ddbe2fd0bb66e0704ad8b3c6bdb36f37aa.png" style="height: 15px;" />
is the bias.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td>Why? Because we have the bias as part of the model, so any constant
offset can be absorbed into the learned bias.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[3]</a></td><td>Note that this outcome is, once again, somewhat arbitrary. We could find
another plane that intersects the x/y axis on the same line, and get
a different classification. For example, if we flip the sign of all the
elements of <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" />, we get the same intersection line. In that
case, however, values &quot;to the right&quot; of the line give us
<img alt="\hat{y}(x) &amp;lt; 0" class="valign-m4" src="../../images/math/d8a7e77c45cecd8e4ba7c8f7d1f02944e9b55ecf.png" style="height: 18px;" />. Since the labels we attach are arbitrary, this
really makes no difference. The only important thing is that we find
a line that separates &quot;true&quot; from &quot;false&quot; samples and be consistent with
our signs and labels throughout the process.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-4">[4]</a></td><td>Note that both the loss and the regularization are called <img alt="L_2" class="valign-m3" src="../../images/math/0d2398f5890edff3f40f1686fc3b51528209bf9b.png" style="height: 15px;" />.
This is a bit confusing, but both are essentially 2nd norms. It's best
to ignore the name of the regularization factor and just refer to it as
&quot;regularization&quot;. I thought it's important to mention initially as there
are other kinds of regularization being used for machine-learning
algorithms and I wanted to make it clear which one is being used here.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-5">[5]</a></td><td>As an exercise, play with the code to increase or decrease the number
of outliers (the code makes it easily controllable), and observe the
effects on the misclassification rates of the different loss functions.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-6">[6]</a></td><td>Note that using the logistic function on the model's output is strictly
a generalization of the binary classifier. We can still make a binary
interpretation of the result if we're so inclined, interpreting
<img alt="S(z) \geq 0.5" class="valign-m4" src="../../images/math/763035b41ff594d664c57d9fcc03c85808d0ccce.png" style="height: 18px;" /> as &quot;yes&quot; and otherwise as &quot;no&quot;. In terms of the
input to <img alt="S(z)" class="valign-m4" src="../../images/math/61bc9efb9d2c99669df519617ee7daee7670e156.png" style="height: 18px;" />, this means &quot;yes&quot; for
<img alt="z=\hat{y}(x) \geq 0" class="valign-m4" src="../../images/math/2599fe02308a2d43e5b29b2f9387ee45c5c67a1b.png" style="height: 18px;" />
which is exactly the formulation we've been using for the binary
classifier.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-7">[7]</a></td><td>In essence, cross entropy is computed between two probability
distributions. Here, one of them is the &quot;real&quot; distribution observed in
the <em>y</em> data. The other is what we predict given <em>X</em> data and our
regression parameters <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" />. The observed real probability is
either 0 or 1 for any given data item, and the corresponding predicted
probability is our model's output. I also discussed cross-entropy in
the <a class="reference external" href="../the-softmax-function-and-its-derivative/index.html">post about softmax</a>.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-8">[8]</a></td><td>Many resources online condense this formula to a single line without
the condition: <img alt="C(x)=-ylog(S(x))-(1-y)log(1-S(x))" class="valign-m4" src="../../images/math/0f9309397d7ef59c72cdb2d861e5532292978ca6.png" style="height: 18px;" />. I'm avoiding
this formulation on purpose, because it requires the possible values of
<em>y</em> to be 0 and 1, not -1 and +1. Although it's possible to play with
constants a bit to reformulate the -1/+1 case in a similarly condensed
fashion, I find the version with the condition more explicit and thus
easier to follow, even if it requires a bit more typing.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-9">[9]</a></td><td>See also <a class="reference external" href="../the-chain-rule-of-calculus/index.html">my post</a>
about the chain rule, where this derivation is shown.</td></tr>
</tbody>
</table>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2016/logistic-regression/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:51:39 GMT -->
</html>