<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:51:27 GMT -->
<head>
    <title>The Softmax function and its derivative - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../theme/css/style.css" type="text/css"/>

        <link href="../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../index.html" class="navbar-brand">
                <img src="../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="index.html"
                       rel="bookmark"
                       title="Permalink to The Softmax function and its derivative">
                        The Softmax function and its derivative
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> October 18, 2016 at 05:20</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../tag/math.html">Math</a>
        ,
    <a href="../../tag/machine-learning.html">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>The softmax function takes an N-dimensional vector of arbitrary real values and
produces another N-dimensional vector with real values in the range (0, 1) that
add up to 1.0. It maps <object class="valign-m4" data="../../images/math/1dd52a52398e38c9549b289449de49ba5fbb98b7.svg" style="height: 19px;" type="image/svg+xml">S(\mathbf{a}):\mathbb{R}^{N}\rightarrow \mathbb{R}^{N}</object>:</p>
<object class="align-center" data="../../images/math/cd593d87595e496072aebf5100dd87c37c889f25.svg" style="height: 86px;" type="image/svg+xml">\[S(\mathbf{a}):\begin{bmatrix}
a_1\\
a_2\\
\cdots\\
a_N
\end{bmatrix}
\rightarrow
\begin{bmatrix}
S_1\\
S_2\\
\cdots\\
S_N
\end{bmatrix}\]</object>
<p>And the actual per-element formula is:</p>
<object class="align-center" data="../../images/math/5470218612381816a8c9a897d43201757560e646.svg" style="height: 46px;" type="image/svg+xml">\[S_j=\frac{e^{a_j}}{\sum_{k=1}^{N}e^{a_k}}
\qquad \forall j \in 1..N\]</object>
<p>It's easy to see that <object class="valign-m6" data="../../images/math/cb8b5683be866b4c177c0c319e14085f25bec523.svg" style="height: 18px;" type="image/svg+xml">S_j</object> is always positive (because of the exponents);
moreover, since the numerator appears in the denominator summed up with some
other positive numbers, <object class="valign-m6" data="../../images/math/5a34de9dd188a5a6f758bb0f7daabb58e03045ec.svg" style="height: 18px;" type="image/svg+xml">S_j&lt;1</object>. Therefore, it's in the range (0, 1).</p>
<p>For example, the 3-element vector <tt class="docutils literal">[1.0, 2.0, 3.0]</tt> gets transformed into
<tt class="docutils literal">[0.09, 0.24, 0.67]</tt>. The order of elements by relative size is
preserved, and they add up to 1.0. Let's tweak this vector slightly into:
<tt class="docutils literal">[1.0, 2.0, 5.0]</tt>. We get the output <tt class="docutils literal">[0.02, 0.05, 0.93]</tt>, which still
preserves these properties. Note that as the last element is farther away
from the first two, it's softmax value is dominating the overall slice of size
1.0 in the output. Intuitively, the softmax function is a &quot;soft&quot; version of the
maximum function. Instead of just selecting one maximal element, softmax breaks
the vector up into parts of a whole (1.0) with the maximal input element getting
a proportionally larger chunk, but the other elements getting some of it as well
<a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a>.</p>
<div class="section" id="probabilistic-interpretation">
<h2>Probabilistic interpretation</h2>
<p>The properties of softmax (all output values in the range (0, 1) and sum up to
1.0) make it suitable for a probabilistic interpretation that's very useful
in machine learning. In particular, in multiclass classification tasks, we
often want to assign probabilities that our input belongs to one of a set of
output classes.</p>
<p>If we have N output classes, we're looking for an N-vector of probabilities that
sum up to 1; sounds familiar?</p>
<p>We can interpret softmax as follows:</p>
<object class="align-center" data="../../images/math/4510f717b770547b90526c714355f4c81d1b4a50.svg" style="height: 19px;" type="image/svg+xml">\[S_j=P(y=j|a)\]</object>
<p>Where <em>y</em> is the output class numbered <object class="valign-m1" data="../../images/math/310debdf2f7fe03ad7888e95000c78a0efae5500.svg" style="height: 13px;" type="image/svg+xml">1..N</object>. <em>a</em> is any N-vector. The
most basic example is <a class="reference external" href="../logistic-regression/index.html">multiclass logistic regression</a>, where an input
vector <em>x</em> is multiplied by a weight matrix <em>W</em>, and the result of this dot
product is fed into a softmax function to produce probabilities. This
architecture is explored in detail later in the post.</p>
</div>
<div class="section" id="some-preliminaries-from-vector-calculus">
<h2>Some preliminaries from vector calculus</h2>
<p>Before diving into computing the derivative of softmax, let's start with some
preliminaries from vector calculus.</p>
<p>Softmax is fundamentally a vector function. It takes a vector as input and
produces a vector as output; in other words, it has multiple inputs and multiple
outputs. Therefore, we cannot just ask for &quot;the derivative of softmax&quot;; We
should instead specify:</p>
<ol class="arabic simple">
<li>Which component (output element) of softmax we're seeking to find the
derivative of.</li>
<li>Since softmax has multiple inputs, with respect to which input element the
partial derivative is computed.</li>
</ol>
<p>If this sounds complicated, don't worry. This is exactly why the notation of
vector calculus was developed. What we're looking for is the partial
derivatives:</p>
<object class="align-center" data="../../images/math/2eae0a040f9eb82a2cf0a596c926aca49a3cdb66.svg" style="height: 42px;" type="image/svg+xml">\[\frac{\partial S_i}{\partial a_j}\]</object>
<p>This is the partial derivative of the i-th output w.r.t. the j-th input. A
shorter way to write it that we'll be using going forward is: <object class="valign-m6" data="../../images/math/ca95d97dc85a733a280ccaab680d01727376e383.svg" style="height: 18px;" type="image/svg+xml">D_{j}S_i</object>.</p>
<p>Since softmax is a <object class="valign-m1" data="../../images/math/91b745aec8f7c3a5501975b040a4aef477c31412.svg" style="height: 16px;" type="image/svg+xml">\mathbb{R}^{N}\rightarrow \mathbb{R}^{N}</object> function,
the most general derivative we compute for it is the Jacobian matrix:</p>
<object class="align-center" data="../../images/math/7af5ba48ed18f62f0fa31b60ba35e8e94054931c.svg" style="height: 76px;" type="image/svg+xml">\[DS=\begin{bmatrix}
D_1 S_1 &amp; \cdots &amp; D_N S_1 \\
\vdots &amp; \ddots  &amp; \vdots \\
D_1 S_N &amp; \cdots &amp; D_N S_N
\end{bmatrix}\]</object>
<p>In ML literature, the term &quot;gradient&quot; is commonly used to stand in for the
derivative. Strictly speaking, gradients are only defined for scalar functions
(such as loss functions in ML); for vector functions like softmax it's imprecise
to talk about a &quot;gradient&quot;; the Jacobian is the fully general derivate of a
vector function, but in most places I'll just be saying &quot;derivative&quot;.</p>
</div>
<div class="section" id="derivative-of-softmax">
<h2>Derivative of softmax</h2>
<p>Let's compute <object class="valign-m6" data="../../images/math/166e309484516e7fea86d27f36f42639ab73b471.svg" style="height: 18px;" type="image/svg+xml">D_j S_i</object> for arbitrary <em>i</em> and <em>j</em>:</p>
<object class="align-center" data="../../images/math/dbee1c4ac839a1eef7202f447f754341eec98904.svg" style="height: 53px;" type="image/svg+xml">\[D_j S_i=\frac{\partial S_i}{\partial a_j}=
\frac{\partial \frac{e^{a_i}}{\sum_{k=1}^{N}e^{a_k}}}{\partial a_j}\]</object>
<p>We'll be using the quotient rule of derivatives. For
<object class="valign-m9" data="../../images/math/25ee22368ab19a6e8608ac7417cf62e235794e54.svg" style="height: 29px;" type="image/svg+xml">f(x) = \frac{g(x)}{h(x)}</object>:</p>
<object class="align-center" data="../../images/math/c0fd805caf8b7d8336e8c52f2759b3ce73295315.svg" style="height: 43px;" type="image/svg+xml">\[f&#x27;(x) = \frac{g&#x27;(x)h(x) - h&#x27;(x)g(x)}{[h(x)]^2}\]</object>
<p>In our case, we have:</p>
<object class="align-center" data="../../images/math/167b7392a9d51fbc4016901d48995f091f627e3a.svg" style="height: 82px;" type="image/svg+xml">\[\begin{align*}
g_i&amp;=e^{a_i} \\
h_i&amp;=\sum_{k=1}^{N}e^{a_k}
\end{align*}\]</object>
<p>Note that no matter which <object class="valign-m6" data="../../images/math/c2d2e987a5cb0df2f497d2dba0da0960fb6fbcc0.svg" style="height: 14px;" type="image/svg+xml">a_j</object> we compute the derivative of <object class="valign-m3" data="../../images/math/969951984c96d748d949ee5e5322f4c2dbb75087.svg" style="height: 16px;" type="image/svg+xml">h_i</object>
for, the answer will always be <object class="valign-0" data="../../images/math/a4c5fca09246e4e7c55473070976f788e032c514.svg" style="height: 12px;" type="image/svg+xml">e^{a_j}</object>. This is not the case for
<object class="valign-m4" data="../../images/math/d141c63d6e5b4ff91ec2936c9b320454461258a0.svg" style="height: 12px;" type="image/svg+xml">g_i</object>, howewer. The derivative of <object class="valign-m4" data="../../images/math/d141c63d6e5b4ff91ec2936c9b320454461258a0.svg" style="height: 12px;" type="image/svg+xml">g_i</object> w.r.t. <object class="valign-m6" data="../../images/math/c2d2e987a5cb0df2f497d2dba0da0960fb6fbcc0.svg" style="height: 14px;" type="image/svg+xml">a_j</object> is
<object class="valign-0" data="../../images/math/a4c5fca09246e4e7c55473070976f788e032c514.svg" style="height: 12px;" type="image/svg+xml">e^{a_j}</object> only if <object class="valign-m4" data="../../images/math/8e4587fc82ce6377530643c5622b41e53cdf3dd3.svg" style="height: 16px;" type="image/svg+xml">i=j</object>, because only then <object class="valign-m4" data="../../images/math/d141c63d6e5b4ff91ec2936c9b320454461258a0.svg" style="height: 12px;" type="image/svg+xml">g_i</object> has
<object class="valign-m6" data="../../images/math/c2d2e987a5cb0df2f497d2dba0da0960fb6fbcc0.svg" style="height: 14px;" type="image/svg+xml">a_j</object> anywhere in it. Otherwise, the derivative is 0.</p>
<p>Going back to our <object class="valign-m6" data="../../images/math/166e309484516e7fea86d27f36f42639ab73b471.svg" style="height: 18px;" type="image/svg+xml">D_j S_i</object>; we'll start with the <object class="valign-m4" data="../../images/math/8e4587fc82ce6377530643c5622b41e53cdf3dd3.svg" style="height: 16px;" type="image/svg+xml">i=j</object> case. Then,
using the quotient rule we have:</p>
<object class="align-center" data="../../images/math/d7489693552878c00ad6788a0c8987416cbb0796.svg" style="height: 53px;" type="image/svg+xml">\[\frac{\partial \frac{e^{a_i}}{\sum_{k=1}^{N}e^{a_k}}}{\partial a_j}=
\frac{{}e^{a_i}\Sigma-e^{a_j}e^{a_i}}{\Sigma^2}\]</object>
<p>For simplicity <object class="valign-0" data="../../images/math/cb5615b3fcee824f137c372e351ccca3ff3a3292.svg" style="height: 12px;" type="image/svg+xml">\Sigma</object> stands for <object class="valign-m6" data="../../images/math/2c3662fbb97e3b5c528e8b1cdf89e108bfeed206.svg" style="height: 23px;" type="image/svg+xml">\sum_{k=1}^{N}e^{a_k}</object>.
Reordering a bit:</p>
<object class="align-center" data="../../images/math/2634d0ab6532983a88a1f55a33cf6a6719a291ee.svg" style="height: 123px;" type="image/svg+xml">\[\begin{align*}
\frac{\partial \frac{e^{a_i}}{\sum_{k=1}^{N}e^{a_k}}}{\partial a_j}&amp;=
\frac{e^{a_i}\Sigma-e^{a_j}e^{a_i}}{\Sigma^2}\\
&amp;=\frac{e^{a_i}}{\Sigma}\frac{\Sigma - e^{a_j}}{\Sigma}\\
&amp;=S_i(1-S_j)
\end{align*}\]</object>
<p>The final formula expresses the derivative in terms of <object class="valign-m3" data="../../images/math/3e218c43050832e5df45f69fb2c8b8a01f7f5a52.svg" style="height: 15px;" type="image/svg+xml">S_i</object> itself - a
common trick when functions with exponents are involved.</p>
<p>Similarly, we can do the <object class="valign-m4" data="../../images/math/09eca402f8bc6311cca3a98625e29e75cc336d31.svg" style="height: 17px;" type="image/svg+xml">i\ne j</object> case:</p>
<object class="align-center" data="../../images/math/d788a4ff0e07827862aaf0ded5befbf1665d90cc.svg" style="height: 123px;" type="image/svg+xml">\[\begin{align*}
\frac{\partial \frac{e^{a_i}}{\sum_{k=1}^{N}e^{a_k}}}{\partial a_j}&amp;=
\frac{0-e^{a_j}e^{a_i}}{\Sigma^2}\\
&amp;=-\frac{e^{a_j}}{\Sigma}\frac{e^{a_i}}{\Sigma}\\
&amp;=-S_j S_i
\end{align*}\]</object>
<p>To summarize:</p>
<object class="align-center" data="../../images/math/f776365373202f727625c0be825d55a2fde47882.svg" style="height: 43px;" type="image/svg+xml">\[D_j S_i=\left\{\begin{matrix}
S_i(1-S_j) &amp; i=j\\
-S_j S_i &amp; i\ne j
\end{matrix}\right\]</object>
<p>I like seeing this explicit breakdown by cases, but if anyone is taking more
pride in being concise and clever than programmers, it's mathematicians. This
is why you'll find various &quot;condensed&quot; formulations of the same equation in the
literature. One of the most common ones is using the Kronecker delta function:</p>
<object class="align-center" data="../../images/math/ff38cb90472289e31bd7f79c1c85c455d7962cbb.svg" style="height: 43px;" type="image/svg+xml">\[\delta_{ij}=\left\{\begin{matrix}
1 &amp; i=j\\
0 &amp; i\ne j
\end{matrix}\right\]</object>
<p>To write:</p>
<object class="align-center" data="../../images/math/6e4b626a68faabba991f9d1e83a12c74fcec0e63.svg" style="height: 19px;" type="image/svg+xml">\[D_j S_i = S_i (\delta_{ij}-S_j)\]</object>
<p>Which is, of course, the same thing. There are a couple of other formulations
one sees in the literature:</p>
<ol class="arabic simple">
<li>Using the matrix formulation of the Jacobian directly to replace
<object class="valign-0" data="../../images/math/3a6a16552e246af497720ffdfe6091b42d2f8938.svg" style="height: 12px;" type="image/svg+xml">\delta</object> with <object class="valign-0" data="../../images/math/ca73ab65568cd125c2d27a22bbd9e863c10b675d.svg" style="height: 12px;" type="image/svg+xml">I</object> - the identity matrix, whose elements are
expressing <object class="valign-0" data="../../images/math/3a6a16552e246af497720ffdfe6091b42d2f8938.svg" style="height: 12px;" type="image/svg+xml">\delta</object> in matrix form.</li>
<li>Using &quot;1&quot; as the function name instead of the Kroneker delta, as follows:
<object class="valign-m6" data="../../images/math/a4fa3293a004c9dc1f5171ddb590ac9cb7178102.svg" style="height: 20px;" type="image/svg+xml">D_j S_i = S_i (1(i=j)-S_j)</object>. Here <object class="valign-m4" data="../../images/math/d9e260212cd116b69ffa42e9c9f824b2bcf6a217.svg" style="height: 18px;" type="image/svg+xml">1(i=j)</object> means the value 1
when <object class="valign-m4" data="../../images/math/8e4587fc82ce6377530643c5622b41e53cdf3dd3.svg" style="height: 16px;" type="image/svg+xml">i=j</object> and the value 0 otherwise.</li>
</ol>
<p>The condensed notation comes useful when we want to compute more complex
derivatives that depend on the softmax derivative; otherwise we'd have to
propagate the condition everywhere.</p>
</div>
<div class="section" id="computing-softmax-and-numerical-stability">
<h2>Computing softmax and numerical stability</h2>
<p>A simple way of computing the softmax function on a given vector in Python is:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the softmax of vector x.&quot;&quot;&quot;</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
</pre></div>
<p>Let's try it with the sample 3-element vector we've used as an example earlier:</p>
<div class="highlight"><pre><span></span>In [146]: softmax([1, 2, 3])
Out[146]: array([ 0.09003057,  0.24472847,  0.66524096])
</pre></div>
<p>However, if we run this function with larger numbers (or large negative numbers)
we have a problem:</p>
<div class="highlight"><pre><span></span>In [148]: softmax([1000, 2000, 3000])
Out[148]: array([ nan,  nan,  nan])
</pre></div>
<p>The numerical range of the floating-point numbers used by Numpy
is limited. For <tt class="docutils literal">float64</tt>, the maximal representable number is on the order
of <object class="valign-m1" data="../../images/math/91d9772e2d01d53580c14ba9801ea3303f45cac7.svg" style="height: 16px;" type="image/svg+xml">10^{308}</object>. Exponentiation in the softmax function makes it possible to
easily overshoot this number, even for fairly modest-sized inputs.</p>
<p>A nice way to avoid this problem is by normalizing the inputs to be
not too large or too small, by observing that we can use an arbitrary constant
<em>C</em> as follows:</p>
<object class="align-center" data="../../images/math/21c627f153906b6de2c2723f4a20629a610945ba.svg" style="height: 46px;" type="image/svg+xml">\[S_j=\frac{e^{a_j}}{\sum_{k=1}^{N}e^{a_k}}=\frac{Ce^{a_j}}{\sum_{k=1}^{N}Ce^{a_k}}\]</object>
<p>And then pushing the constant into the exponent, we get:</p>
<object class="align-center" data="../../images/math/c5b631159b49e84338269e0943e00da2fb7f5d21.svg" style="height: 51px;" type="image/svg+xml">\[S_j=\frac{e^{a_j+log(C)}}{\sum_{k=1}^{N}e^{a_k+log(C)}}\]</object>
<p>Since <em>C</em> is just an arbitrary constant, we can instead write:</p>
<object class="align-center" data="../../images/math/7ae51c811f1348f4762e3eee1a3cc9e8aad1890c.svg" style="height: 49px;" type="image/svg+xml">\[S_j=\frac{e^{a_j+D}}{\sum_{k=1}^{N}e^{a_k+D}}\]</object>
<p>Where <em>D</em> is also an arbitrary constant. This formula is equivalent to the
original <object class="valign-m6" data="../../images/math/cb8b5683be866b4c177c0c319e14085f25bec523.svg" style="height: 18px;" type="image/svg+xml">S_j</object> for any <em>D</em>, so we're free to choose a <em>D</em> that will make
our computation better numerically. A good choice is the maximum between all
inputs, negated:</p>
<object class="align-center" data="../../images/math/0433b741304b0b54a6e11be1602b63d4b6326e98.svg" style="height: 18px;" type="image/svg+xml">\[D=-max(a_1, a_2, \cdots, a_N)\]</object>
<p>This will shift the inputs to a range close to zero, assuming the inputs
themselves are not too far from each other. Crucially, it shifts them all to be
negative (except the maximal <object class="valign-m6" data="../../images/math/c2d2e987a5cb0df2f497d2dba0da0960fb6fbcc0.svg" style="height: 14px;" type="image/svg+xml">a_j</object> which turns into a zero). Negatives
with large exponents &quot;saturate&quot; to zero rather than infinity, so we have a
better chance of avoiding NaNs.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">stablesoftmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the softmax of vector x in a numerically stable way.&quot;&quot;&quot;</span>
    <span class="n">shiftx</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">exps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">shiftx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
</pre></div>
<p>And now:</p>
<div class="highlight"><pre><span></span>In [150]: stablesoftmax([1000, 2000, 3000])
Out[150]: array([ 0.,  0.,  1.])
</pre></div>
<p>Note that this is still imperfect, since mathematically softmax would never
really produce a zero, but this is much better than NaNs, and since the distance
between the inputs is very large it's expected to get a result extremely close
to zero anyway.</p>
</div>
<div class="section" id="the-softmax-layer-and-its-derivative">
<h2>The softmax layer and its derivative</h2>
<p>A common use of softmax appears in machine learning, in particular in logistic
regression: the softmax &quot;layer&quot;, wherein we apply softmax to the output of a
fully-connected layer (matrix multiplication):</p>
<img alt="Generic softmax layer diagram" class="align-center" src="../../images/2016/softmax-layer-generic.png" />
<p>In this diagram, we have an input <em>x</em> with N features, and T possible
output classes. The weight matrix <em>W</em> is used to transform <em>x</em> into a vector
with T elements (called &quot;logits&quot; in ML folklore), and the softmax function is
used to &quot;collapse&quot; the logits into a vector of probabilities denoting the
probability of <em>x</em> belonging to each one of the T output classes.</p>
<p>How do we compute the derivative of this &quot;softmax layer&quot; (fully-connected matrix
multiplication followed by softmax)? Using the chain rule, of course! You'll
find any number of derivations of this derivative online, but I want to approach
it from first principles, by carefully applying the <a class="reference external" href="../the-chain-rule-of-calculus/index.html">multivariate chain rule</a> to the
Jacobians of the functions involved.</p>
<p>An important point before we get started: you may think that <em>x</em> is a natural
variable to compute the derivative for. But it's not. In fact, in machine learning
we usually want to find the best weight matrix <em>W</em>, and thus it is <em>W</em> we want
to update with every step of <a class="reference external" href="../understanding-gradient-descent.html">gradient descent</a>. Therefore,
we'll be computing the derivative of this layer w.r.t. <em>W</em>.</p>
<p>Let's start by rewriting this diagram as a composition of vector functions.
First, we have the matrix multiplication, which we denote <object class="valign-m4" data="../../images/math/a0e38e0d2b015bcbf88c39139b08982ae8b9529d.svg" style="height: 18px;" type="image/svg+xml">g(W)</object>. It maps
<object class="valign-m1" data="../../images/math/41cbe7438e5529bcab383579b09d611cd97f0444.svg" style="height: 16px;" type="image/svg+xml">\mathbb{R}^{NT}\rightarrow \mathbb{R}^{T}</object>, because the input (matrix
<em>W</em>) has <em>N times T</em> elements, and the output has T elements.</p>
<p>Next we have the softmax. If we denote the vector of logits as <object class="valign-0" data="../../images/math/b3931f1ce298c536432fd324b3a1ab4337120689.svg" style="height: 12px;" type="image/svg+xml">\lambda</object>,
we have <object class="valign-m4" data="../../images/math/9af2279e6f8c350d3e301ff7ed97ff2d23d2b478.svg" style="height: 19px;" type="image/svg+xml">S(\lambda):\mathbb{R}^{T}\rightarrow \mathbb{R}^{T}</object>. Overall,
we have the function composition:</p>
<object class="align-center" data="../../images/math/10e8a3123f66fe60ae76a3fe83b2a9b73ea3fa57.svg" style="height: 45px;" type="image/svg+xml">\[\begin{align*}
P(W)&amp;=S(g(W)) \\
    &amp;=(S\circ g)(W)
\end{align*}\]</object>
<p>By applying the multivariate chain rule, the Jacobian of <object class="valign-m4" data="../../images/math/f6dd867bfc20ac609f598f54ed834172e0985b0b.svg" style="height: 18px;" type="image/svg+xml">P(W)</object> is:</p>
<object class="align-center" data="../../images/math/80f6a3c715eb405a68968e4c579d3a2b562cfab0.svg" style="height: 18px;" type="image/svg+xml">\[DP(W)=D(S\circ g)(W)=DS(g(W))\cdot Dg(W)\]</object>
<p>We've computed the Jacobian of <object class="valign-m4" data="../../images/math/7f3a73c41d966d0cade30c5b1fadd35290358a15.svg" style="height: 18px;" type="image/svg+xml">S(a)</object> earlier in this post; what's
remaining is the Jacobian of <object class="valign-m4" data="../../images/math/a0e38e0d2b015bcbf88c39139b08982ae8b9529d.svg" style="height: 18px;" type="image/svg+xml">g(W)</object>. Since <em>g</em> is a very simple function,
computing its Jacobian is easy; the only complication is dealing with the
indices correctly. We have to keep track of which weight each derivative is for.
Since <object class="valign-m4" data="../../images/math/50dd3f482e6e8490b6b54b110c2b8e9018c6a607.svg" style="height: 19px;" type="image/svg+xml">g(W):\mathbb{R}^{NT}\rightarrow \mathbb{R}^{T}</object>, its Jacobian has
<em>T</em> rows and <em>NT</em> columns:</p>
<object class="align-center" data="../../images/math/0d59698eb2307932fdb5a94b7f089da40688f368.svg" style="height: 76px;" type="image/svg+xml">\[Dg=\begin{bmatrix}
D_1 g_1 &amp; \cdots &amp; D_{NT} g_1 \\
\vdots &amp; \ddots  &amp; \vdots \\
D_1 g_T &amp; \cdots &amp; D_{NT} g_T
\end{bmatrix}\]</object>
<p>In a sense, the weight matrix <em>W</em> is &quot;linearized&quot; to a vector of length <em>NT</em>. If
you're familiar with the <a class="reference external" href="../../2015/memory-layout-of-multi-dimensional-arrays.html">memory layout of multi-dimensional arrays</a>,
it should be easy to understand how it's done. In our case, one simple thing we
can do is linearize it in row-major order, where the first row is consecutive,
followed by the second row, etc. Mathematically, <object class="valign-m6" data="../../images/math/14147644eaa95a20bf61a81af56045475f386a83.svg" style="height: 18px;" type="image/svg+xml">W_{ij}</object> will get column
number <object class="valign-m4" data="../../images/math/ef7b2d987af3c0ceb75381d096c35e8c19085642.svg" style="height: 18px;" type="image/svg+xml">(i-1)N+j</object> in the Jacobian. To populate <object class="valign-m4" data="../../images/math/38b655437da0880bd70168fcbadb50ebdbf46ca5.svg" style="height: 16px;" type="image/svg+xml">Dg</object>, let's recall
what <object class="valign-m4" data="../../images/math/434575851c19a9826fb6be1ca130ffa3243a2a34.svg" style="height: 12px;" type="image/svg+xml">g_1</object> is:</p>
<object class="align-center" data="../../images/math/64a7924d431e1a8e82f753f1f04943ddd619fedb.svg" style="height: 16px;" type="image/svg+xml">\[g_1=W_{11}x_1+W_{12}x_2+\cdots +W_{1N}x_N\]</object>
<p>Therefore:</p>
<object class="align-center" data="../../images/math/2a64f2f7fdb74ca1e0e3bf86da7e9874e8855928.svg" style="height: 177px;" type="image/svg+xml">\[\begin{align*}
D_1g_1&amp;=x_1 \\
D_2g_1&amp;=x_2 \\
\cdots \\
D_Ng_1&amp;=x_N \\
D_{N+1}g_1&amp;=0 \\
\cdots \\
D_{NT}g_1&amp;=0
\end{align*}\]</object>
<p>If we follow the same approach to compute <object class="valign-m4" data="../../images/math/eeb76bb8cb07245435e01abcd03dec71f9c051df.svg" style="height: 12px;" type="image/svg+xml">g_2...g_T</object>, we'll get the
Jacobian matrix:</p>
<object class="align-center" data="../../images/math/5b0d880f118ea950dd4c676a9aad2e481d83b0bf.svg" style="height: 76px;" type="image/svg+xml">\[Dg=\begin{bmatrix}
x_1 &amp; x_2 &amp; \cdots &amp; x_N &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; x_1 &amp; x_2 &amp; \cdots &amp; x_N
\end{bmatrix}\]</object>
<p>Looking at it differently, if we split the index of <em>W</em> to <em>i</em> and <em>j</em>, we get:</p>
<object class="align-center" data="../../images/math/3ca9791a8734377178476d2069bbb072b7e345ac.svg" style="height: 44px;" type="image/svg+xml">\[\begin{align*}
D_{ij}g_t&amp;=\frac{\partial(W_{t1}x_1+W_{t2}x_2+\cdots+W_{tN}x_N)}{\partial W_{ij}}
         &amp;= \left\{\begin{matrix}
x_j &amp; i = t\\
0 &amp; i \ne t
\end{matrix}\right.
\end{align*}\]</object>
<p>This goes into row <em>t</em>, column <object class="valign-m4" data="../../images/math/ef7b2d987af3c0ceb75381d096c35e8c19085642.svg" style="height: 18px;" type="image/svg+xml">(i-1)N+j</object> in the Jacobian matrix.</p>
<p>Finally, to compute the full Jacobian of the softmax layer, we just do a dot
product between <object class="valign-0" data="../../images/math/2ee0d2dca289c3eb54f4cc5e98db8d63e9b0794b.svg" style="height: 12px;" type="image/svg+xml">DS</object> and <object class="valign-m4" data="../../images/math/38b655437da0880bd70168fcbadb50ebdbf46ca5.svg" style="height: 16px;" type="image/svg+xml">Dg</object>. Note that
<object class="valign-m4" data="../../images/math/be12618361f03651d2f459ce0fa3ac82aad3b766.svg" style="height: 19px;" type="image/svg+xml">P(W):\mathbb{R}^{NT}\rightarrow \mathbb{R}^{T}</object>, so the Jacobian
dimensions work out. Since <object class="valign-0" data="../../images/math/2ee0d2dca289c3eb54f4cc5e98db8d63e9b0794b.svg" style="height: 12px;" type="image/svg+xml">DS</object> is <em>TxT</em> and <object class="valign-m4" data="../../images/math/38b655437da0880bd70168fcbadb50ebdbf46ca5.svg" style="height: 16px;" type="image/svg+xml">Dg</object> is <em>TxNT</em>, their
dot product <object class="valign-0" data="../../images/math/9f2059fa4172536236c9acfa22a911f918547e55.svg" style="height: 12px;" type="image/svg+xml">DP</object> is <em>TxNT</em>.</p>
<p>In literature you'll see a much shortened derivation of the derivative of the
softmax layer. That's fine, since the two functions involved are simple and well
known. If we carefully compute a dot product between a row in <object class="valign-0" data="../../images/math/2ee0d2dca289c3eb54f4cc5e98db8d63e9b0794b.svg" style="height: 12px;" type="image/svg+xml">DS</object> and a
column in <object class="valign-m4" data="../../images/math/38b655437da0880bd70168fcbadb50ebdbf46ca5.svg" style="height: 16px;" type="image/svg+xml">Dg</object>:</p>
<object class="align-center" data="../../images/math/699151b941880c8adf5d363048a97c6731482ed6.svg" style="height: 54px;" type="image/svg+xml">\[D_{ij}P_t=\sum_{k=1}^{T}D_kS_t\cdot D_{ij}g_k\]</object>
<p><object class="valign-m4" data="../../images/math/38b655437da0880bd70168fcbadb50ebdbf46ca5.svg" style="height: 16px;" type="image/svg+xml">Dg</object> is mostly zeros, so the end result is simpler. The only <em>k</em> for which
<object class="valign-m6" data="../../images/math/fca24bbbbf8cac80ccc0253802b13d2749770585.svg" style="height: 18px;" type="image/svg+xml">D_{ij}g_k</object> is nonzero is when <object class="valign-0" data="../../images/math/f4b7e42a4b8c52f40eb9458e68e81c74d70c1c61.svg" style="height: 13px;" type="image/svg+xml">i=k</object>; then it's equal to
<object class="valign-m6" data="../../images/math/73058e43db0f4edc791b10f27f913cbc5d361ab6.svg" style="height: 14px;" type="image/svg+xml">x_j</object>. Therefore:</p>
<object class="align-center" data="../../images/math/7f5cbb15243987230b4fa5741769938a78c9c2f2.svg" style="height: 44px;" type="image/svg+xml">\[\begin{align*}
D_{ij}P_t&amp;=D_iS_tx_j \\
         &amp;=S_t(\delta_{ti}-S_i)x_j
\end{align*}\]</object>
<p>So it's entirely possible to compute the derivative of the softmax layer without
actual Jacobian matrix multiplication; and that's good, because matrix
multiplication is expensive! The reason we can avoid most computation is that
the Jacobian of the fully-connected layer is <em>sparse</em>.</p>
<p>That said, I still felt it's important to show how this derivative comes to life
from first principles based on the composition of Jacobians for the functions
involved. The advantage of this approach is that it works exactly the same for
more complex compositions of functions, where the &quot;closed form&quot; of the derivative
for each element is much harder to compute otherwise.</p>
</div>
<div class="section" id="softmax-and-cross-entropy-loss">
<h2>Softmax and cross-entropy loss</h2>
<p>We've just seen how the softmax function is used as part of a machine learning
network, and how to compute its derivative using the multivariate chain rule.
While we're at it, it's worth to take a look at a loss function that's
commonly used along with softmax for training a network: cross-entropy.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy">Cross-entropy</a> has an
interesting probabilistic and information-theoretic interpretation, but here
I'll just focus on the mechanics. For two discrete probability distributions <em>p</em>
and <em>q</em>, the cross-entropy function is defined as:</p>
<object class="align-center" data="../../images/math/b26f68a12667ba254facf9815252f52ebf2238d9.svg" style="height: 38px;" type="image/svg+xml">\[xent(p,q)=-\sum_{k}p(k)log(q(k))\]</object>
<p>Where <em>k</em> goes over all the possible values of the random variable the
distributions are defined for. Specifically, in our case there are <em>T</em> output
classes, so <em>k</em> would go from 1 to <em>T</em>.</p>
<p>If we start from the softmax output <em>P</em> - this is one probability distribution
<a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a>. The other probability distribution is the &quot;correct&quot; classification
output, usually denoted by <em>Y</em>. This is a one-hot encoded vector of size <em>T</em>,
where all elements except one are 0.0, and one element is 1.0 - this element
marks the correct class for the data being classified. Let's rephrase the
cross-entropy loss formula for our domain:</p>
<object class="align-center" data="../../images/math/b02b400caa1de3f720f3c51b4891204a85a0d482.svg" style="height: 54px;" type="image/svg+xml">\[xent(Y, P)=-\sum_{k=1}^{T}Y(k)log(P(k))\]</object>
<p><em>k</em> goes over all the output classes. <object class="valign-m4" data="../../images/math/1801d6549d7f256091d8d687062875facf870a80.svg" style="height: 18px;" type="image/svg+xml">P(k)</object> is the probability of the
class as predicted by the model. <object class="valign-m4" data="../../images/math/369b88be91e9aecb20f084f95946d171096ec2ad.svg" style="height: 18px;" type="image/svg+xml">Y(k)</object> is the &quot;true&quot; probability of the
class as provided by the data. Let's mark the sole index where <object class="valign-m4" data="../../images/math/bf2a1a90dbf5ee8f3e1240a2aff2b64220f3e876.svg" style="height: 18px;" type="image/svg+xml">Y(k)=1.0</object>
by <em>y</em>. Since for all <object class="valign-m4" data="../../images/math/e0e4ad3507e9dde8cc37658b436305ef9eb14ca0.svg" style="height: 17px;" type="image/svg+xml">k\ne y</object> we have <object class="valign-m4" data="../../images/math/9d1a77958eb2fd853cb41001e41efcfa46a099d3.svg" style="height: 18px;" type="image/svg+xml">Y(k)=0</object>, the cross-entropy
formula can be simplified to:</p>
<object class="align-center" data="../../images/math/ca79e575abc3ff07571f9b7bd9ee477c4cac1b7a.svg" style="height: 18px;" type="image/svg+xml">\[xent(Y, P)=-log(P(y))\]</object>
<p>Actually, let's make it a function of just <em>P</em>, treating <em>y</em> as a constant.
Moreover, since in our case <em>P</em> is a vector, we can express <object class="valign-m4" data="../../images/math/033e08901a43a52bb55ac6d36bcb0cebb8781a4e.svg" style="height: 18px;" type="image/svg+xml">P(y)</object> as
the <em>y</em>-th element of <em>P</em>, or <object class="valign-m6" data="../../images/math/12b5ad2733328bc7191f23d13e05c4e246bb8e26.svg" style="height: 18px;" type="image/svg+xml">P_y</object>:</p>
<object class="align-center" data="../../images/math/e659a9fdd830a347c3aae214b31013eb52c59dc7.svg" style="height: 19px;" type="image/svg+xml">\[xent(P)=-log(P_y)\]</object>
<p>The Jacobian of <em>xent</em> is a <em>1xT</em> matrix (a row vector), since the output is a
scalar and we have <em>T</em> inputs (the vector <em>P</em> has <em>T</em> elements):</p>
<object class="align-center" data="../../images/math/2e515cd8235b0385a95e5cbfff5fbcca9a78c631.svg" style="height: 22px;" type="image/svg+xml">\[Dxent=\begin{bmatrix}
D_1xent &amp; D_2xent &amp; \cdots &amp; D_Txent
\end{bmatrix}\]</object>
<p>Now recall that <em>P</em> can be expressed as a function of input weights:
<object class="valign-m4" data="../../images/math/ad179bfd313d392ad156b509370b8f407e7bd20a.svg" style="height: 18px;" type="image/svg+xml">P(W)=S(g(W))</object>. So we have another function composition:</p>
<object class="align-center" data="../../images/math/ab2f487f02c386d5f532900ffd0927c28ed23b7c.svg" style="height: 18px;" type="image/svg+xml">\[xent(W)=(xent\circ P)(W)=xent(P(W))\]</object>
<p>And we can, once again, use the multivariate chain rule to find the gradient of
<em>xent</em> w.r.t. <em>W</em>:</p>
<object class="align-center" data="../../images/math/ef938751c387283a7be6461ab0c244ac09db85be.svg" style="height: 18px;" type="image/svg+xml">\[Dxent(W)=D(xent\circ P)(W)=Dxent(P(W))\cdot DP(W)\]</object>
<p>Let's check that the dimensions of the Jacobian matrices work out. We already
computed <object class="valign-m4" data="../../images/math/3f90f5becd4cc377e50cd6885718feb039eabcc9.svg" style="height: 18px;" type="image/svg+xml">DP(W)</object>; it's <em>TxNT</em>. <object class="valign-m4" data="../../images/math/676107d1b425649d04d82c75a37b391aa99edcf1.svg" style="height: 18px;" type="image/svg+xml">Dxent(P(W))</object> is <em>1xT</em>, so the
resulting Jacobian <object class="valign-m4" data="../../images/math/86f6a5ad8eb3128d2d86c826df3d8831403e64ac.svg" style="height: 18px;" type="image/svg+xml">Dxent(W)</object> is <em>1xNT</em>, which makes sense because the
whole network has one output (the cross-entropy loss - a scalar value) and <em>NT</em>
inputs (the weights).</p>
<p>Here again, there's a straightforward way to find a simple formula for
<object class="valign-m4" data="../../images/math/86f6a5ad8eb3128d2d86c826df3d8831403e64ac.svg" style="height: 18px;" type="image/svg+xml">Dxent(W)</object>, since many elements in the matrix multiplication end up
cancelling out. Note that <object class="valign-m4" data="../../images/math/bb805dc98dfe8b48ded94e4f27a90e74b64371e4.svg" style="height: 18px;" type="image/svg+xml">xent(P)</object> depends only on the <em>y</em>-th element of
<em>P</em>. Therefore, only <object class="valign-m6" data="../../images/math/fb396ced0aaf5ee006e13bb7b0925ba833e01a12.svg" style="height: 18px;" type="image/svg+xml">D_{y}xent</object> is non-zero in the Jacobian:</p>
<object class="align-center" data="../../images/math/c2b2a7aa200023fd2988991212edc5053a85731e.svg" style="height: 22px;" type="image/svg+xml">\[Dxent=\begin{bmatrix}
0 &amp; 0 &amp; D_{y}xent &amp; \cdots &amp; 0
\end{bmatrix}\]</object>
<p>And <object class="valign-m10" data="../../images/math/ba6bd8869680cb3dab4a5138b909d4f4155ae6a8.svg" style="height: 26px;" type="image/svg+xml">D_{y}xent=-\frac{1}{P_y}</object>. Going back to the full Jacobian
<object class="valign-m4" data="../../images/math/86f6a5ad8eb3128d2d86c826df3d8831403e64ac.svg" style="height: 18px;" type="image/svg+xml">Dxent(W)</object>, we multiply <object class="valign-m4" data="../../images/math/3845e2788792dc92a7072833fa019ce1182f4dbc.svg" style="height: 18px;" type="image/svg+xml">Dxent(P)</object> by each column of <object class="valign-m4" data="../../images/math/31bc3dde97a870d7b85f78efe4d178d38eae0fdb.svg" style="height: 18px;" type="image/svg+xml">D(P(W))</object>
to get each element in the resulting row-vector. Recall that the row vector
represents the whole weight matrix <em>W</em> &quot;linearized&quot; in row-major order. We'll
index into it with <em>i</em> and <em>j</em> for clarity (<object class="valign-m6" data="../../images/math/d82e04a1bce5f5f685c8b6ac356997c847fa95a5.svg" style="height: 18px;" type="image/svg+xml">D_{ij}</object> points to element
number <object class="valign-m4" data="../../images/math/ef7b2d987af3c0ceb75381d096c35e8c19085642.svg" style="height: 18px;" type="image/svg+xml">(i-1)N+j</object> in the row vector):</p>
<object class="align-center" data="../../images/math/b61c7d91efebf65b53f3dada643d86b63d06b6b5.svg" style="height: 54px;" type="image/svg+xml">\[D_{ij}xent(W)=\sum_{k=1}^{T}D_{k}xent(P)\cdot D_{ij}P_k(W)\]</object>
<p>Since only the <em>y</em>-th element in <object class="valign-m4" data="../../images/math/2b703a6ad534070bbe698f8d8a3a1261b5bb4549.svg" style="height: 18px;" type="image/svg+xml">D_{k}xent(P)</object> is non-zero, we get the
following, also substituting the derivative of the softmax layer from earlier in
the post:</p>
<object class="align-center" data="../../images/math/d7823846aecfb3673906d65e8da6b290b7b2f608.svg" style="height: 68px;" type="image/svg+xml">\[\begin{align*}
D_{ij}xent(W)&amp;=D_{y}xent(P)\cdot D_{ij}P_y(W) \\
             &amp;=-\frac{1}{P_y}\cdot S_y(\delta_{yi}-S_i)x_j
\end{align*}\]</object>
<p>By our definition, <object class="valign-m6" data="../../images/math/2ec0ba51607b94096ad077ab55cc181698494e1a.svg" style="height: 18px;" type="image/svg+xml">P_y=S_y</object>, so we get:</p>
<object class="align-center" data="../../images/math/e417398a544821300668c777d55ad489934d744c.svg" style="height: 96px;" type="image/svg+xml">\[\begin{align*}
D_{ij}xent(W)&amp;=-\frac{1}{S_y}\cdot S_y(\delta_{yi}-S_i)x_j \\
             &amp;=-(\delta_{yi}-S_i)x_j \\
             &amp;=(S_i-\delta_{yi})x_j
\end{align*}\]</object>
<p>Once again, even though in this case the end result is nice and clean, it didn't
necessarily have to be so. The formula for <object class="valign-m6" data="../../images/math/b0cfb602e63642cc6146ca57731821d6a9866a1e.svg" style="height: 20px;" type="image/svg+xml">D_{ij}xent(W)</object> could end up
being a fairly involved sum (or sum of sums). The technique of multiplying
Jacobian matrices is oblivious to all this, as the computer can do all the sums
for us. All we have to do is compute the individial Jacobians, which is usually
easier because they are for simpler, non-composed functions. This is the beauty
and utility of the multivariate chain rule.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td>To play more with sample inputs and Softmax outputs, Michael Nielsen's
online book has a <a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap3.html#softmax">nice interactive Javascript visualization</a> - check
it out.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td>Take a moment to recall that, by definition, the output of the softmax
function is indeed a valid discrete probability distribution.</td></tr>
</tbody>
</table>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:51:27 GMT -->
</html>