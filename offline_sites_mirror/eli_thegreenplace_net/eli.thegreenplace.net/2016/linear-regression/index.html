<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2016/linear-regression/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:59:23 GMT -->
<head>
    <title>Linear regression - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../theme/css/style.css" type="text/css"/>

        <link href="../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../index.html" class="navbar-brand">
                <img src="../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="index.html"
                       rel="bookmark"
                       title="Permalink to Linear regression">
                        Linear regression
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> August 06, 2016 at 05:28</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../tag/math.html">Math</a>
        ,
    <a href="../../tag/machine-learning.html">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>Linear regression is one of the most basic, and yet most useful approaches for
predicting a single quantitative (real-valued) variable given any number of
real-valued predictors. This article presents the basics of linear
regression for the &quot;simple&quot; (single-variable) case, as well as for the more
general multivariate case. The <a class="reference external" href="https://github.com/eliben/deep-learning-samples/tree/main/linear-regression">companion code in Python</a>
implements the techniques described in the article on simulated and realistic
data sets. The code is self-contained, using only Numpy as a dependency.</p>
<div class="section" id="simple-linear-regression">
<h2>Simple linear regression</h2>
<p>The most basic kind of regression problem has a single <em>predictor</em> (the
input) and a single outcome. Given a list of input values
<img alt="x_i" class="valign-m3" src="../../images/math/34e03e6559b14df9fe5a97bbd2ed10109dfebbd3.png" style="height: 11px;" /> and corresponding output values <img alt="y_i" class="valign-m4" src="../../images/math/35c2ac2f82d0ff8f9011b596ed7e54bfcc55f471.png" style="height: 12px;" />, we have to find
parameters <em>m</em> and <em>b</em> such that the linear function:</p>
<img alt="\[\hat{y}(x) = mx + b\]" class="align-center" src="../../images/math/2dabbcda3b1953b08211f7e334698366d647d697.png" style="height: 18px;" />
<p>Is &quot;as close as possible&quot; to the observed outcome <em>y</em>. More concretely, suppose
we get this data <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a>:</p>
<img alt="Linear regression input data" class="align-center" src="../../images/2016/linreg-data.png" />
<p>We have to find a slope <em>m</em> and intercept <em>b</em> for a line that approximates this
data as well as possible. We evaluate how well some pair of <em>m</em> and <em>b</em>
approximates the data by defining a &quot;cost function&quot;. For linear regression, a
good cost function to use is the <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean Square Error (MSE)</a> <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a>:</p>
<img alt="\[\operatorname{MSE}(m, b)=\frac{1}{n}\sum_{i=1}^n(\hat{y_i} - y_i)^2\]" class="align-center" src="../../images/math/e4b7b4ce3abd90f20144e6ab468b7870cedf3b07.png" style="height: 50px;" />
<p>Expanding <img alt="\hat{y_i}=m{x_i}+b" class="valign-m4" src="../../images/math/daecd48b7bb0a06ddd4326da5b87ee14fddaeb8e.png" style="height: 17px;" />, we get:</p>
<img alt="\[\operatorname{MSE}(m, b)=\frac{1}{n}\sum_{i=1}^n(m{x_i} + b - y_i)^2\]" class="align-center" src="../../images/math/3de1df776434b29620488aef327a9204757bc493.png" style="height: 50px;" />
<p>Let's turn this into Python code (<a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/linear-regression/simple_linear_regression.py">link to the full code sample</a>):</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the MSE cost of a prediction based on m, b.</span>

<span class="sd">    x: inputs vector</span>
<span class="sd">    y: observed outputs vector</span>
<span class="sd">    m, b: regression parameters</span>

<span class="sd">    Returns: a scalar cost.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span>
    <span class="c1"># Vectorized computation using a dot product to compute sum of squares.</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># Cost is a 1x1 matrix, we need a scalar.</span>
    <span class="k">return</span> <span class="n">cost</span><span class="o">.</span><span class="n">flat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
<p>Now we're faced with a classical optimization problem: we have some parameters
(<em>m</em> and <em>b</em>) we can tweak, and some cost function <img alt="J(m, b)" class="valign-m4" src="../../images/math/d61807c64b6ab8087a11167224df4b5f818aeae3.png" style="height: 18px;" /> we want to
minimize. The topic of mathematical optimization is vast, but what ends up
working very well for machine learning is a fairly simple algorithm called
<em>gradient descent</em>.</p>
<p>Imagine plotting <img alt="J(m, b)" class="valign-m4" src="../../images/math/d61807c64b6ab8087a11167224df4b5f818aeae3.png" style="height: 18px;" /> as a 3-dimensional surface, and picking some
random point on it. Our goal is to find the lowest point on the surface, but we
have no idea where that is. A reasonable guess is to move a bit &quot;downwards&quot; from
our current location, and then repeat.</p>
<p>&quot;Downwards&quot; is exactly what &quot;gradient descent&quot; means. We make a small change to
our location (defined by <em>m</em> and <em>b</em>) in the direction in which <img alt="J(m, b)" class="valign-m4" src="../../images/math/d61807c64b6ab8087a11167224df4b5f818aeae3.png" style="height: 18px;" />
decreases most  - the gradient <a class="footnote-reference" href="#footnote-3" id="footnote-reference-3">[3]</a>. We then repeat this process until we reach
a minimum, hopefully global. In fact, since the linear regression cost function
is <em>convex</em> we will find the global minimum this way. But in the general case
this is not guaranteed, and many sophisticated extensions of gradient descent
exist that try to avoid local minima and maximize the chance of finding a global
one.</p>
<p>Back to our function, <img alt="\operatorname{MSE}(m, b)" class="valign-m4" src="../../images/math/e42329899ba53adedf0a7884b1844dba4f01bdee.png" style="height: 18px;" />. The gradient is defined
as the vector:</p>
<img alt="\[\nabla \operatorname{MSE}=\left \langle \frac{\partial \operatorname{MSE}}{\partial m}, \frac{\partial \operatorname{MSE}}{\partial b} \right \rangle\]" class="align-center" src="../../images/math/50b0404ea5a8f76da73caae5b8109dd384dbd18e.png" style="height: 43px;" />
<p>To find it, we have to compute the partial derivatives of MSE w.r.t. the
learning parameters <em>m</em> and <em>b</em>:</p>
<img alt="\[\begin{align*} \frac{\partial \operatorname{MSE}}{\partial m}&amp;amp;=\frac{2}{n}\sum_{i=i}^n(m{x_i}+b-y_i)x_i\\ \frac{\partial \operatorname{MSE}}{\partial b}&amp;amp;=\frac{2}{n}\sum_{i=i}^n(m{x_i}+b-y_i) \end{align*}\]" class="align-center" src="../../images/math/dbd383b0d7ee194a417b88ad117b451531758fe7.png" style="height: 108px;" />
<p>And then update <em>m</em> and <em>b</em> in each step of the learning with:</p>
<img alt="\[\begin{align*} m &amp;amp;= m-\eta \frac{\partial \operatorname{MSE}}{\partial m} \\ b &amp;amp;= b-\eta \frac{\partial \operatorname{MSE}}{\partial b} \\ \end{align*}\]" class="align-center" src="../../images/math/b0c7ff699fc61836051968db56224e6470b56d3c.png" style="height: 81px;" />
<p>Where <img alt="\eta" class="valign-m4" src="../../images/math/2899aeb886ad0fa72652bffd5511e452aaf084ab.png" style="height: 12px;" /> is a customizable &quot;learning rate&quot;, a hyperparameter. Here is
the gradient descent loop in Python. Note that we examine the whole data set in
every step; for much larger data sets, SGD (Stochastic Gradient Descent) with
some reasonable mini-batch would make more sense, but for simple linear
regression problems the data size is rarely very big.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">nsteps</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Runs gradient descent optimization to fit a line y^ = x * m + b.</span>

<span class="sd">    x, y: input data and observed outputs.</span>
<span class="sd">    nsteps: how many steps to run the optimization for.</span>
<span class="sd">    learning_rate: learning rate of gradient descent.</span>

<span class="sd">    Yields &#39;nsteps + 1&#39; triplets of (m, b, cost) where m, b are the fit</span>
<span class="sd">    parameters for the given step, and cost is their cost vs the real y.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Start with m and b initialized to 0s for the first try.</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">yield</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsteps</span><span class="p">):</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">dm</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">diff</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">n</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">diff</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">n</span>
        <span class="n">m</span> <span class="o">-=</span> <span class="n">dm</span>
        <span class="n">b</span> <span class="o">-=</span> <span class="n">db</span>
        <span class="k">yield</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
<p>After running this for 30 steps, the gradient converges and the parameters
barely change. Here's a 3D plot of the cost as a function of the regression
parameters, along with a contour plot of the same function. It's easy to see
this function is convex, as expected. This makes finding the global minimum
simple, since no matter where we start, the gradient will lead us directly to
it.</p>
<p>To help visualize this, I marked the cost for each successive training step on
the contour plot - you can see how the algorithm relentlessly converges to the
minimum</p>
<img alt="Linear regression cost and contour" class="align-center" src="../../images/2016/linreg-cost-contour.png" />
<p>The final parameters learned by the regression are 2.2775 for <em>m</em> and
6.0028 for <em>b</em>, which is very close to the actual parameters I used to
generate this fake data with.</p>
<p>Here's a visualization that shows how the regression line improves progressively
during learning:</p>
<img alt="Regression fit visualization" class="align-center" src="../../images/2016/regressionfit.gif" />
</div>
<div class="section" id="evaluating-how-good-the-fit-is">
<h2>Evaluating how good the fit is</h2>
<p>In statistics, there are many ways to evaluate how good a &quot;fit&quot; some model is
on the given data. One of the most popular ones is the <em>r-squared</em> test
(&quot;coefficient of determination&quot;). It measures the proportion of the total
variance in the output (<em>y</em>) that can be explained by the variation in <em>x</em>:</p>
<img alt="\[R^2 = 1 - \frac{\sum_{i=1}^n (y_i - (m{x_i} + b))^2}{n\cdot var(y)}\]" class="align-center" src="../../images/math/2c989c7345d6901a0cf7c17f9b08762ef27c5148.png" style="height: 43px;" />
<p>This is trivial to translate to code:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_rsquared</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">SE_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span>
    <span class="n">SE_y</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">SE_line</span> <span class="o">/</span> <span class="n">SE_y</span>
</pre></div>
<p>For our regression results, I get <em>r-squared</em> of 0.76, which isn't too bad. Note
that the data is very jittery, so it's natural the regression cannot explain all
the variance. As an interesting exercise, try to modify the code that generates
the data with different standard deviations for the random noise and see the
effect on <em>r-squared</em>.</p>
</div>
<div class="section" id="an-analytical-solution-to-simple-linear-regression">
<h2>An analytical solution to simple linear regression</h2>
<p>Using the equations for the partial derivatives of MSE (shown above) it's
possible to find the minimum analytically, without having to resort to a
computational procedure (gradient descent). We compare the derivatives to zero:</p>
<img alt="\[\begin{align*} \frac{\partial \operatorname{MSE}}{\partial m}&amp;amp;=\frac{2}{n}\sum_{i=i}^n(m{x_i}+b-y_i)x_i = 0\\ \frac{\partial \operatorname{MSE}}{\partial b}&amp;amp;=\frac{2}{n}\sum_{i=i}^n(m{x_i}+b-y_i) = 0 \end{align*}\]" class="align-center" src="../../images/math/aef02f077919896478d0456619f934dcc5809142.png" style="height: 108px;" />
<p>And solve for <em>m</em> and <em>b</em>. To make the equations easier to follow, let's
introduce a bit of notation. <img alt="\bar{x}" class="valign-0" src="../../images/math/8eebe76c6f552df3f8b9480d5544fe47b1028322.png" style="height: 11px;" /> is the mean value of <em>x</em> across
all samples. Similarly <img alt="\bar{y}" class="valign-m4" src="../../images/math/1e3bffc7f71c01acbc2c12e015be3086a06f824d.png" style="height: 15px;" /> is the mean value of <em>y</em>. So the sum
<img alt="\sum_{i=1}^n x_i" class="valign-m6" src="../../images/math/c42eb1b96dfa184fee1bc0f3a4b713b9c38b2a1a.png" style="height: 20px;" /> is actually <img alt="n\bar{x}" class="valign-0" src="../../images/math/ea6008aefff0c7d79044287c44e890b1fba97c22.png" style="height: 11px;" />. Now let's take the second
equation from above and see how to simplify it:</p>
<img alt="\[\begin{align*} \frac{\partial \operatorname{MSE}}{\partial b} &amp;amp;= \frac{2}{n}\sum_{i=i}^n(m{x_i}+b-y_i) \\                                 &amp;amp;= \frac{2}{n}(mn\bar{x}+nb-n\bar{y}) \\                                 &amp;amp;= 2m\bar{x} + 2b - 2\bar{y} = 0 \end{align*}\]" class="align-center" src="../../images/math/c97c0c9ca8a66d54974fc914fcf929085dc63879.png" style="height: 119px;" />
<p>Similarly, for the partial derivative by <em>m</em> we can reach:</p>
<img alt="\[\frac{\partial \operatorname{MSE}}{\partial m}= 2m\overline{x^2} + 2b\bar{x} - 2\overline{xy} = 0\]" class="align-center" src="../../images/math/d9545273e11c9e179794f943e2c972bf62c38113.png" style="height: 38px;" />
<p>In these equations, all quantities except <em>m</em> and <em>b</em> are constant. Solving them
for the unknowns <em>m</em> and <em>b</em>, we get <a class="footnote-reference" href="#footnote-4" id="footnote-reference-4">[4]</a>:</p>
<img alt="\[m = \frac{\bar{x}\bar{y} - \overline{xy}}{\bar{x}^2 - \overline{x^2}} \qquad b = \bar{y} - \bar{x}\frac{\bar{x}\bar{y} - \overline{xy}}{\bar{x}^2 - \overline{x^2}}\]" class="align-center" src="../../images/math/becd671e8c032d0568e33b986033c181ac5c133b.png" style="height: 38px;" />
<p>If we plug the data values we have for <em>x</em> and <em>y</em> in these equations, we get
2.2777 for <em>m</em> and 6.0103 for <em>b</em> - almost exactly the values we obtained
with regression <a class="footnote-reference" href="#footnote-5" id="footnote-reference-5">[5]</a>.</p>
<p>Remember that by comparing the partial derivatives to zero we find a <em>critical
point</em>, which is not necessarily a minimum. We can use the <a class="reference external" href="https://en.wikipedia.org/wiki/Second_partial_derivative_test">second derivative
test</a> to find
what kind of critical point that is, by computing the Hessian of the cost:</p>
<img alt="\[H(m, b) = \begin{pmatrix} \operatorname{MSE}_{mm}(x, y) &amp;amp; \operatorname{MSE}_{mb}(x, y) \\ \operatorname{MSE}_{bm}(x, y) &amp;amp; \operatorname{MSE}_{bb}(x, y) \end{pmatrix}\]" class="align-center" src="../../images/math/39c2e86ae1437d3b19bc8e77b66501486550d3bc.png" style="height: 43px;" />
<p>Plugging the numbers and running the test, we can indeed verify that the
critical point is a minimum.</p>
</div>
<div class="section" id="multiple-linear-regression">
<h2>Multiple linear regression</h2>
<p>The good thing about simple regression is that it's easy to visualize. The model
is trained using just two parameters, and visualizing the cost as a function of
these two parameters is possible since we get a 3D plot. Anything beyond that
becomes increasingly more difficult to visualize.</p>
<p>In simple linear regression, every <em>x</em> is just a number; so is every <em>y</em>. In
multiple linear regression this is no longer so, and each data point <em>x</em> is a
vector. The model parameters can also be represented by the vector
<img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" />. To avoid confusion of indices and subscripts, let's agree that
we use subscripts to denote components of vectors, while parenthesized
superscripts are used to denote different samples. So <img alt="x_1^{(6)}" class="valign-m6" src="../../images/math/d01999f5014c6aea058368231c0d2b958fa8a89e.png" style="height: 26px;" /> is the
second component of sample 6.</p>
<p>Our goal is to find the vector <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> such that the linear function:</p>
<img alt="\[\hat{y}(x) = \theta_0 x_0 + \theta_1 x_1 + \cdots + \theta_n x_n\]" class="align-center" src="../../images/math/ae682f9fda97c28c8e100c87aecad635c7c1d96c.png" style="height: 18px;" />
<p>Is as close as possible to the actual <em>y</em> across all samples. Since working with
vectors is easier for this problem, we define <img alt="x_0" class="valign-m3" src="../../images/math/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;" /> to always be equal to
1, so that the first term in the equation above denotes the intercept.
Expressing the regression coefficients as a vector:</p>
<img alt="\[\begin{pmatrix} \theta_0\\ \theta_1\\ ...\\ \theta_n \end{pmatrix}\in\mathbb{R}^{n+1}\]" class="align-center" src="../../images/math/b16fd3d2b3041f13cb70199837a7c02c756078c7.png" style="height: 86px;" />
<p>We can now rewrite <img alt="\hat{y}(x)" class="valign-m4" src="../../images/math/11533fb1b0218620907f5859e6e22aeb65c12cd8.png" style="height: 18px;" /> as:</p>
<img alt="\[\hat{y}(x) = \theta^T x\]" class="align-center" src="../../images/math/8156e2dc4e654f77a8664180c168829f6b4cdb0b.png" style="height: 21px;" />
<p>Where both <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> and <em>x</em> are column vectors with <em>n+1</em> elements, as
shown above. The mean square error (over <em>k</em> samples) now becomes:</p>
<img alt="\[\operatorname{MSE}=\frac{1}{k}\sum_{i=1}^k(\hat{y}(x^{(i)}) - y^{(i)})^2\]" class="align-center" src="../../images/math/1e0a7c0c85c1827b992671b88e89ba052d37a204.png" style="height: 54px;" />
<p>Now we have to find the partial derivative of this cost by each <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" />.
Using the chain rule, it's easy to see that:</p>
<img alt="\[\frac{\partial \operatorname{MSE}}{\partial \theta_j} = \frac{2}{k}\sum_{i=1}^k(\hat{y}(x^{(i)}) - y^{(i)})x_j^{(i)}\]" class="align-center" src="../../images/math/4c2fcfed81c294ef7313198debe3801f50bea92a.png" style="height: 54px;" />
<p>And use this to update the parameters in every training step. The code is
actually not much different from the simple regression case; here is a <a class="reference external" href="https://github.com/eliben/deep-learning-samples/blob/main/linear-regression/multiple_linear_regression.py">well
documented, completely worked out example</a>.
The code takes a realistic dataset from the <a class="reference external" href="http://archive.ics.uci.edu/ml/">UCI machine learning repository</a> with 4 predictors and a single outcome and
builds a regression model. 4 predictors plus one intercept give us a
5-dimensional <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" />, which is utterly impossible to visualize, so we
have to stick to math in order to analyze it.</p>
</div>
<div class="section" id="an-analytical-solution-to-multiple-linear-regression">
<h2>An analytical solution to multiple linear regression</h2>
<p>Multiple linear regression also has an analytical solution. If we compute the
derivative of the cost by each <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" />, we'll end up with <em>n+1</em> equations
with the same number of variables, which we can solve analytically.</p>
<p>An elegant matrix formula that computes <img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> from <em>X</em> and <em>y</em> is
called the Normal Equation:</p>
<img alt="\[\theta=(X^TX)^{-1}X^Ty\]" class="align-center" src="../../images/math/20baabd9d33dcd26003bc44c7d81ba39e1ad4caa.png" style="height: 21px;" />
<p>I've written about <a class="reference external" href="../../2014/derivation-of-the-normal-equation-for-linear-regression.html">deriving the normal equation</a>
previously, so I won't spend more time on it. The accompanying code computes
<img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> using the normal equation and compares the result with the
<img alt="\theta" class="valign-0" src="../../images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;" /> obtained from gradient descent.</p>
<p>As an excercise, you can double check that the analytical solution for simple
linear regression (formulae for <em>m</em> and <em>b</em>) is just a special case of applying
the normal equation in two dimensions.</p>
<p>You may wonder: when should we use the analytical solution, and when is gradient
descent better? In general, whenever we can use the analytical solution - we
should. But it's not always feasible, computationally.</p>
<p>Consider a data set with <em>k</em> samples and <em>n</em> features. Then <em>X</em> is a <em>k x n</em>
matrix, and hence <img alt="X^TX" class="valign-0" src="../../images/math/5c817c84ec1f83b23494df6125edd091a7c413dd.png" style="height: 15px;" /> is a <em>n x n</em> matrix. Inverting a matrix is a
<img alt="O(n^3)" class="valign-m4" src="../../images/math/62a87bfd600dc05059675e34b881c78648f53401.png" style="height: 19px;" /> operation, so for large <em>n</em>, finding <img alt="(X^TX)^{-1}" class="valign-m4" src="../../images/math/57f592cee6ceac659262d97e61c64f9ca405d7f1.png" style="height: 19px;" /> can take
quite a bit of time. Moreover, keeping <img alt="X^TX" class="valign-0" src="../../images/math/5c817c84ec1f83b23494df6125edd091a7c413dd.png" style="height: 15px;" /> in memory can be
computationally infeasible if <img alt="X" class="valign-0" src="../../images/math/c032adc1ff629c9b66f22749ad667e6beadf144b.png" style="height: 12px;" /> is huge and sparse, but <img alt="X^TX" class="valign-0" src="../../images/math/5c817c84ec1f83b23494df6125edd091a7c413dd.png" style="height: 15px;" /> is
dense. In all these cases, iterative gradient descent is a more feasible
approach.</p>
<p>In addition, the moment we deviate from the linear regression a bit, such as
adding nonlinear terms, regularization, or some other model enhancement, the
analytical solutions no longer apply. Gradient descent keeps working just the
same, however, as long as we know how to compute the gradient of the new cost
function.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td>This data was generated by using a slope of 2.25, intercept of 6 and
added Gaussian noise with a standard deviation of 1.5</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td>Some resources use SSE - the Squared Sum Error, which is just the MSE
without the averaging. Yet others have <em>2n</em> in the denominator to
make the gradient derivation cleaner. None of this really matters in
practice. When finding the minimum analytically, we compare derivatives
to zero so constant factors cancel out. When running gradient descent,
all constant factors are subsumed into the learning rate which is
arbitrary.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[3]</a></td><td>For a mathematical justification for <em>why</em> the gradient leads us in the
direction of most change, see <a class="reference external" href="../understanding-gradient-descent.html">this post</a>.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-4">[4]</a></td><td>An alternative way I've seen this equation written is to express <em>m</em>
as:</td></tr>
</tbody>
</table>
<img alt="\[\begin{align*} m &amp;amp;= \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} \\   &amp;amp;= \frac{cov(x, y)}{var(x)} \end{align*}\]" class="align-center" src="../../images/math/53639f1f77080dbe8a6d3a8cd06e08a90de69a8e.png" style="height: 92px;" />
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-5">[5]</a></td><td>Can you figure out why even the analytical solution is a little off from
the actual parameters used to generated this data?</td></tr>
</tbody>
</table>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2016/linear-regression/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:59:23 GMT -->
</html>