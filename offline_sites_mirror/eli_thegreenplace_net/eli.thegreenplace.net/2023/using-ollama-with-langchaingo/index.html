<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2023/using-ollama-with-langchaingo/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:46:28 GMT -->
<head>
    <title>Using Ollama with LangChainGo - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../theme/css/style.css" type="text/css"/>

        <link href="../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../index.html" class="navbar-brand">
                <img src="../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="index.html"
                       rel="bookmark"
                       title="Permalink to Using Ollama with LangChainGo">
                        Using Ollama with LangChainGo
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> November 22, 2023 at 05:25</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../tag/go.html">Go</a>
        ,
    <a href="../../tag/machine-learning.html">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>One of the most exciting areas of LLM-related development in 2023 is the
availability of powerful (and sometimes even open-source) models we can run
locally on our machines.</p>
<p>Several tools exist that make it relatively easy to obtain, run and manage
such models locally; for example <a class="reference external" href="https://ollama.ai/">Ollama</a> (written in Go!)
<a class="reference external" href="https://localai.io/">LocalAI</a> (also largely in Go!).</p>
<p>In this post I'm going to describe how to use Ollama to run a model locally,
communicate with it using its API and integrate it into a Go program using
LangChainGo.</p>
<img alt="Ollama logo, taken from the Ollama website" class="align-center" src="../../images/2023/ollama.png" />
<div class="section" id="setting-up-ollama">
<h2>Setting up Ollama</h2>
<p>To start, follow the installation and setup instructions <a class="reference external" href="https://ollama.ai/download">from the Ollama
website</a>. Ollama runs as a service, exposing a
REST API on a localhost port. Once installed, you can invoke <tt class="docutils literal">ollama run
&lt;modelname&gt;</tt> to talk to this model; the model is downloaded and cached the
first time it's requested.</p>
<p>In this blog post, we'll be talking to the <tt class="docutils literal">llama2</tt> model, so run <tt class="docutils literal">ollama run
llama2</tt>. After the <tt class="docutils literal">ollama</tt> command finishes installing the model, we'll see
a prompt and will be able to chat with it <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a>:</p>
<div class="highlight"><pre><span></span>&gt;&gt;&gt; very briefly, tell me the difference between a comet and a meteor

 Sure! Here&#39;s a brief difference:

A comet is a small, icy body that orbits the sun. When a comet approaches the
inner solar system, the heat from the sun causes the comet to release gas and
dust, creating a bright tail that can be seen from Earth.

A meteor, on the other hand, is a small piece of rock or metal that enters the
Earth&#39;s atmosphere. As it travels through the atmosphere, the friction causes
the meteor to heat up and burn, producing a bright streak of light in the sky,
commonly known as a shooting star. If the meteor survives its passage through
the atmosphere and lands on Earth, it is called a meteorite.
</pre></div>
</div>
<div class="section" id="manually-invoking-the-rest-api">
<h2>Manually invoking the REST API</h2>
<p><tt class="docutils literal">ollama</tt> runs in the background and exposes a REST API on port 11434. We
can talk to it &quot;manually&quot; using <tt class="docutils literal">curl</tt> commands:</p>
<div class="highlight"><pre><span></span>$ curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama2&quot;,
  &quot;prompt&quot;: &quot;very briefly, tell me the difference between a comet and a meteor&quot;,
  &quot;stream&quot;: false
}&#39;
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:53:47.32607236Z&quot;,
 &quot;response&quot;:&quot;\nSure! Here&#39;s the difference:\n\nA comet is a small,
  icy body that orbits the sun. Comets are composed of dust and frozen
  gases, such as water, methane, and ammonia. When a comet approaches
  the inner solar system, the sun&#39;s heat causes the comet&#39;s ices
  to vaporize, creating a bright tail of gas and dust that can be seen
  from Earth.\n\nA meteor, on the other hand, is a small body of rock
[...]
</pre></div>
<p>This may take a bit of time, especially if your machine doesn't have a powerful
GPU. We can also ask Ollama to <em>stream</em> the model's responses so we get output
as soon as it's ready, before waiting for the model to complete its reply. We
can do that by passing <tt class="docutils literal">&quot;stream&quot;: true</tt>:</p>
<div class="highlight"><pre><span></span>$ curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama2&quot;,
  &quot;prompt&quot;: &quot;very briefly, tell me the difference between a comet and a meteor&quot;,
  &quot;stream&quot;: true
}&#39;
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:06.709696317Z&quot;,&quot;response&quot;:&quot;\n&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:06.89584866Z&quot;,&quot;response&quot;:&quot; Sure&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:07.053242632Z&quot;,&quot;response&quot;:&quot;!&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:07.217867169Z&quot;,&quot;response&quot;:&quot; Here&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:07.374557181Z&quot;,&quot;response&quot;:&quot;&#39;&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:07.560674269Z&quot;,&quot;response&quot;:&quot;s&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:07.719981235Z&quot;,&quot;response&quot;:&quot; the&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:07.878008762Z&quot;,&quot;response&quot;:&quot; quick&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:08.035846088Z&quot;,&quot;response&quot;:&quot; and&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:08.192951527Z&quot;,&quot;response&quot;:&quot; dirty&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:08.372491712Z&quot;,&quot;response&quot;:&quot;:&quot;,&quot;done&quot;:false}
{&quot;model&quot;:&quot;llama2&quot;,&quot;created_at&quot;:&quot;2023-11-20T14:57:08.530388951Z&quot;,&quot;response&quot;:&quot;\n&quot;,&quot;done&quot;:false}
[...]
</pre></div>
<p>The response is broken into separate JSON messages with <tt class="docutils literal">&quot;done&quot;: false</tt>. The
last message will have <tt class="docutils literal">&quot;done&quot;: true</tt>.</p>
<p>We can send other kinds of requests to the model; for example, we can ask it
to calculate embeddings:</p>
<div class="highlight"><pre><span></span>$ curl http://localhost:11434/api/embeddings -d &#39;{
  &quot;model&quot;: &quot;llama2&quot;,
  &quot;prompt&quot;: &quot;article about asteroids&quot;
}&#39; | jq
{
  &quot;embedding&quot;: [
    0.5615004897117615,
    -2.90958833694458,
    0.836567759513855,
    -0.3081018626689911,
    -1.1424092054367065,
    -1.5503573417663574,
    0.93345707654953,
    -3.008531093597412,
    3.6917684078216553,
    0.3383431136608124,
    1.0924581289291382,
    -2.1573197841644287,
[...]
</pre></div>
</div>
<div class="section" id="programmatic-access-to-models-through-ollama">
<h2>Programmatic access to models through Ollama</h2>
<p>The <a class="reference external" href="https://github.com/jmorganca/ollama">Ollama README</a> lists some ways to
interact with <tt class="docutils literal">ollama</tt> models programmatically; the most common way seems
to be through <a class="reference external" href="https://www.langchain.com/">LangChain</a> and related tools.
LangChain is emerging as a common framework for interacting with LLMs; it has
high-level tools for chaining LLM-related tasks together, but also low-level
SDKs for each model's REST API.</p>
<p>Here I will show how to talk to Ollama via the Go
port of LangChain - <a class="reference external" href="https://github.com/tmc/langchaingo">LangChainGo</a>.</p>
<p>Let's start with a simple non-streaming completion request:</p>
<div class="highlight"><pre><span></span><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w"></span>

<span class="kn">import</span><span class="w"> </span><span class="p">(</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;context&quot;</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;fmt&quot;</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;log&quot;</span><span class="w"></span>

<span class="w">  </span><span class="s">&quot;github.com/tmc/langchaingo/llms&quot;</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;github.com/tmc/langchaingo/llms/ollama&quot;</span><span class="w"></span>
<span class="p">)</span><span class="w"></span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nx">llm</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ollama</span><span class="p">.</span><span class="nx">New</span><span class="p">(</span><span class="nx">ollama</span><span class="p">.</span><span class="nx">WithModel</span><span class="p">(</span><span class="s">&quot;llama2&quot;</span><span class="p">))</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nx">query</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="s">&quot;very briefly, tell me the difference between a comet and a meteor&quot;</span><span class="w"></span>

<span class="w">  </span><span class="nx">ctx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">Background</span><span class="p">()</span><span class="w"></span>
<span class="w">  </span><span class="nx">completion</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">llms</span><span class="p">.</span><span class="nx">GenerateFromSinglePrompt</span><span class="p">(</span><span class="nx">ctx</span><span class="p">,</span><span class="w"> </span><span class="nx">llm</span><span class="p">,</span><span class="w"> </span><span class="nx">query</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">&quot;Response:\n&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">completion</span><span class="p">)</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
<p>For streaming, <tt class="docutils literal">GenerateFromSinglePrompt</tt> will take a streaming function as an
option. The streaming function is invoked with each chunk of data as it's
received; at the end, it's called with an empty chunk:</p>
<div class="highlight"><pre><span></span><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w"></span>

<span class="kn">import</span><span class="w"> </span><span class="p">(</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;context&quot;</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;fmt&quot;</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;log&quot;</span><span class="w"></span>

<span class="w">  </span><span class="s">&quot;github.com/tmc/langchaingo/llms&quot;</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;github.com/tmc/langchaingo/llms/ollama&quot;</span><span class="w"></span>
<span class="p">)</span><span class="w"></span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nx">llm</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ollama</span><span class="p">.</span><span class="nx">New</span><span class="p">(</span><span class="nx">ollama</span><span class="p">.</span><span class="nx">WithModel</span><span class="p">(</span><span class="s">&quot;llama2&quot;</span><span class="p">))</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nx">query</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="s">&quot;very briefly, tell me the difference between a comet and a meteor&quot;</span><span class="w"></span>

<span class="w">  </span><span class="nx">ctx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">Background</span><span class="p">()</span><span class="w"></span>
<span class="w">  </span><span class="nx">_</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">llms</span><span class="p">.</span><span class="nx">GenerateFromSinglePrompt</span><span class="p">(</span><span class="nx">ctx</span><span class="p">,</span><span class="w"> </span><span class="nx">llm</span><span class="p">,</span><span class="w"> </span><span class="nx">query</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nx">llms</span><span class="p">.</span><span class="nx">WithStreamingFunc</span><span class="p">(</span><span class="kd">func</span><span class="p">(</span><span class="nx">ctx</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">Context</span><span class="p">,</span><span class="w"> </span><span class="nx">chunk</span><span class="w"> </span><span class="p">[]</span><span class="kt">byte</span><span class="p">)</span><span class="w"> </span><span class="kt">error</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">&quot;chunk len=%d: %s\n&quot;</span><span class="p">,</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">chunk</span><span class="p">),</span><span class="w"> </span><span class="nx">chunk</span><span class="p">)</span><span class="w"></span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="kc">nil</span><span class="w"></span>
<span class="w">    </span><span class="p">}))</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
<p>The final completion is still returned from <tt class="docutils literal">GenerateFromSinglePrompt</tt>, in
case it's needed. Running this, we'll get something like the following output:</p>
<div class="highlight"><pre><span></span>$ go run ollama-completion-stream.go
chunk len=1:

chunk len=5:  Sure
chunk len=1: !
chunk len=5:  Here
chunk len=1: &#39;
chunk len=1: s
chunk len=2:  a
chunk len=6:  brief
chunk len=12:  explanation
[...]
chunk len=0:
</pre></div>
<p>Finally, we can also obtain embeddings from the model using the <tt class="docutils literal">langchain</tt>
package:</p>
<div class="highlight"><pre><span></span><span class="kn">package</span><span class="w"> </span><span class="nx">main</span><span class="w"></span>

<span class="kn">import</span><span class="w"> </span><span class="p">(</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;context&quot;</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;fmt&quot;</span><span class="w"></span>
<span class="w">  </span><span class="s">&quot;log&quot;</span><span class="w"></span>

<span class="w">  </span><span class="s">&quot;github.com/tmc/langchaingo/llms/ollama&quot;</span><span class="w"></span>
<span class="p">)</span><span class="w"></span>

<span class="kd">func</span><span class="w"> </span><span class="nx">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="nx">llm</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">ollama</span><span class="p">.</span><span class="nx">New</span><span class="p">(</span><span class="nx">ollama</span><span class="p">.</span><span class="nx">WithModel</span><span class="p">(</span><span class="s">&quot;llama2&quot;</span><span class="p">))</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nx">texts</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="p">[]</span><span class="kt">string</span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="s">&quot;meteor&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s">&quot;comet&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="s">&quot;puppy&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nx">ctx</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">context</span><span class="p">.</span><span class="nx">Background</span><span class="p">()</span><span class="w"></span>
<span class="w">  </span><span class="nx">embs</span><span class="p">,</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">llm</span><span class="p">.</span><span class="nx">CreateEmbedding</span><span class="p">(</span><span class="nx">ctx</span><span class="p">,</span><span class="w"> </span><span class="nx">texts</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="nx">err</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="kc">nil</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="nx">err</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">&quot;Got %d embeddings:\n&quot;</span><span class="p">,</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">embs</span><span class="p">))</span><span class="w"></span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nx">emb</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">embs</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">&quot;%d: len=%d; first few=%v\n&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">emb</span><span class="p">),</span><span class="w"> </span><span class="nx">emb</span><span class="p">[:</span><span class="mi">4</span><span class="p">])</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<div class="section" id="code">
<h2>Code</h2>
<p>The full code for this post is <a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/ollama-go-langchain">available on GitHub</a>.</p>
<p><strong>Update 2024-02-22</strong>: See <a class="reference external" href="../../2024/gemma-ollama-and-langchaingo/index.html">a followup post on using additional models</a>
like Google's Gemma with the same setup.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td>ML models involve a huge amount of mathematical computations and
typically run best on beefy GPUs. If your machine (like mine!) doesn't
have a GPU installed, the model will still work on the CPU, but runs
very slowly.</td></tr>
</tbody>
</table>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2023/using-ollama-with-langchaingo/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:46:28 GMT -->
</html>