<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">

<!-- Mirrored from eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:46:24 GMT -->
<head>
    <title>Retrieval Augmented Generation in Go - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="../../favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="../../theme/css/bootstrap.min.css" type="text/css"/>
    <link href="../../theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="../../theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="../../theme/css/style.css" type="text/css"/>

        <link href="../../feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="../../index.html" class="navbar-brand">
                <img src="../../images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="../../pages/about.html">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="../../pages/projects.html">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="../../archives/all.html">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="index.html"
                       rel="bookmark"
                       title="Permalink to Retrieval Augmented Generation in Go">
                        Retrieval Augmented Generation in Go
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> November 10, 2023 at 09:02</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="../../tag/go.html">Go</a>
        ,
    <a href="../../tag/machine-learning.html">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>I've been reading more and more about LLM-based applications recently, itching
to build something useful as a learning experience. In this post, I want to
share a Retrieval Augmented Generation (RAG) system I've built in 100% Go and
some insights I learned along the way.</p>
<div class="section" id="some-limitations-of-current-llms">
<h2>Some limitations of current LLMs</h2>
<p>Let's take OpenAI's API as an example; for your hard-earned dollars, it gives
you access to powerful LLMs and related models. These LLMs have some
limitations as a general knowledge system <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a>:</p>
<ul class="simple">
<li>They have a training cutoff date somewhere in the past; recently, OpenAI
moved the cutoff of their GPT models from 2021 to April 2023, but it's
still not real-time.</li>
<li>Even if LLMs develop more real-time training, they still only have
access to public data. They aren't familiar with your internal documents,
which you may want to use them on.</li>
<li>You pay per <em>token</em>, which is
about 3/4 of a word; there can be different pricing for input tokens and
output tokens. The prices are low if you're only experimenting, but can grow
fast if you're working at scale. This may limit how many tokens you want an
LLM to crunch for you in each request.</li>
</ul>
</div>
<div class="section" id="retrieval-augmented-generation">
<h2>Retrieval Augmented Generation</h2>
<p>One of the most popular emerging techniques to address these limitations
is Retrieval Augmented Generation (RAG). Here's a useful diagram borrowed
from a <a class="reference external" href="https://cloud.google.com/blog/products/ai-machine-learning/generative-ai-applications-with-vertex-ai-palm-2-models-and-langchain">GCP blog post</a>:</p>
<img alt="Diagram of RAG" class="align-center" src="../../images/2023/gcprag.png" />
<p>The idea is:</p>
<ul class="simple">
<li>We want the LLM to &quot;ingest&quot; a large body of text it wasn't trained on, and
then chat to it about it</li>
<li>Even if the full body of text fits the LLM's context window, this may be
too expensive for each query <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a></li>
<li>Therefore, we'll run a separate information retrieval stage, finding
the most relevant information for our query</li>
<li>Finally, we'll add this information as the context for our query and chat
with the LLM about it</li>
</ul>
<p>The third step in the list above is the trickiest part - finding the most
&quot;relevant&quot; information is difficult in the general case. Are we supposed to
build a search engine? Well, that would be one approach! Powerful full-text
search engines exist and could be helpful here, but there's a better way
using embeddings. Read on to see how it works.</p>
</div>
<div class="section" id="implementing-rag-in-go">
<h2>Implementing RAG in Go</h2>
<p>In the course of my research on the subject, I wrote a bunch of Python code to
perform RAG, and then ported it to Go. It was easier to find Python samples
online, but once everything clicked in my head, porting to Go was trivial.</p>
<p>This process led me to the following observation:</p>
<blockquote>
LLM-based applications like RAG are a data-pipeline task, not
a machine-learning task.</blockquote>
<p>What I mean by this is that the application doesn't crunch matrices, doesn't
explore the best loss function or gradient update, doesn't
train and evaluate models. It simply hooks up textual tools together; LLMs
are one such textual tool, embeddings are another. Therefore, Go is very well
suited for such applications! Go is much faster than Python, just as capable
with text processing, and its easy concurrency is helpful for applications that
spend a long time waiting for network I/O.</p>
</div>
<div class="section" id="the-motivating-problem">
<h2>The motivating problem</h2>
<p>When I started hacking on this project, OpenAI models had a cutoff in 2021,
but it remains relevant today. I wanted the LLM to answer questions about the
Go documentation - including recently published pages. For example, Go shipped
forward compatibility and toolchain management <a class="reference external" href="https://go.dev/blog/toolchain">in 1.21 this summer</a>.
Asking ChatGPT about it yields sub-optimal results, because of the training
cutoff:</p>
<div class="highlight"><pre><span></span>Me: what does GOTOOLCHAIN mean in go?

ChatGPT: In Go, the GOTOOLCHAIN environment variable is used to specify an
alternative toolchain to be used for compiling and linking Go code. This can
be useful when you need to use tools that are different from the default ones
provided with the Go installation, such as gccgo or other experimental
toolchains.
</pre></div>
<p>This is a classical LLM hallucination, because ChatGPT is not familiar with the
<tt class="docutils literal">GOTOOLCHAIN</tt> env var released in Go 1.21; it's a <em>plausible</em> explanation
of what such an env var <em>could</em> do in theory, but clearly not what I'm looking
for.</p>
<div class="highlight"><pre><span></span>Me: what is the latest released version of the Go programming language?

ChatGPT: As of my last update in April 2023, the latest stable version of Go
was 1.20. Please check the official Go website or the Go GitHub repository for
the most current version.
</pre></div>
<p>To address this, I wanted to build a RAG system that:</p>
<ol class="arabic simple">
<li>Reads all the latest Go documentation pages (Markdown files straight from the Go
<a class="reference external" href="https://github.com/golang/website">website repository</a>)</li>
<li>When a question is asked, finds the most relevant information from these
documentation pages</li>
<li>This relevant information is added as context to the question via basic
prompt engineering, and the question + context is passed to ChatGPT via its
API</li>
</ol>
<p>Let's get to it.</p>
</div>
<div class="section" id="step-1-ingest-documentation">
<h2>Step 1: ingest documentation</h2>
<p>Step 1 is trivial and doesn't involve any LLM-related technology. It's a
<a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/go-rag-openai/cmd/chunker">command-line tool</a> that recursively walks a locally cloned
<tt class="docutils literal">_content</tt> directory of the Go website's source code, reads each Markdown file
and splits it to &quot;chunks&quot; of approximately 1000 tokens <a class="footnote-reference" href="#footnote-3" id="footnote-reference-3">[3]</a>, consisting of
whole paragraphs. Each chunk is then stored in a SQLite DB with some additional
information:</p>
<div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">chunks</span><span class="w"> </span><span class="p">(</span><span class="w"></span>
<span class="w">  </span><span class="n">id</span><span class="w"> </span><span class="nb">INTEGER</span><span class="w"> </span><span class="k">PRIMARY</span><span class="w"> </span><span class="k">KEY</span><span class="w"> </span><span class="n">AUTOINCREMENT</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n">path</span><span class="w"> </span><span class="nb">TEXT</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n">nchunk</span><span class="w"> </span><span class="nb">INTEGER</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n">content</span><span class="w"> </span><span class="nb">TEXT</span><span class="w"></span>
<span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<div class="section" id="step-2a-calculate-embedding-for-each-chunk">
<h2>Step 2a: calculate embedding for each chunk</h2>
<p>Embeddings are a fascinating aspect of modern ML and LLMs. I won't
cover them in detail here - there are plenty of excellent resources online.
For our needs - an embedding model is a function that takes arbitrary text
and returns a fixed-size vector of real numbers that represents this text in
vector space (N-dimensional Cartesian coordinates). Related chunks of text will
be closer to each other in this space (using regular vector space distance
metrics) than unrelated chunks.</p>
<p>For this step, <a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/go-rag-openai/cmd/rag">a command-line tool</a>
with the <tt class="docutils literal"><span class="pre">--calculate</span></tt> flag will read the DB table created in step 1,
calculate the embedding for each chunk using <a class="reference external" href="https://platform.openai.com/docs/guides/embeddings">OpenAI's API</a>, and store the embedding
for each chunk in a new table:</p>
<div class="highlight"><pre><span></span><span class="k">CREATE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="k">IF</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="n">embeddings</span><span class="w"> </span><span class="p">(</span><span class="w"></span>
<span class="w">  </span><span class="n">id</span><span class="w"> </span><span class="nb">INTEGER</span><span class="w"> </span><span class="k">PRIMARY</span><span class="w"> </span><span class="k">KEY</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n">embedding</span><span class="w"> </span><span class="nb">BLOB</span><span class="w"></span>
<span class="p">)</span><span class="w"></span>
</pre></div>
<p>It's easy to correlate each chunk's text to its embedding by joining the
tables on <tt class="docutils literal">id</tt>.</p>
<p>This step takes a bit of time and costs a bit of $$$, because it sends an
embedding API request to OpenAI for each chunk. The embeddings are stored
in the DB as a cache, so we shouldn't have to compute them more than once.</p>
</div>
<div class="section" id="step-2b-find-the-most-relevant-context-for-a-question">
<h2>Step 2b: find the most relevant context for a question</h2>
<p>When a user asks a question - we can calculate the question's embedding, and
use <a class="reference external" href="https://en.wikipedia.org/wiki/Cosine_similarity">a bit of math</a> to
find the most relevant (closest in vector space) chunks to it from our DB
collected in the previous step.</p>
<p>This is configurable, but my demo program finds the 3 closest
chunks to the question (total of ~3K tokens).</p>
<p>Theoretically, this step requires a bit of engineering sophistication to
efficiently find the closest chunks; a whole exciting domain of
infrastructure called <em>vector databases</em> exists to address this need. Many of
them are implemented in Go, e.g. <a class="reference external" href="https://github.com/milvus-io/milvus">milvus</a>
or <a class="reference external" href="https://github.com/weaviate/weaviate">weaviate</a>.</p>
<p>Since our sample is small, I simply do this by iterating over all the chunks
in the DB one by one, calculating the question's vector similarity with each one
and keep the 3 chunks with best similarity.</p>
</div>
<div class="section" id="step-3-prompt-engineering-to-answer-the-question-with-context">
<h2>Step 3: prompt engineering to answer the question with context</h2>
<p>Now comes a small bit of prompt engineering, which is just a
fancy name for string interpolation :-) This is the literal Go code doing
this:</p>
<div class="highlight"><pre><span></span><span class="w">  </span><span class="nx">query</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Sprintf</span><span class="p">(</span><span class="s">`Use the below information to answer the subsequent question.</span>
<span class="s">Information:</span>
<span class="s">%v</span>

<span class="s">Question: %v`</span><span class="p">,</span><span class="w"> </span><span class="nx">contextInfo</span><span class="p">,</span><span class="w"> </span><span class="nx">theQuestion</span><span class="p">)</span><span class="w"></span>
</pre></div>
<p>Where <tt class="docutils literal">contextInfo</tt> is simply the 3 relevant chunks pasted together with
some newlines. That's it!</p>
<p>Asking our question about <tt class="docutils literal">GOTOOLCHAIN</tt> again using this approach, we get
a much better answer:</p>
<div class="highlight"><pre><span></span>In Go, `GOTOOLCHAIN` is an environment variable that determines which Go
toolchain version should be used. The value of `GOTOOLCHAIN` can be set to
various options:

1. `local`: This indicates that the bundled Go toolchain should be used.
2. `&lt;name&gt;`: This specifies a specific Go toolchain version that should
   be used. If a binary with that name is found in the system PATH, it will be
   used; otherwise, the `go` command will download and use the specified Go
   toolchain.
3. `&lt;name&gt;+auto` or `&lt;name&gt;+path`: This tells the `go` command to
   select and run a newer Go version as needed. It will consult the
   `toolchain` and `go` lines in the current workspace&#39;s `go.work` file (or
   the main module&#39;s `go.mod` file if there is no workspace) to determine
   which toolchain version to use. If a newer version is specified in those
   files, it will be used; otherwise, it will fallback to the default Go
   toolchain.

Overall, `GOTOOLCHAIN` is used to specify the specific Go toolchain version or
the selection process of a newer Go version that should be used by the `go`
command.
</pre></div>
</div>
<div class="section" id="code-and-final-words">
<h2>Code and final words</h2>
<p>The full code for this project is <a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2023/go-rag-openai">available on GitHub</a>; all
you need to run it is your own <tt class="docutils literal">OPENAI_API_KEY</tt>. The repository includes the
SQLite DB with the embeddings already pre-populated, so you don't even need to
run the <tt class="docutils literal">rag</tt> tool with <tt class="docutils literal"><span class="pre">--calculate</span></tt>. See the <tt class="docutils literal">README</tt> file for full
instructions.</p>
<p><strong>Update (2024-01-03)</strong>: the GitHub repository now includes a <tt class="docutils literal"><span class="pre">cmd/gemini-rag</span></tt>
directory that reimplements this RAG tool using the
<a class="reference external" href="https://ai.google.dev/">Google Gemini model</a>.</p>
<p>I'd like to thank <a class="reference external" href="https://simonwillison.net/">Simon Willison</a>, whose lucid
writing on this subject has been very helpful in my research for this project.
Specifically, the following resources were invaluable:</p>
<ul class="simple">
<li><a class="reference external" href="https://simonwillison.net/2023/Oct/23/embeddings/">Embeddings: What they are and why they matter</a></li>
<li><a class="reference external" href="https://simonwillison.net/2023/Aug/27/wordcamp-llms/">Making Large Language Models work for you</a></li>
<li><a class="reference external" href="https://github.com/simonw/llm">llm command-line tool</a></li>
</ul>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td>LLMs have much more serious limitations, of course, w.r.t. factfulness
and hallucinations. This list is focused on the topic of our specific
example and isn't a general review of LLMs.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td>Let's take OpenAI's newly announced <a class="reference external" href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">GPT 4 Turbo</a>, for example.
It has a whopping 128K token context window and costs 1 cent per 1K
tokens. If we use the full context for the input (ignoring for
the moment output tokens, which are more expensive), that's $1.28 per
query. Not for the faint of heart, if you want this to run at scale!</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[3]</a></td><td>Tokens are counted using the <a class="reference external" href="https://github.com/pkoukk/tiktoken-go">Go port of OpenAI's tiktoken library</a>.</td></tr>
</tbody>
</table>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2025 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="http://code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="../../theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="../../theme/js/count.js"></script>
</body>

<!-- Mirrored from eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/ by HTTrack Website Copier/3.x [XR&CO'2014], Sun, 16 Feb 2025 23:46:24 GMT -->
</html>